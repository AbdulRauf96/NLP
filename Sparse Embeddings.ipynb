{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtZUZxRsjFOZ"
      },
      "source": [
        "# <font color = 'pickle'> Import/install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QXeaTOfQROWC"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i3sPqk-fRT71"
      },
      "outputs": [],
      "source": [
        "import sys # library to access system parameters https://docs.python.org/3/library/sys.html\n",
        "from basic import basic_functions as bf # basic functions\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "T0nri2djPi_D",
        "outputId": "610d3ee5-3787-418b-ed74-be98105111bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not Running on Colab\n",
            "Base Folder is C:\\Users\\abdul\\OneDrive\\Documents\\MSBA\n",
            "Data Folder is C:\\Users\\abdul\\OneDrive\\Documents\\MSBA\\data_sets\n",
            "Archive Folder is C:\\Users\\abdul\\OneDrive\\Documents\\MSBA\\archive\n",
            "Output Folder is C:\\Users\\abdul\\OneDrive\\Documents\\MSBA\\output\n",
            "The path to the custom functions is C:/Users/abdul/OneDrive/Documents/MSBA/custom_functions\n",
            "The working directory is c:\\Users\\abdul\\OneDrive\\Documents\\MSBA\\notebooks\\NLP\n"
          ]
        }
      ],
      "source": [
        "base_folder,data,archive,output = bf.set_folders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T20:24:56.540071Z",
          "iopub.status.busy": "2021-09-11T20:24:56.539257Z",
          "iopub.status.idle": "2021-09-11T20:24:56.544797Z",
          "shell.execute_reply": "2021-09-11T20:24:56.544485Z",
          "shell.execute_reply.started": "2021-09-11T20:24:56.539974Z"
        },
        "id": "NBkcfK5xQkjv",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np   # For numerical operations\n",
        "import spacy         # For NLP preprocessing \n",
        "\n",
        "# Import required nltk packages\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Download the stopwords corpus\n",
        "from nltk.corpus import stopwords as nltk_stopwords  # Stopwords corpus\n",
        "\n",
        "# Import tweet tokenizer from nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Import CountVectorizer and TfidfVectorizer from scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Import pathlib for managing file paths\n",
        "from pathlib import Path\n",
        "\n",
        "# import custom-preprocessor from python file\n",
        "import custom_preprocessor_mod as cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t2_XksvXxCEj",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.5.1'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mFu3jvtTvOxN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "     --------------------------------------- 12.8/12.8 MB 28.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# download spacy model\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T20:24:59.919129Z",
          "iopub.status.busy": "2021-09-11T20:24:59.918662Z",
          "iopub.status.idle": "2021-09-11T20:25:00.179324Z",
          "shell.execute_reply": "2021-09-11T20:25:00.178886Z",
          "shell.execute_reply.started": "2021-09-11T20:24:59.919114Z"
        },
        "id": "PmZmCvuQQezx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# load spacy model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bzV0Las4fygR"
      },
      "source": [
        "# <font color = 'pickle'> Bag of Words (Sparse Embeddings) - Count Vectorizer </font>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aEeEQp3Pf8Kl"
      },
      "source": [
        "## <font color = 'pickle'>**What is Bag of Words (BoW)?** </font>\n",
        "\n",
        "A **bag-of-words** is a representation of text that describes the occurrence of words within a document <font color ='dodgerblue'>**disregarding grammar and word order**</font>. It involves two steps:\n",
        "\n",
        "    1. Create Vocabulary. Each word in vocabulary forms feature(independent variable) to represent document.\n",
        "    2. Score words (based on frequency or occurence) to create Vectors."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pmUenp0Q4YDU"
      },
      "source": [
        "## <font color = 'pickle'> **Why do you need to learn Bag of Words?** </font>\n",
        "\n",
        "- Till now we have learnt how to pre-process the text data i.e clean the text data.\n",
        "- Our final goal is to use text data in Machine Learning (ML) models. For example - we want to predict whether e-mail is a spam or not based on the text of the data. \n",
        "- But ML models can understand only numbers. Therefore we need to convert text to vectors (numbers).\n",
        "- The simple method of converting text to numbers is to use 'Bag of Words approach'\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzKYz9DZ4YDU"
      },
      "source": [
        "## <font color = 'pickle'>**Learning Outcome** </font>\n",
        "After completing this tutorial, you will know\n",
        "\n",
        "1. What the bag-of-words approach is and how you can use it to represent text data.\n",
        "2. What are different techniques to prepare a vocabulary and score words.\n",
        "3. How to implement 'Bag-of-words' approach in python using sklearn."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uTmQrTrVxCEh"
      },
      "source": [
        "# <font color = 'pickle'> **Tutorial Overview** </font>\n",
        " - Generating Vocab\n",
        " - Generating vectors using Vocab\n",
        "     - Binary Vectorizer\n",
        "     - Count Vectorizer\n",
        "     - tfidf Vectorizer\n",
        " \n",
        " - Modifying Vocab\n",
        " - Example - IMDB Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTlvVAjziFYm"
      },
      "source": [
        "## <font color = 'pickle'> **Generating Vocab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whv4QgKPB0Gv"
      },
      "source": [
        "###  <font color = 'pickle'> **Dummy Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T20:25:03.583435Z",
          "iopub.status.busy": "2021-09-11T20:25:03.582835Z",
          "iopub.status.idle": "2021-09-11T20:25:03.590454Z",
          "shell.execute_reply": "2021-09-11T20:25:03.589100Z",
          "shell.execute_reply.started": "2021-09-11T20:25:03.583366Z"
        },
        "id": "YzXnofA3nF3W",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Dummy corpus\n",
        "Corpus = [\"Count Vectorizer - for this vectorizer, scoring is done based on frequency. For this vectorizer frequency is key. @vectorizer #frequency @frequency, doesn’t\",\n",
        "          \"tfidf vectorizer - for this vectorizer, scoring is done based on tfidf,  higher tfidf higher score #tfidf @vectorizer \"  ,\n",
        "          \"Binary vectorizer - for this vectorizer, scoring is done based on presence of word. For this vectorizer, dummy is key #dummy @dummy @vectorizer \"]\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyhgUzi5CNgg"
      },
      "source": [
        "### <font color = 'pickle'>**Create an instance of Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T20:25:17.155888Z",
          "iopub.status.busy": "2021-09-11T20:25:17.155757Z",
          "iopub.status.idle": "2021-09-11T20:25:17.158014Z",
          "shell.execute_reply": "2021-09-11T20:25:17.157740Z",
          "shell.execute_reply.started": "2021-09-11T20:25:17.155874Z"
        },
        "id": "J3ZVqkIHCePq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIo86goBv6sg"
      },
      "source": [
        "The above code creates an instance of the `CountVectorizer` class from the `sklearn.feature_extraction.text module`. This class is used to convert a collection of text documents to a matrix of token counts. \n",
        "\n",
        "It accomplishes this by \n",
        "  1. tokenizing the input text\n",
        "  2. creating a vocabulary of all the tokens found in the text\n",
        "  3. encoding the text as a matrix of token counts based on this vocabulary.\n",
        "\n",
        "The created instance vectorizer can then be used to fit the text data to the vocabulary and generate the token count matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k5fgQ7Kb_9j3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mInit signature:\u001b[0m\n",
            "\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.int64'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mSource:\u001b[0m        \n",
            "\u001b[1;32mclass\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_VectorizerMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;34mr\"\"\"Convert a collection of text documents to a matrix of token counts.\n",
            "\n",
            "    This implementation produces a sparse representation of the counts using\n",
            "    scipy.sparse.csr_matrix.\n",
            "\n",
            "    If you do not provide an a-priori dictionary and you do not use an analyzer\n",
            "    that does some kind of feature selection then the number of features will\n",
            "    be equal to the vocabulary size found by analyzing the data.\n",
            "\n",
            "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    input : {'filename', 'file', 'content'}, default='content'\n",
            "        - If `'filename'`, the sequence passed as an argument to fit is\n",
            "          expected to be a list of filenames that need reading to fetch\n",
            "          the raw content to analyze.\n",
            "\n",
            "        - If `'file'`, the sequence items must have a 'read' method (file-like\n",
            "          object) that is called to fetch the bytes in memory.\n",
            "\n",
            "        - If `'content'`, the input is expected to be a sequence of items that\n",
            "          can be of type string or byte.\n",
            "\n",
            "    encoding : str, default='utf-8'\n",
            "        If bytes or files are given to analyze, this encoding is used to\n",
            "        decode.\n",
            "\n",
            "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
            "        Instruction on what to do if a byte sequence is given to analyze that\n",
            "        contains characters not of the given `encoding`. By default, it is\n",
            "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
            "        values are 'ignore' and 'replace'.\n",
            "\n",
            "    strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
            "        Remove accents and perform other character normalization\n",
            "        during the preprocessing step.\n",
            "        'ascii' is a fast method that only works on characters that have\n",
            "        a direct ASCII mapping.\n",
            "        'unicode' is a slightly slower method that works on any characters.\n",
            "        None (default) does nothing.\n",
            "\n",
            "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
            "        :func:`unicodedata.normalize`.\n",
            "\n",
            "    lowercase : bool, default=True\n",
            "        Convert all characters to lowercase before tokenizing.\n",
            "\n",
            "    preprocessor : callable, default=None\n",
            "        Override the preprocessing (strip_accents and lowercase) stage while\n",
            "        preserving the tokenizing and n-grams generation steps.\n",
            "        Only applies if ``analyzer`` is not callable.\n",
            "\n",
            "    tokenizer : callable, default=None\n",
            "        Override the string tokenization step while preserving the\n",
            "        preprocessing and n-grams generation steps.\n",
            "        Only applies if ``analyzer == 'word'``.\n",
            "\n",
            "    stop_words : {'english'}, list, default=None\n",
            "        If 'english', a built-in stop word list for English is used.\n",
            "        There are several known issues with 'english' and you should\n",
            "        consider an alternative (see :ref:`stop_words`).\n",
            "\n",
            "        If a list, that list is assumed to contain stop words, all of which\n",
            "        will be removed from the resulting tokens.\n",
            "        Only applies if ``analyzer == 'word'``.\n",
            "\n",
            "        If None, no stop words will be used. max_df can be set to a value\n",
            "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
            "        words based on intra corpus document frequency of terms.\n",
            "\n",
            "    token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
            "        Regular expression denoting what constitutes a \"token\", only used\n",
            "        if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
            "        or more alphanumeric characters (punctuation is completely ignored\n",
            "        and always treated as a token separator).\n",
            "\n",
            "        If there is a capturing group in token_pattern then the\n",
            "        captured group content, not the entire match, becomes the token.\n",
            "        At most one capturing group is permitted.\n",
            "\n",
            "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
            "        The lower and upper boundary of the range of n-values for different\n",
            "        word n-grams or char n-grams to be extracted. All values of n such\n",
            "        such that min_n <= n <= max_n will be used. For example an\n",
            "        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
            "        unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
            "        Only applies if ``analyzer`` is not callable.\n",
            "\n",
            "    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
            "        Whether the feature should be made of word n-gram or character\n",
            "        n-grams.\n",
            "        Option 'char_wb' creates character n-grams only from text inside\n",
            "        word boundaries; n-grams at the edges of words are padded with space.\n",
            "\n",
            "        If a callable is passed it is used to extract the sequence of features\n",
            "        out of the raw, unprocessed input.\n",
            "\n",
            "        .. versionchanged:: 0.21\n",
            "\n",
            "        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
            "        first read from the file and then passed to the given callable\n",
            "        analyzer.\n",
            "\n",
            "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
            "        When building the vocabulary ignore terms that have a document\n",
            "        frequency strictly higher than the given threshold (corpus-specific\n",
            "        stop words).\n",
            "        If float, the parameter represents a proportion of documents, integer\n",
            "        absolute counts.\n",
            "        This parameter is ignored if vocabulary is not None.\n",
            "\n",
            "    min_df : float in range [0.0, 1.0] or int, default=1\n",
            "        When building the vocabulary ignore terms that have a document\n",
            "        frequency strictly lower than the given threshold. This value is also\n",
            "        called cut-off in the literature.\n",
            "        If float, the parameter represents a proportion of documents, integer\n",
            "        absolute counts.\n",
            "        This parameter is ignored if vocabulary is not None.\n",
            "\n",
            "    max_features : int, default=None\n",
            "        If not None, build a vocabulary that only consider the top\n",
            "        max_features ordered by term frequency across the corpus.\n",
            "\n",
            "        This parameter is ignored if vocabulary is not None.\n",
            "\n",
            "    vocabulary : Mapping or iterable, default=None\n",
            "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
            "        indices in the feature matrix, or an iterable over terms. If not\n",
            "        given, a vocabulary is determined from the input documents. Indices\n",
            "        in the mapping should not be repeated and should not have any gap\n",
            "        between 0 and the largest index.\n",
            "\n",
            "    binary : bool, default=False\n",
            "        If True, all non zero counts are set to 1. This is useful for discrete\n",
            "        probabilistic models that model binary events rather than integer\n",
            "        counts.\n",
            "\n",
            "    dtype : dtype, default=np.int64\n",
            "        Type of the matrix returned by fit_transform() or transform().\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    vocabulary_ : dict\n",
            "        A mapping of terms to feature indices.\n",
            "\n",
            "    fixed_vocabulary_ : bool\n",
            "        True if a fixed vocabulary of term to indices mapping\n",
            "        is provided by the user.\n",
            "\n",
            "    stop_words_ : set\n",
            "        Terms that were ignored because they either:\n",
            "\n",
            "          - occurred in too many documents (`max_df`)\n",
            "          - occurred in too few documents (`min_df`)\n",
            "          - were cut off by feature selection (`max_features`).\n",
            "\n",
            "        This is only available if no vocabulary was given.\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    HashingVectorizer : Convert a collection of text documents to a\n",
            "        matrix of token counts.\n",
            "\n",
            "    TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
            "        of TF-IDF features.\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The ``stop_words_`` attribute can get large and increase the model size\n",
            "    when pickling. This attribute is provided only for introspection and can\n",
            "    be safely removed using delattr or set to None before pickling.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.feature_extraction.text import CountVectorizer\n",
            "    >>> corpus = [\n",
            "    ...     'This is the first document.',\n",
            "    ...     'This document is the second document.',\n",
            "    ...     'And this is the third one.',\n",
            "    ...     'Is this the first document?',\n",
            "    ... ]\n",
            "    >>> vectorizer = CountVectorizer()\n",
            "    >>> X = vectorizer.fit_transform(corpus)\n",
            "    >>> vectorizer.get_feature_names_out()\n",
            "    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
            "           'this'], ...)\n",
            "    >>> print(X.toarray())\n",
            "    [[0 1 1 1 0 0 1 0 1]\n",
            "     [0 2 0 1 0 1 1 0 1]\n",
            "     [1 0 0 1 1 0 1 1 1]\n",
            "     [0 1 1 1 0 0 1 0 1]]\n",
            "    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
            "    >>> X2 = vectorizer2.fit_transform(corpus)\n",
            "    >>> vectorizer2.get_feature_names_out()\n",
            "    array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
            "           'second document', 'the first', 'the second', 'the third', 'third one',\n",
            "           'this document', 'this is', 'this the'], ...)\n",
            "     >>> print(X2.toarray())\n",
            "     [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            "     [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            "     [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            "     [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
            "    \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"input\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"filename\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"content\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"decode_error\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"strip_accents\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"ascii\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"unicode\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"lowercase\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"boolean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"preprocessor\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"tokenizer\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"stop_words\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"token_pattern\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"ngram_range\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"analyzer\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStrOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"char\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"char_wb\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"max_df\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"both\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"min_df\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"both\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"max_features\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mInterval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"vocabulary\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHasMethods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__iter__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"boolean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"no_validation\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# delegate to numpy\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mdecode_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"(?u)\\b\\w\\w+\\b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_error\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip_accents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Sort features by name\n",
            "\n",
            "        Returns a reordered matrix and modifies the vocabulary in place\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0msorted_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmap_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmap_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"clip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_limit_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Remove too rare or too common features.\n",
            "\n",
            "        Prune features that are non zero in more samples than high or less\n",
            "        documents than low, modifying the vocabulary, and restricting it to\n",
            "        at most the limit most frequent.\n",
            "\n",
            "        This does not prune samples with zero features.\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;31m# Calculate a mask based on document frequencies\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_document_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmask\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[0mdfs\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmask\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[0mdfs\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mtfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmask_inds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mnew_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mnew_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_inds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mask\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mnew_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# maps old indices to new\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mremoved_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mdel\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0mremoved_terms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mkept_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;34m\"After pruning, no terms remain. Try a lower min_df or a higher max_df.\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;31m# Add a new value when a new vocabulary item is seen\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0manalyze\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mj_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;31m# disable defaultdict behaviour\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# = 2**31 - 1\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\"sparse CSR array has {} non-zero \"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\"elements and requires 64 bit indexing, \"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\"which is unsupported with 32 bit Python.\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mindices_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mindices_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mj_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        raw_documents : iterable\n",
            "            An iterable which generates either str, unicode or file objects.\n",
            "\n",
            "        y : None\n",
            "            This parameter is ignored.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        self : object\n",
            "            Fitted vectorizer.\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Learn the vocabulary dictionary and return document-term matrix.\n",
            "\n",
            "        This is equivalent to fit followed by transform, but more efficiently\n",
            "        implemented.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        raw_documents : iterable\n",
            "            An iterable which generates either str, unicode or file objects.\n",
            "\n",
            "        y : None\n",
            "            This parameter is ignored.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        X : array of shape (n_samples, n_features)\n",
            "            Document-term matrix.\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;31m# We intentionally don't call the transform method to make\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;31m# fit_transform overridable without unwanted side effects in\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;31m# TfidfVectorizer.\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_ngram_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\"Upper case characters found in\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\" vocabulary while 'lowercase'\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\" is True. These entries will not\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                        \u001b[1;34m\" be matched with any documents\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                    \u001b[1;32mbreak\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mn_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmax_doc_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_doc\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mmin_doc_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_doc\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_df corresponds to < documents than min_df\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_limit_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Transform documents to document-term matrix.\n",
            "\n",
            "        Extract token counts out of raw text documents using the vocabulary\n",
            "        fitted with fit or the one provided to the constructor.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        raw_documents : iterable\n",
            "            An iterable which generates either str, unicode or file objects.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        X : sparse matrix of shape (n_samples, n_features)\n",
            "            Document-term matrix.\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Return terms per document with nonzero entries in X.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            "            Document-term matrix.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        X_inv : list of arrays of shape (n_samples,)\n",
            "            List of arrays of terms.\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;31m# We need CSR format for fast row manipulations.\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0minverse_vocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0minverse_vocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[0minverse_vocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;34m\"\"\"Get output feature names for transformation.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        input_features : array-like of str or None, default=None\n",
            "            Not used, present here for API consistency by convention.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        feature_names_out : ndarray of str objects\n",
            "            Transformed feature names.\n",
            "        \"\"\"\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m            \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
            "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"X_types\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"string\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFile:\u001b[0m           c:\\users\\abdul\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\n",
            "\u001b[1;31mType:\u001b[0m           type\n",
            "\u001b[1;31mSubclasses:\u001b[0m     TfidfVectorizer"
          ]
        }
      ],
      "source": [
        "CountVectorizer??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kX6-YQCSuq"
      },
      "source": [
        "### <font color = 'pickle'>**Fit Vectorizer on corpus to generate vocab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnehlHO7RQKx",
        "outputId": "803c5486-a32f-4953-c162-ea811e6cdaf4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the vectorizer on corpus\n",
        "vectorizer.fit(Corpus)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R-8UMn_FSNCF"
      },
      "source": [
        "<font color = 'indianred'>**Vectorizer().fit() does the following**: </font>\n",
        "- lowercases your text \n",
        "- uses utf-8 encoding\n",
        "- performs tokenization (converts raw text to smaller units of text)\n",
        "- uses word level tokenization (meaning each word is treated as a separate token) and  ignores single characters during tokenization ( words like ‘a’ and ‘I’ are removed)\n",
        "- By default, the regular expression that is used to split the text and create tokens is : `\"\\b\\w\\w+\\b\"`. \n",
        "  - This means it finds all sequences of characters that consist of at least two letters or numbers(\\w) and that are separated by word boundaries (\\b). \n",
        "  - It does not find single-letter words, and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single word. \n",
        "- The CountVectorizer then converts all words to lowercasecharacters, so that “soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).\n",
        "- It then creates a dictionary of unique words.\n",
        "- The set of unique words is used as features in the CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayvQssZBRTNr",
        "outputId": "3fc8854a-c3a6-446a-c117-5a7df27d0e6e",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'count': 2,\n",
              " 'vectorizer': 18,\n",
              " 'for': 6,\n",
              " 'this': 17,\n",
              " 'scoring': 15,\n",
              " 'is': 9,\n",
              " 'done': 4,\n",
              " 'based': 0,\n",
              " 'on': 12,\n",
              " 'frequency': 7,\n",
              " 'key': 10,\n",
              " 'doesn': 3,\n",
              " 'tfidf': 16,\n",
              " 'higher': 8,\n",
              " 'score': 14,\n",
              " 'binary': 1,\n",
              " 'presence': 13,\n",
              " 'of': 11,\n",
              " 'word': 19,\n",
              " 'dummy': 5}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let us see the dictionary created \n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z1haa0zRkXM",
        "outputId": "9b34e162-d5e6-403e-e8ff-6acc1b52ad55",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['based' 'binary' 'count' 'doesn' 'done' 'dummy' 'for' 'frequency'\n",
            " 'higher' 'is' 'key' 'of' 'on' 'presence' 'score' 'scoring' 'tfidf' 'this'\n",
            " 'vectorizer' 'word']\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "# The set of unique words is used as features in the CountVectorizer\n",
        "features = vectorizer.get_feature_names_out()\n",
        "print(features)\n",
        "print(len(features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP8odLcIC66i"
      },
      "source": [
        "## <font color = 'pickle'>**Generate Vectors using Vocab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DwforYaDNsY"
      },
      "source": [
        "### <font color = 'pickle'>**Binary Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x5YgOMdDiYC",
        "outputId": "be19fafc-02ef-4905-fba4-44ee5a1148c4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CountVectorizer(binary=True)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "binary_vectorizer = CountVectorizer(binary=True)\n",
        "binary_vectorizer.fit(Corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4k8F2UTE4SU"
      },
      "source": [
        "- We can now call transform() method to transform sentences in our corpus to vectors.\n",
        "- <font color = 'dodgerblue'>**Each sentence**</font> in vocab will be represented by <font color = 'dodgerblue'>**vector of length equal to len(dictionary)**.</font>\n",
        "- The vectors are stored in the form of a <font color = 'dodgerblue'>**sparse matrix**.</font>\n",
        "- We can use <font color = 'dodgerblue'>**toarray()**</font> function to get complete matrix.\n",
        "- Number of columns represent the number of features (len(vocab)).\n",
        "- Number of rows represent the number the sentences in a corpus.\n",
        "- <font color = 'dodgerblue'>**For each row, the numbers displayed are 0 or 1 - indicating absence or presence of a word in a sentence.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T20:25:24.312760Z",
          "iopub.status.busy": "2021-09-11T20:25:24.312601Z",
          "iopub.status.idle": "2021-09-11T20:25:24.315437Z",
          "shell.execute_reply": "2021-09-11T20:25:24.315041Z",
          "shell.execute_reply.started": "2021-09-11T20:25:24.312747Z"
        },
        "id": "DzwuRnsInKHa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "binary_vectors = binary_vectorizer.transform(Corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfXmM7P3nVYy",
        "outputId": "4656099d-86cb-4cf2-930c-c423aab913d2",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vectors in sparse format\n",
            "  (0, 0)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 18)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 12)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 15)\t1\n",
            "  (1, 16)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 18)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 11)\t1\n",
            "  (2, 12)\t1\n",
            "  (2, 13)\t1\n",
            "  (2, 15)\t1\n",
            "  (2, 17)\t1\n",
            "  (2, 18)\t1\n",
            "  (2, 19)\t1\n"
          ]
        }
      ],
      "source": [
        "print(f'vectors in sparse format') \n",
        "print(binary_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr4aHet7Z3t_",
        "outputId": "2d4620c7-5cb1-472c-81af-538b571fa645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "binary vectors in array(dense) format\n",
            "[[1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0]\n",
            " [1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0]\n",
            " [1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1]]\n",
            "\n",
            "The shape of the binary vectors is : (3, 20)\n"
          ]
        }
      ],
      "source": [
        "print(f'\\nbinary vectors in array(dense) format') \n",
        "print(binary_vectors.toarray())\n",
        "print(f'\\nThe shape of the binary vectors is : {binary_vectors.toarray().shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3otH8XphHQJX",
        "outputId": "c705f449-30b7-4539-eb07-50d8d3af3639"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>based</th>\n",
              "      <th>binary</th>\n",
              "      <th>count</th>\n",
              "      <th>doesn</th>\n",
              "      <th>done</th>\n",
              "      <th>dummy</th>\n",
              "      <th>for</th>\n",
              "      <th>frequency</th>\n",
              "      <th>higher</th>\n",
              "      <th>is</th>\n",
              "      <th>key</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>presence</th>\n",
              "      <th>score</th>\n",
              "      <th>scoring</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>this</th>\n",
              "      <th>vectorizer</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   based  binary  count  doesn  done  dummy  for  frequency  higher  is  key  \\\n",
              "0      1       0      1      1     1      0    1          1       0   1    1   \n",
              "1      1       0      0      0     1      0    1          0       1   1    0   \n",
              "2      1       1      0      0     1      1    1          0       0   1    1   \n",
              "\n",
              "   of  on  presence  score  scoring  tfidf  this  vectorizer  word  \n",
              "0   0   1         0      0        1      0     1           1     0  \n",
              "1   0   1         0      1        1      1     1           1     0  \n",
              "2   1   1         1      0        1      0     1           1     1  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe for better visualization\n",
        "df_binary = pd.DataFrame(binary_vectors.toarray(), columns = features)\n",
        "df_binary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OAPPPILCATX-"
      },
      "source": [
        "### <font color = 'pickle'>**Count Vectorizer** </font>\n",
        "-  The vectors are stored in the form of a sparse matrix.\n",
        "- Number of columns represent the number of features (len(vocab))\n",
        "- Number of rows represent the number the sentences in a corpus\n",
        "- Thus, each sentence is represented by a vector of size of length of vocab.\n",
        "- For each row, <font color = 'dodgerblue'>**the numbers displayed are the number of times a particular word has occurred in the sentence.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuF8IkLUA8zp",
        "outputId": "e5df8bf5-53a6-41ec-fc5e-ff8af7ffbfe1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count vectors in array (dense) format\n",
            "\n",
            "[[1 0 1 1 1 0 2 4 0 2 1 0 1 0 0 1 0 2 4 0]\n",
            " [1 0 0 0 1 0 1 0 2 1 0 0 1 0 1 1 4 1 3 0]\n",
            " [1 1 0 0 1 3 2 0 0 2 1 1 1 1 0 1 0 2 4 1]]\n",
            "\n",
            "The shape of the count vectors is : (3, 20)\n"
          ]
        }
      ],
      "source": [
        "term_freq_vectorizer = CountVectorizer(binary=False)\n",
        "# we can combine fit and transform steps into a single step using fit_transform()\n",
        "count_vectors = term_freq_vectorizer.fit_transform(Corpus)\n",
        "print(f'count vectors in array (dense) format\\n') \n",
        "print(count_vectors.toarray())\n",
        "print(f'\\nThe shape of the count vectors is : {count_vectors.toarray().shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLJCKNMVH0nm",
        "outputId": "5d0f6418-0fed-4fd4-b117-cc3e3828d8d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>based</th>\n",
              "      <th>binary</th>\n",
              "      <th>count</th>\n",
              "      <th>doesn</th>\n",
              "      <th>done</th>\n",
              "      <th>dummy</th>\n",
              "      <th>for</th>\n",
              "      <th>frequency</th>\n",
              "      <th>higher</th>\n",
              "      <th>is</th>\n",
              "      <th>key</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>presence</th>\n",
              "      <th>score</th>\n",
              "      <th>scoring</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>this</th>\n",
              "      <th>vectorizer</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   based  binary  count  doesn  done  dummy  for  frequency  higher  is  key  \\\n",
              "0      1       0      1      1     1      0    2          4       0   2    1   \n",
              "1      1       0      0      0     1      0    1          0       2   1    0   \n",
              "2      1       1      0      0     1      3    2          0       0   2    1   \n",
              "\n",
              "   of  on  presence  score  scoring  tfidf  this  vectorizer  word  \n",
              "0   0   1         0      0        1      0     2           4     0  \n",
              "1   0   1         0      1        1      4     1           3     0  \n",
              "2   1   1         1      0        1      0     2           4     1  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe for better visualization\n",
        "df_count = pd.DataFrame(count_vectors.toarray(), columns = term_freq_vectorizer.get_feature_names_out())\n",
        "df_count"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dVlJmMtmhCYM"
      },
      "source": [
        "### <font color = 'pickle'>**tf-idf Vectorizer** </font>\n",
        "\n",
        "- One measure of how important a word is term frequency (tf) (how frequently a word occurs in a document). We examined term frequency in previous sections where we used CountVectorizer to get the freqency of each word.\n",
        "- But there may be words in a document, that occur many times but these words also occur in all other documents as well.\n",
        "- Therefore the word might not be a good representation of the document.\n",
        "- We can account for this by  <font color = 'dodgerblue'>giving more importance to words that occur in fewer documents using inverse document frequency </font> ((# Number of documents) / (Number of documents containing the word)).\n",
        "- This can be <font color = 'dodgerblue'>combined with term frequency</font> to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.\n",
        "- The idea of tf-idf is to <font color = 'dodgerblue'>find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.</font>\n",
        "- tf-idf gives more weight to the the words that are important (i.e., occur more frequently) in a given document, but occur rarely in other documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmb0UrD_xCEo",
        "outputId": "e5dd4f80-1042-4f1b-a553-d0e3a554605b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfidf vectors in array (dense) format\n",
            "\n",
            "[[0.10829999 0.         0.18336782 0.18336782 0.10829999 0.\n",
            "  0.21659998 0.73347128 0.         0.21659998 0.13945595 0.\n",
            "  0.10829999 0.         0.         0.10829999 0.         0.21659998\n",
            "  0.43319995 0.        ]\n",
            " [0.11455596 0.         0.         0.         0.11455596 0.\n",
            "  0.11455596 0.         0.3879202  0.11455596 0.         0.\n",
            "  0.11455596 0.         0.1939601  0.11455596 0.77584039 0.11455596\n",
            "  0.34366788 0.        ]\n",
            " [0.11874019 0.20104462 0.         0.         0.11874019 0.60313387\n",
            "  0.23748039 0.         0.         0.23748039 0.15289962 0.20104462\n",
            "  0.11874019 0.20104462 0.         0.11874019 0.         0.23748039\n",
            "  0.47496077 0.20104462]]\n",
            "\n",
            "The shape of the tfidf vectors is : (3, 20)\n"
          ]
        }
      ],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "# we can combine fit and transform steps into a single step using fit_transform()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(Corpus)\n",
        "print(f'tfidf vectors in array (dense) format\\n') \n",
        "print(tfidf_vectors.toarray())\n",
        "print(f'\\nThe shape of the tfidf vectors is : {tfidf_vectors.toarray().shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8SIqG_IZo1",
        "outputId": "26f797d0-fcc3-4283-d942-dac57463de5f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>based</th>\n",
              "      <th>binary</th>\n",
              "      <th>count</th>\n",
              "      <th>doesn</th>\n",
              "      <th>done</th>\n",
              "      <th>dummy</th>\n",
              "      <th>for</th>\n",
              "      <th>frequency</th>\n",
              "      <th>higher</th>\n",
              "      <th>is</th>\n",
              "      <th>key</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>presence</th>\n",
              "      <th>score</th>\n",
              "      <th>scoring</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>this</th>\n",
              "      <th>vectorizer</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.1834</td>\n",
              "      <td>0.1834</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2166</td>\n",
              "      <td>0.7335</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2166</td>\n",
              "      <td>0.1395</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2166</td>\n",
              "      <td>0.4332</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.3879</td>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.7758</td>\n",
              "      <td>0.1146</td>\n",
              "      <td>0.3437</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1187</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.1187</td>\n",
              "      <td>0.6031</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.1529</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.1187</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.1187</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.4750</td>\n",
              "      <td>0.201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    based  binary   count   doesn    done   dummy     for  frequency  higher  \\\n",
              "0  0.1083   0.000  0.1834  0.1834  0.1083  0.0000  0.2166     0.7335  0.0000   \n",
              "1  0.1146   0.000  0.0000  0.0000  0.1146  0.0000  0.1146     0.0000  0.3879   \n",
              "2  0.1187   0.201  0.0000  0.0000  0.1187  0.6031  0.2375     0.0000  0.0000   \n",
              "\n",
              "       is     key     of      on  presence  score  scoring   tfidf    this  \\\n",
              "0  0.2166  0.1395  0.000  0.1083     0.000  0.000   0.1083  0.0000  0.2166   \n",
              "1  0.1146  0.0000  0.000  0.1146     0.000  0.194   0.1146  0.7758  0.1146   \n",
              "2  0.2375  0.1529  0.201  0.1187     0.201  0.000   0.1187  0.0000  0.2375   \n",
              "\n",
              "   vectorizer   word  \n",
              "0      0.4332  0.000  \n",
              "1      0.3437  0.000  \n",
              "2      0.4750  0.201  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe for better visualization\n",
        "df_tfidf = pd.DataFrame(tfidf_vectors.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n",
        "df_tfidf.round(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq8pvYnkxCEo"
      },
      "source": [
        "### <font color = 'pickle'>**Undertstanding tfidf calculations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T20:59:13.152712Z",
          "iopub.status.busy": "2021-09-11T20:59:13.152444Z",
          "iopub.status.idle": "2021-09-11T20:59:13.158508Z",
          "shell.execute_reply": "2021-09-11T20:59:13.157542Z",
          "shell.execute_reply.started": "2021-09-11T20:59:13.152683Z"
        },
        "id": "wSoWEk3DxCEo",
        "tags": []
      },
      "source": [
        "By default <br> \n",
        "$\\text{tfidf}(w, d) = \\text{tf(w, d)} * \\text{idf(w)}$\n",
        "<br>\n",
        "$\\text{idf(w)} = \\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1$\n",
        "<br><br>\n",
        "if smooth_idf = False (default is True):\n",
        "<br>\n",
        "$\\text{idf(w)} = \\log\\big(\\frac{N }{N_w}\\big) + 1$\n",
        "<br><br>\n",
        "if sublinear_tfbool = True (default is False)\n",
        "<br>\n",
        "$\\text{tf(w, d)} = \\log(\\text{tf(w, d)} ) + 1$\n",
        "\n",
        "Here:<br>\n",
        "- $\\text{tf}(w, d)$ is number of times word $w$ appears in document $d$ \n",
        "<br>\n",
        "- $\\text{idf}(w)$ is inverse document frequency of word $w$\n",
        "- $N$ is total number of documents\n",
        "- $N_w$ is number of documents that contain word w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuTBzPljxCEo",
        "outputId": "08752cfb-6f71-4df6-97e2-2fb71eb0f9a4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.        , 1.69314718, 1.69314718, 1.69314718, 1.        ,\n",
              "       1.69314718, 1.        , 1.69314718, 1.69314718, 1.        ,\n",
              "       1.28768207, 1.69314718, 1.        , 1.69314718, 1.69314718,\n",
              "       1.        , 1.69314718, 1.        , 1.        , 1.69314718])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate inverse document frequency for each feature (word)\n",
        "term_idf = tfidf_vectorizer.idf_\n",
        "term_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnwOVSQbJhVf",
        "outputId": "ef4fc918-7e60-4217-e567-f1a53fcfba41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>based</th>\n",
              "      <th>binary</th>\n",
              "      <th>count</th>\n",
              "      <th>doesn</th>\n",
              "      <th>done</th>\n",
              "      <th>dummy</th>\n",
              "      <th>for</th>\n",
              "      <th>frequency</th>\n",
              "      <th>higher</th>\n",
              "      <th>is</th>\n",
              "      <th>key</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>presence</th>\n",
              "      <th>score</th>\n",
              "      <th>scoring</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>this</th>\n",
              "      <th>vectorizer</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2877</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6931</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   based  binary   count   doesn  done   dummy  for  frequency  higher   is  \\\n",
              "0    1.0  1.6931  1.6931  1.6931   1.0  1.6931  1.0     1.6931  1.6931  1.0   \n",
              "\n",
              "      key      of   on  presence   score  scoring   tfidf  this  vectorizer  \\\n",
              "0  1.2877  1.6931  1.0    1.6931  1.6931      1.0  1.6931   1.0         1.0   \n",
              "\n",
              "     word  \n",
              "0  1.6931  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe for better visualization\n",
        "df_idf = pd.DataFrame(term_idf, index = tfidf_vectorizer.get_feature_names_out())\n",
        "df_idf.round(4).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGXv36hp6wIk",
        "outputId": "314f5e5c-908a-4154-ab19-21969c283c94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>based</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>binary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>count</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>doesn</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>done</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dummy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>for</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>frequency</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>higher</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>is</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>key</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>on</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>presence</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>score</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>scoring</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>this</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>vectorizer</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>word</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      features  tf\n",
              "0        based   1\n",
              "1       binary   0\n",
              "2        count   1\n",
              "3        doesn   1\n",
              "4         done   1\n",
              "5        dummy   0\n",
              "6          for   2\n",
              "7    frequency   4\n",
              "8       higher   0\n",
              "9           is   2\n",
              "10         key   1\n",
              "11          of   0\n",
              "12          on   1\n",
              "13    presence   0\n",
              "14       score   0\n",
              "15     scoring   1\n",
              "16       tfidf   0\n",
              "17        this   2\n",
              "18  vectorizer   4\n",
              "19        word   0"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe for tf vectors for the first document\n",
        "\n",
        "# Create a dense numpy array from the sparse count vector for the first document\n",
        "first_document_tf = count_vectors[0].toarray().ravel()\n",
        "\n",
        "# Get the feature names for the term frequency vectors\n",
        "feature_names_tf = term_freq_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a dataframe from the term frequency feature names and values\n",
        "df_tf = pd.DataFrame({'features':feature_names_tf, 'tf':first_document_tf})\n",
        "df_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MUwbSGf-C3V"
      },
      "source": [
        "Note: The `toarray` method is used to convert the sparse matrix into a dense numpy array, and `ravel` is used to flatten the resulting 2-dimensional array into a 1-dimensional array. This is necessary because pandas dataframes expect 1-dimensional arrays as values for the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmWgJ7RP-AgQ",
        "outputId": "20a711bc-d9d4-449b-f6a8-0397d5bb9eb0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>norm_tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>frequency</td>\n",
              "      <td>4</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.733471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>vectorizer</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.433200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>this</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>for</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>is</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>count</td>\n",
              "      <td>1</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.183368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>doesn</td>\n",
              "      <td>1</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.183368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>key</td>\n",
              "      <td>1</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>0.139456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>on</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>scoring</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>based</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>done</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>binary</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>presence</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>score</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>higher</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dummy</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>word</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      features  tf       idf  norm_tfidf\n",
              "7    frequency   4  1.693147    0.733471\n",
              "18  vectorizer   4  1.000000    0.433200\n",
              "17        this   2  1.000000    0.216600\n",
              "6          for   2  1.000000    0.216600\n",
              "9           is   2  1.000000    0.216600\n",
              "2        count   1  1.693147    0.183368\n",
              "3        doesn   1  1.693147    0.183368\n",
              "10         key   1  1.287682    0.139456\n",
              "12          on   1  1.000000    0.108300\n",
              "15     scoring   1  1.000000    0.108300\n",
              "0        based   1  1.000000    0.108300\n",
              "4         done   1  1.000000    0.108300\n",
              "11          of   0  1.693147    0.000000\n",
              "1       binary   0  1.693147    0.000000\n",
              "13    presence   0  1.693147    0.000000\n",
              "14       score   0  1.693147    0.000000\n",
              "8       higher   0  1.693147    0.000000\n",
              "16       tfidf   0  1.693147    0.000000\n",
              "5        dummy   0  1.693147    0.000000\n",
              "19        word   0  1.693147    0.000000"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe for tfidf vectors for the first document\n",
        "first_document_tfidf = tfidf_vectors[0].toarray().ravel()\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "df_tfidf = pd.DataFrame({'features':feature_names_tfidf , 'idf':term_idf, 'norm_tfidf':first_document_tfidf})\n",
        "\n",
        "# combine dataframes\n",
        "\n",
        "# Merge the tf and tf-idf dataframes on the 'features' column\n",
        "df = pd.merge(left = df_tf, right = df_tfidf)\n",
        "\n",
        "# Sort the combined dataframe by the 'norm_tfidf' column in descending order\n",
        "df.sort_values(by=[\"norm_tfidf\"],ascending=False, inplace = True)\n",
        "\n",
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1Jj6GfxCEo"
      },
      "source": [
        "**Observations from above results**\n",
        "- words 'frequency' and 'vectorizer' occurs 4 times in the document and hence term frequency is 4.\n",
        "- Word 'vectorizer' occurs in every document and hence idf is 1 (log(1) + 1).\n",
        "- norm_tfidf gives higher score to word 'frequency' than 'vectorizer'.\n",
        "- norm_tfidf is not equal to idf * tf\n",
        "\n",
        "Let us know understand how norm_tfidf is calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T19:08:38.389005Z",
          "iopub.status.busy": "2021-09-11T19:08:38.388843Z",
          "iopub.status.idle": "2021-09-11T19:08:38.392671Z",
          "shell.execute_reply": "2021-09-11T19:08:38.392237Z",
          "shell.execute_reply.started": "2021-09-11T19:08:38.388992Z"
        },
        "id": "-k9PB58_xCEo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# calculate tfidf (without any normalization)\n",
        "df['tfidf'] = df.eval('tf*idf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-12T03:13:47.640165Z",
          "iopub.status.busy": "2021-09-12T03:13:47.639897Z",
          "iopub.status.idle": "2021-09-12T03:13:47.651782Z",
          "shell.execute_reply": "2021-09-12T03:13:47.651488Z",
          "shell.execute_reply.started": "2021-09-12T03:13:47.640137Z"
        },
        "id": "Betig4AjxCEp",
        "outputId": "989f7dae-e3f4-4b62-ac84-400f372492cd",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>norm_tfidf</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>sq_tfidf</th>\n",
              "      <th>norm_tfidf_manually</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>frequency</td>\n",
              "      <td>4</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.733471</td>\n",
              "      <td>6.772589</td>\n",
              "      <td>45.867958</td>\n",
              "      <td>0.733471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>vectorizer</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.433200</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.433200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>this</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.216600</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>for</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.216600</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>is</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.216600</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>count</td>\n",
              "      <td>1</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.183368</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>2.866747</td>\n",
              "      <td>0.183368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>doesn</td>\n",
              "      <td>1</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.183368</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>2.866747</td>\n",
              "      <td>0.183368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>key</td>\n",
              "      <td>1</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>0.139456</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>1.658125</td>\n",
              "      <td>0.139456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>on</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>scoring</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>based</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>done</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>of</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>binary</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>presence</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>score</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>higher</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tfidf</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dummy</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>word</td>\n",
              "      <td>0</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      features  tf       idf  norm_tfidf     tfidf   sq_tfidf  \\\n",
              "7    frequency   4  1.693147    0.733471  6.772589  45.867958   \n",
              "18  vectorizer   4  1.000000    0.433200  4.000000  16.000000   \n",
              "17        this   2  1.000000    0.216600  2.000000   4.000000   \n",
              "6          for   2  1.000000    0.216600  2.000000   4.000000   \n",
              "9           is   2  1.000000    0.216600  2.000000   4.000000   \n",
              "2        count   1  1.693147    0.183368  1.693147   2.866747   \n",
              "3        doesn   1  1.693147    0.183368  1.693147   2.866747   \n",
              "10         key   1  1.287682    0.139456  1.287682   1.658125   \n",
              "12          on   1  1.000000    0.108300  1.000000   1.000000   \n",
              "15     scoring   1  1.000000    0.108300  1.000000   1.000000   \n",
              "0        based   1  1.000000    0.108300  1.000000   1.000000   \n",
              "4         done   1  1.000000    0.108300  1.000000   1.000000   \n",
              "11          of   0  1.693147    0.000000  0.000000   0.000000   \n",
              "1       binary   0  1.693147    0.000000  0.000000   0.000000   \n",
              "13    presence   0  1.693147    0.000000  0.000000   0.000000   \n",
              "14       score   0  1.693147    0.000000  0.000000   0.000000   \n",
              "8       higher   0  1.693147    0.000000  0.000000   0.000000   \n",
              "16       tfidf   0  1.693147    0.000000  0.000000   0.000000   \n",
              "5        dummy   0  1.693147    0.000000  0.000000   0.000000   \n",
              "19        word   0  1.693147    0.000000  0.000000   0.000000   \n",
              "\n",
              "    norm_tfidf_manually  \n",
              "7              0.733471  \n",
              "18             0.433200  \n",
              "17             0.216600  \n",
              "6              0.216600  \n",
              "9              0.216600  \n",
              "2              0.183368  \n",
              "3              0.183368  \n",
              "10             0.139456  \n",
              "12             0.108300  \n",
              "15             0.108300  \n",
              "0              0.108300  \n",
              "4              0.108300  \n",
              "11             0.000000  \n",
              "1              0.000000  \n",
              "13             0.000000  \n",
              "14             0.000000  \n",
              "8              0.000000  \n",
              "16             0.000000  \n",
              "5              0.000000  \n",
              "19             0.000000  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate tfidf - normalized\n",
        "df['sq_tfidf'] = df.eval('tfidf**2')\n",
        "df['norm_tfidf_manually'] = df['tfidf']/np.sqrt(df['sq_tfidf'].sum())\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1jKODj4iRNQ"
      },
      "source": [
        "## <font color = 'pickle'>**Modifying Vocab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeTUZZmJiccS"
      },
      "source": [
        "### <font color = 'pickle'>**Case sensitive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:08:17.486661Z",
          "iopub.status.busy": "2021-09-11T06:08:17.486532Z",
          "iopub.status.idle": "2021-09-11T06:08:17.490721Z",
          "shell.execute_reply": "2021-09-11T06:08:17.490336Z",
          "shell.execute_reply.started": "2021-09-11T06:08:17.486649Z"
        },
        "id": "2omkpfjJnZ_L",
        "outputId": "23ef7bac-d747-44a0-f962-d076bcede563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Count': 1,\n",
              " 'Vectorizer': 3,\n",
              " 'for': 8,\n",
              " 'this': 19,\n",
              " 'vectorizer': 20,\n",
              " 'scoring': 17,\n",
              " 'is': 11,\n",
              " 'done': 6,\n",
              " 'based': 4,\n",
              " 'on': 14,\n",
              " 'frequency': 9,\n",
              " 'For': 2,\n",
              " 'key': 12,\n",
              " 'doesn': 5,\n",
              " 'tfidf': 18,\n",
              " 'higher': 10,\n",
              " 'score': 16,\n",
              " 'Binary': 0,\n",
              " 'presence': 15,\n",
              " 'of': 13,\n",
              " 'word': 21,\n",
              " 'dummy': 7}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The lowercase argument is set to False to indicate that the text should \n",
        "# not be converted to lowercase before tokenizing. \n",
        "# The resulting vocab may have same word in upper and lower case\n",
        "vectorizer = CountVectorizer(lowercase=False)\n",
        "\n",
        "# we can use fit_transform to use fit() and transform() in one step\n",
        "vectors = vectorizer.fit_transform(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75w3LP5li7VM"
      },
      "source": [
        "### <font color = 'pickle'>**Filtering words based on frequency**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP_IKZG9jKvD"
      },
      "source": [
        "The `max_df`, `min_df`, and `max_features` parameters in the `CountVectorizer`` class control the feature selection for the resulting term frequency (tf) vectors.\n",
        "\n",
        "- `max_df`: This parameter sets the maximum threshold for the frequency of a term in the document collection. If a term has a document frequency (i.e., the number of documents that contain the term) higher than max_df, it will be ignored. <font color = 'dodgerblue' >**This parameter is used to filter out stop words (corpus specific) that appear in too many documents.** </font>\n",
        "\n",
        "- min_df: This parameter sets the minimum threshold for the frequency of a term in the document collection. If a term has a document frequency lower than min_df, it will be ignored.  <font color = 'dodgerblue' >**This parameter is used to filter out rare words that appear in too few documents.**\n",
        "\n",
        "- max_features: This parameter sets the maximum number of features (i.e., the maximum number of unique terms) that should be included in the resulting tf vectors. If the number of unique terms in the document collection is larger than max_features, the terms with the highest tf values will be kept and the others will be ignored.  <font color = 'dodgerblue' >**This parameter is used to reduce the dimensionality of the resulting tf vectors, which can help reduce the computational cost of downstream processing.**\n",
        "\n",
        "By using the max_df, min_df, and max_features parameters, you can control the feature selection process and determine the most informative terms to include in the tf vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:08:40.000725Z",
          "iopub.status.busy": "2021-09-11T06:08:40.000235Z",
          "iopub.status.idle": "2021-09-11T06:08:40.007239Z",
          "shell.execute_reply": "2021-09-11T06:08:40.006996Z",
          "shell.execute_reply.started": "2021-09-11T06:08:40.000670Z"
        },
        "id": "YxzFCRGEMCkm",
        "outputId": "82ab38bb-ce78-4522-9eb0-50e771c840ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'vectorizer': 8,\n",
              " 'for': 2,\n",
              " 'this': 7,\n",
              " 'scoring': 6,\n",
              " 'is': 3,\n",
              " 'done': 1,\n",
              " 'based': 0,\n",
              " 'on': 5,\n",
              " 'key': 4}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove rare words - remove words which appear in less than 2 documents\n",
        "vectorizer = CountVectorizer(min_df=2)\n",
        "vectorizer.fit(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:09:31.316083Z",
          "iopub.status.busy": "2021-09-11T06:09:31.315553Z",
          "iopub.status.idle": "2021-09-11T06:09:31.322432Z",
          "shell.execute_reply": "2021-09-11T06:09:31.322174Z",
          "shell.execute_reply.started": "2021-09-11T06:09:31.316023Z"
        },
        "id": "bHDT20dLxCEq",
        "outputId": "45f92cd1-58f0-40e8-c524-5aed6172c058"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'count': 1,\n",
              " 'frequency': 4,\n",
              " 'key': 6,\n",
              " 'doesn': 2,\n",
              " 'tfidf': 10,\n",
              " 'higher': 5,\n",
              " 'score': 9,\n",
              " 'binary': 0,\n",
              " 'presence': 8,\n",
              " 'of': 7,\n",
              " 'word': 11,\n",
              " 'dummy': 3}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove words which appear in more than 2 documents - remove corpus specific rare words\n",
        "vectorizer = CountVectorizer(max_df=2)\n",
        "vectorizer.fit(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:10:33.933882Z",
          "iopub.status.busy": "2021-09-11T06:10:33.933354Z",
          "iopub.status.idle": "2021-09-11T06:10:33.940572Z",
          "shell.execute_reply": "2021-09-11T06:10:33.940289Z",
          "shell.execute_reply.started": "2021-09-11T06:10:33.933825Z"
        },
        "id": "UW_UErB8xCEq",
        "outputId": "d361e529-0cf7-418a-d870-8d90b76d2495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'vectorizer': 4, 'for': 0, 'this': 3, 'is': 1, 'tfidf': 2}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# retain most frequent words only - retain top n words based on term frequency across corpus\n",
        "vectorizer = CountVectorizer(max_features=5)\n",
        "vectorizer.fit(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6mnf0ZHxCEq"
      },
      "source": [
        "### <font color = 'pickle'>**Stop Words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:14:28.793903Z",
          "iopub.status.busy": "2021-09-11T06:14:28.793490Z",
          "iopub.status.idle": "2021-09-11T06:14:28.800896Z",
          "shell.execute_reply": "2021-09-11T06:14:28.800576Z",
          "shell.execute_reply.started": "2021-09-11T06:14:28.793866Z"
        },
        "id": "XZ4dTc6YxCEq",
        "outputId": "bdfa06e7-c123-400c-d43a-1f449d084ea5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'vectorizer': 4, 'done': 1, 'based': 0, 'frequency': 2, 'tfidf': 3}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also specify list of stopwords to countvectorizer to get the feature without stopwords\n",
        "\n",
        "# Import libraries\n",
        "nltk_stop_words = nltk_stopwords.words('english')\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5, stop_words= nltk_stop_words)\n",
        "vectorizer.fit(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWwFq44kxCEq"
      },
      "source": [
        "### <font color = 'pickle'>**Custom Tokenizer and Preprocessor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrVX1WEtxCEq"
      },
      "source": [
        "#### <font color = 'pickle'>**nltk tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-12T03:16:09.705726Z",
          "iopub.status.busy": "2021-09-12T03:16:09.705542Z",
          "iopub.status.idle": "2021-09-12T03:16:09.711195Z",
          "shell.execute_reply": "2021-09-12T03:16:09.710860Z",
          "shell.execute_reply.started": "2021-09-12T03:16:09.705710Z"
        },
        "id": "XrmIi5seqljd",
        "outputId": "64524f07-cb71-489d-d9cf-72f0fad7d129",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abdul\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'count': 11,\n",
              " 'vectorizer': 28,\n",
              " '-': 4,\n",
              " 'for': 15,\n",
              " 'this': 27,\n",
              " ',': 3,\n",
              " 'scoring': 24,\n",
              " 'is': 18,\n",
              " 'done': 13,\n",
              " 'based': 9,\n",
              " 'on': 21,\n",
              " 'frequency': 16,\n",
              " '.': 5,\n",
              " 'key': 19,\n",
              " '@vectorizer': 8,\n",
              " '#frequency': 1,\n",
              " '@frequency': 7,\n",
              " 'doesn': 12,\n",
              " '’': 30,\n",
              " 't': 25,\n",
              " 'tfidf': 26,\n",
              " 'higher': 17,\n",
              " 'score': 23,\n",
              " '#tfidf': 2,\n",
              " 'binary': 10,\n",
              " 'presence': 22,\n",
              " 'of': 20,\n",
              " 'word': 29,\n",
              " 'dummy': 14,\n",
              " '#dummy': 0,\n",
              " '@dummy': 6}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can use custom tokenizer e.g. we can use nltk tweet tokenizer to get each tokens as feature\n",
        "\n",
        "# Create an instance of the TweetTokenizer class\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "# Initialize the CountVectorizer with the custom tokenizer\n",
        "# only works if analyzer = 'word'\n",
        "vectorizer = CountVectorizer(analyzer='word', tokenizer=tweet_tokenizer.tokenize)\n",
        "\n",
        "vectorizer.fit_transform(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMzV-qHwxCEr"
      },
      "source": [
        "#### <font color = 'pickle'>**spacy pre-processor and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-12T03:16:12.204316Z",
          "iopub.status.busy": "2021-09-12T03:16:12.204153Z",
          "iopub.status.idle": "2021-09-12T03:16:12.207192Z",
          "shell.execute_reply": "2021-09-12T03:16:12.206765Z",
          "shell.execute_reply.started": "2021-09-12T03:16:12.204303Z"
        },
        "id": "nmyYzK9AVzXw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def spacy_preprocessor(text):    \n",
        "\n",
        "    # Create spacy object\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # remove punctuations and get a list of tokens\n",
        "    filtered_text = [token.text for token in doc if not token.is_punct]\n",
        "\n",
        "    # join the processed tokens in to string\n",
        "    return \" \".join(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-12T03:16:12.863124Z",
          "iopub.status.busy": "2021-09-12T03:16:12.862607Z",
          "iopub.status.idle": "2021-09-12T03:16:12.872726Z",
          "shell.execute_reply": "2021-09-12T03:16:12.870950Z",
          "shell.execute_reply.started": "2021-09-12T03:16:12.863065Z"
        },
        "id": "YnFartqtWe6X",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Spacy Tokenizer\n",
        "def spacy_tokenizer(data):\n",
        "  doc=nlp(data)\n",
        "  return [token.text for token in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T20:36:02.024537Z",
          "iopub.status.busy": "2021-09-11T20:36:02.024028Z",
          "iopub.status.idle": "2021-09-11T20:36:02.043042Z",
          "shell.execute_reply": "2021-09-11T20:36:02.042678Z",
          "shell.execute_reply.started": "2021-09-11T20:36:02.024478Z"
        },
        "id": "S0GAbq-3WsT_",
        "outputId": "5f89de8a-8464-47ed-94e7-96836c40dac4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Count': 5,\n",
              " 'Vectorizer': 7,\n",
              " 'for': 12,\n",
              " 'this': 24,\n",
              " 'vectorizer': 25,\n",
              " 'scoring': 22,\n",
              " 'is': 15,\n",
              " 'done': 10,\n",
              " 'based': 8,\n",
              " 'on': 19,\n",
              " 'frequency': 13,\n",
              " 'For': 6,\n",
              " 'key': 16,\n",
              " '@vectorizer': 3,\n",
              " '@frequency': 2,\n",
              " 'does': 9,\n",
              " 'n’t': 17,\n",
              " 'tfidf': 23,\n",
              " '  ': 0,\n",
              " 'higher': 14,\n",
              " 'score': 21,\n",
              " 'Binary': 4,\n",
              " 'presence': 20,\n",
              " 'of': 18,\n",
              " 'word': 26,\n",
              " 'dummy': 11,\n",
              " '@dummy': 1}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# custom preprocessor and spacy tokenizer\n",
        "vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor , tokenizer=spacy_tokenizer, token_pattern=None)\n",
        "vectors=vectorizer.fit(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSSOPlCRK38o"
      },
      "source": [
        "#### <font color = 'pickle'>**custom preprocessor we created earlier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EcX3HWLFF2qU"
      },
      "outputs": [],
      "source": [
        "custom_preprocessor = cp.SpacyPreprocessor('en_core_web_sm',remove_stop=True, lemmatize=True, stemming=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BKUl2hjTI1Ig"
      },
      "outputs": [],
      "source": [
        "def spacy_preprocessor(text):   \n",
        "    filtered_text = custom_preprocessor.transform([text])\n",
        "    return \" \".join(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy5XeSb6GHwU",
        "outputId": "7a05bbf3-7772-4a08-edb5-5b80f9372fa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'count': 3,\n",
              " 'vectorizer': 12,\n",
              " 'scoring': 10,\n",
              " 'base': 1,\n",
              " 'frequency': 5,\n",
              " 'key': 7,\n",
              " 'tfidf': 11,\n",
              " '  ': 0,\n",
              " 'high': 6,\n",
              " 'score': 9,\n",
              " 'binary': 2,\n",
              " 'presence': 8,\n",
              " 'word': 13,\n",
              " 'dummy': 4}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# custom preprocessor and spacy tokenizer\n",
        "vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor , tokenizer=spacy_tokenizer, token_pattern=None)\n",
        "vectors=vectorizer.fit(Corpus)\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STiVvP5qxCEr"
      },
      "source": [
        "#### <font color = 'pickle'>**token patterns with regular expressions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:32:57.889450Z",
          "iopub.status.busy": "2021-09-11T06:32:57.888935Z",
          "iopub.status.idle": "2021-09-11T06:32:57.893152Z",
          "shell.execute_reply": "2021-09-11T06:32:57.892901Z",
          "shell.execute_reply.started": "2021-09-11T06:32:57.889434Z"
        },
        "id": "2xEae96cC5e3",
        "outputId": "618fe843-f601-49d8-de55-6098314f1c91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'count': 9,\n",
              " 'vectorizer': 28,\n",
              " '-': 3,\n",
              " 'for': 13,\n",
              " 'this': 27,\n",
              " 'vectorizer,': 29,\n",
              " 'scoring': 24,\n",
              " 'is': 17,\n",
              " 'done': 11,\n",
              " 'based': 7,\n",
              " 'on': 21,\n",
              " 'frequency.': 15,\n",
              " 'frequency': 14,\n",
              " 'key.': 19,\n",
              " '@vectorizer': 6,\n",
              " '#frequency': 1,\n",
              " '@frequency,': 5,\n",
              " 'doesn’t': 10,\n",
              " 'tfidf': 25,\n",
              " 'tfidf,': 26,\n",
              " 'higher': 16,\n",
              " 'score': 23,\n",
              " '#tfidf': 2,\n",
              " 'binary': 8,\n",
              " 'presence': 22,\n",
              " 'of': 20,\n",
              " 'word.': 30,\n",
              " 'dummy': 12,\n",
              " 'key': 18,\n",
              " '#dummy': 0,\n",
              " '@dummy': 4}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can pass regex to the argument token_pattern to get required pattern\n",
        "# whitespace tokenizer\n",
        "# This can be very useful if we have allready cleaned the text\n",
        "vectorizer = CountVectorizer(analyzer='word', token_pattern=r\"[\\S]+\")\n",
        "\n",
        "# Assign the encoded(transformed) vectors to a variable\n",
        "vectors = vectorizer.fit_transform(Corpus)\n",
        "\n",
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haBuVWSyxCEs"
      },
      "source": [
        "### <font color = 'pickle'>**ngrams**\n",
        "\n",
        "- Till now our features consists of single token. However, in some cases we may want to use sequence of tokens as features\n",
        "- Consider the following corpus\n",
        " 1. This item is good\n",
        " 2. This item is not good\n",
        "- Now  both the documents will have feature 'good' and 'not' will be an additional feature in document 2.\n",
        "- For applications like sentiment analysis - it might be a good idea to consider 'not good' as a single token.\n",
        "\n",
        "- We can use ngram_range(min_n, max_n) in CountVectorizer to create features that consists of sequence of words.\n",
        "\n",
        "- if we specify min_n = 2 and max_n = 3, we will get bigrams and trigrams as features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T06:47:29.553885Z",
          "iopub.status.busy": "2021-09-11T06:47:29.553741Z",
          "iopub.status.idle": "2021-09-11T06:47:29.559538Z",
          "shell.execute_reply": "2021-09-11T06:47:29.559122Z",
          "shell.execute_reply.started": "2021-09-11T06:47:29.553873Z"
        },
        "id": "Ix4jKILgDLxa",
        "outputId": "68311248-9535-463c-e3ba-b24914cda7ce",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features for text 1\n",
            "\n",
            "is not\n",
            "item is\n",
            "not good\n",
            "this item\n",
            "\n",
            "Features for text 2\n",
            "\n",
            "is terribly\n",
            "item is\n",
            "terribly good\n",
            "this item\n"
          ]
        }
      ],
      "source": [
        "min_n = 2\n",
        "max_n = 2\n",
        "\n",
        "# only works if analyzer = 'word'\n",
        "vectorizer1 = CountVectorizer(analyzer = 'word', ngram_range=(min_n, max_n))\n",
        "vectorizer2 = CountVectorizer(analyzer = 'word', ngram_range=(min_n, max_n))\n",
        "\n",
        "text1= [\"This item is not good\"]\n",
        "text2 = [\"This item is terribly good\"]\n",
        "\n",
        "# Fit the vectorizers to text\n",
        "vectorizer1.fit_transform(text1)\n",
        "vectorizer2.fit_transform(text2)\n",
        "\n",
        "features1 = vectorizer1.get_feature_names_out()\n",
        "features2 = vectorizer2.get_feature_names_out()\n",
        "\n",
        "print('Features for text 1\\n')\n",
        "for feature in features1:\n",
        "  print(feature)\n",
        "\n",
        "print(f'\\nFeatures for text 2\\n')\n",
        "for feature in features2:\n",
        "  print(feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eaVPcHeRXRl"
      },
      "source": [
        "## <font color = 'pickle'>**Example : IMDB Data set** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr_e08FxxCEs"
      },
      "source": [
        "### <font color = 'pickle'>**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T07:45:50.456908Z",
          "iopub.status.busy": "2021-09-11T07:45:50.456747Z",
          "iopub.status.idle": "2021-09-11T07:45:50.459467Z",
          "shell.execute_reply": "2021-09-11T07:45:50.459100Z",
          "shell.execute_reply.started": "2021-09-11T07:45:50.456895Z"
        },
        "id": "TnakQosgOsMY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Use train.csv of IMDB movie review data (we downloaded this in the last lecture)\n",
        "\n",
        "train_data = data/'train.csv'\n",
        "test_data = data/'test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T07:41:27.376086Z",
          "iopub.status.busy": "2021-09-11T07:41:27.375913Z",
          "iopub.status.idle": "2021-09-11T07:41:27.497930Z",
          "shell.execute_reply": "2021-09-11T07:41:27.497477Z",
          "shell.execute_reply.started": "2021-09-11T07:41:27.376071Z"
        },
        "id": "rLsuPS6lxCEs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !head -n 2 {str(train_data)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T07:53:35.249495Z",
          "iopub.status.busy": "2021-09-11T07:53:35.249349Z",
          "iopub.status.idle": "2021-09-11T07:53:35.584630Z",
          "shell.execute_reply": "2021-09-11T07:53:35.584212Z",
          "shell.execute_reply.started": "2021-09-11T07:53:35.249483Z"
        },
        "id": "2CgPhuOixCEs",
        "outputId": "2960f44d-168c-4cf4-8bc8-dddcb7c15c67",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Training data set is : (25000, 2)\n",
            "Shape of Test data set is : (25000, 2)\n",
            "\n",
            "Top five rows of Training data set:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I think this movie would be more enjoyable if ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Timeless musical gem, with Gene Kelly in top f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I just found the IMDb and searched this film a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It's a short movie from David Lynch with just ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I dug out from my garage some old musicals and...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Reviews  Labels\n",
              "0  I think this movie would be more enjoyable if ...       1\n",
              "1  Timeless musical gem, with Gene Kelly in top f...       1\n",
              "2  I just found the IMDb and searched this film a...       1\n",
              "3  It's a short movie from David Lynch with just ...       1\n",
              "4  I dug out from my garage some old musicals and...       1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Reading data\n",
        "train_df = pd.read_csv(train_data, index_col=0)\n",
        "test_df = pd.read_csv(test_data, index_col=0)\n",
        "print(f'Shape of Training data set is : {train_df.shape}')\n",
        "print(f'Shape of Test data set is : {test_df.shape}')\n",
        "print(f'\\nTop five rows of Training data set:\\n')\n",
        "train_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FDh2M0G9xCEs"
      },
      "source": [
        "### <font color = 'pickle'>**Generating Vocab** </font>\n",
        "- <font color = 'indianred'>**Vocab should be created only based on training dataset** </font>\n",
        "- We will generate vocab using CountVectorizer\n",
        "- <font color = 'indianred'>**Use fit_transform() on Training data set**. </font>\n",
        "- **Use only transform() on Test dataset**. This make sures that we generate vocab only based on training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T07:46:54.988708Z",
          "iopub.status.busy": "2021-09-11T07:46:54.988551Z",
          "iopub.status.idle": "2021-09-11T07:46:57.356248Z",
          "shell.execute_reply": "2021-09-11T07:46:57.355769Z",
          "shell.execute_reply.started": "2021-09-11T07:46:54.988695Z"
        },
        "id": "rudMW4B_UGVE",
        "outputId": "166e89c4-14d5-4ffe-b0d3-6e464d356a91"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
              "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
              "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
              "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
              "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
              "                            &#x27;itself&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
              "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
              "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
              "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
              "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
              "                            &#x27;itself&#x27;, ...])</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
              "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
              "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
              "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
              "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
              "                            'itself', ...])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize vectorizer\n",
        "nltk_stop_words = nltk_stopwords.words('english')\n",
        "bag_of_word = CountVectorizer(stop_words= nltk_stop_words)\n",
        "\n",
        "# Fit on training data\n",
        "bag_of_word.fit(train_df['Reviews'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T07:47:13.492919Z",
          "iopub.status.busy": "2021-09-11T07:47:13.492761Z",
          "iopub.status.idle": "2021-09-11T07:47:13.527237Z",
          "shell.execute_reply": "2021-09-11T07:47:13.526788Z",
          "shell.execute_reply.started": "2021-09-11T07:47:13.492905Z"
        },
        "id": "6DsUyK2LVCg4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# get feature names\n",
        "features = bag_of_word.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T07:47:20.015894Z",
          "iopub.status.busy": "2021-09-11T07:47:20.015734Z",
          "iopub.status.idle": "2021-09-11T07:47:20.019068Z",
          "shell.execute_reply": "2021-09-11T07:47:20.018602Z",
          "shell.execute_reply.started": "2021-09-11T07:47:20.015881Z"
        },
        "id": "yIDLMOzoxCEt",
        "outputId": "e4d6a06d-4b27-4397-b738-9fc8e46171d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "74704"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the legth of the vocab\n",
        "len(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxjTROQkxCEt"
      },
      "source": [
        "### <font color = 'pickle'>**Create vectors for reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T08:00:01.403847Z",
          "iopub.status.busy": "2021-09-11T08:00:01.403637Z",
          "iopub.status.idle": "2021-09-11T08:00:05.617042Z",
          "shell.execute_reply": "2021-09-11T08:00:05.616588Z",
          "shell.execute_reply.started": "2021-09-11T08:00:01.403825Z"
        },
        "id": "sUvgnrjZVJYn"
      },
      "outputs": [],
      "source": [
        "# Transform the training and test dataset \n",
        "bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)\n",
        "bow_vector_test = bag_of_word.transform(test_df['Reviews'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T08:00:08.442306Z",
          "iopub.status.busy": "2021-09-11T08:00:08.442071Z",
          "iopub.status.idle": "2021-09-11T08:00:08.446901Z",
          "shell.execute_reply": "2021-09-11T08:00:08.446266Z",
          "shell.execute_reply.started": "2021-09-11T08:00:08.442282Z"
        },
        "id": "6xk4M3NPxCEt",
        "outputId": "1466305b-4ac4-401e-e980-3247f76d7cf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<25000x74704 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2479678 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Shape of the matrix for train dataset\n",
        "bow_vector_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T08:00:56.304151Z",
          "iopub.status.busy": "2021-09-11T08:00:56.303619Z",
          "iopub.status.idle": "2021-09-11T08:00:56.314758Z",
          "shell.execute_reply": "2021-09-11T08:00:56.312719Z",
          "shell.execute_reply.started": "2021-09-11T08:00:56.304091Z"
        },
        "id": "_LALNShqxCEt",
        "outputId": "6fbf61ad-6ece-4ab2-94a3-579407239b95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<25000x74704 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2385031 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Shape of the matrix for test dataset\n",
        "bow_vector_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaUcb2LBXBhw"
      },
      "source": [
        "### <font color = 'pickle'>**Limit vocab using max_features**\n",
        "We got 25k rows with 78k+ features, but what if we want only top 5k features.\n",
        "We can do this by providing max_features parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T08:02:16.620779Z",
          "iopub.status.busy": "2021-09-11T08:02:16.620635Z",
          "iopub.status.idle": "2021-09-11T08:02:18.859959Z",
          "shell.execute_reply": "2021-09-11T08:02:18.859536Z",
          "shell.execute_reply.started": "2021-09-11T08:02:16.620764Z"
        },
        "id": "wWaQV1tkVM2L",
        "outputId": "469eccfe-cb98-49f8-cb93-c25c7adddcfe"
      },
      "outputs": [
        {
          "ename": "InvalidParameterError",
          "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'thereupon', 'show', 'least', 'should', 'can', 'without', 'top', 'whence', 'three', 'seeming', 'become', 'might', 'again', 'off', 'yourselves', 'moreover', 'keep', 'because', 'own', 'if', 'others', 'through', 'whole', 'latterly', '’d', 'various', 'such', \"'s\", 'anyhow', 'together', 'done', 'eight', 'seemed', 'which', 'his', 'serious', 'another', 're', 'twenty', 'most', 'themselves', \"'ve\", 'could', 'somewhere', 'being', 'five', 'already', \"'re\", 'more', 'hereby', 'now', 'along', 'himself', 'into', 'whose', 'ours', 'noone', 'alone', 'else', 'she', 'beside', 'to', 'did', 'full', 'nevertheless', 'am', 'please', 'among', 'last', 'anyway', 'do', 'whereby', 'about', 'were', 'what', 'except', 'once', 'well', 'often', 'few', 'hundred', 'front', 'may', 'yours', 'here', 'someone', 'her', 'neither', 'will', 'had', 'much', 'my', 'yet', 'those', 'fifty', 'wherein', '‘m', 'from', 'afterwards', 'first', 'one', 'due', 'therefore', 'really', 'never', 'get', 'although', 'beforehand', 'upon', 'take', 'whither', 'an', 'empty', 'third', '‘ll', 'used', 'ca', 'then', 'before', 'also', 'nobody', '‘d', 'either', \"'ll\", 'back', 'whatever', 'four', 'amount', 'make', 'bottom', 'who', 'each', 'with', 'elsewhere', 'nothing', 'anyone', 'above', 'almost', 'ever', 'see', 'next', 'whoever', 'anywhere', 'further', 'for', '‘ve', 'only', 'meanwhile', 'rather', 'me', 'some', 'against', 'out', 'whereupon', 'around', 'your', 'whether', 'nine', 'otherwise', 'perhaps', '’m', 'all', 'why', 'at', 'give', 'mine', 'latter', 'over', 'say', 'became', 'very', 'several', 'two', 'side', 'sometimes', 'n’t', 'every', 'than', 'after', 'itself', 'of', 'former', 'is', 'nowhere', 'yourself', 'in', 'beyond', 'been', 'part', 'always', 'down', 'must', 'toward', 'be', 'thence', 'however', 'too', 'so', 'these', 'somehow', 'seem', 'behind', 'none', '’ve', '’ll', 'eleven', 'forty', 'via', 'their', 'nor', 'he', 'towards', 'on', 'onto', 'a', 'whereas', 'call', 'n‘t', 'we', 'no', 'thereby', 'under', 'you', 'thereafter', 'hereupon', 'amongst', 'therein', 'since', 'everything', 'would', 'whereafter', 'name', 'how', 'not', 'even', 'ten', 'seems', 'it', 'its', 'them', 'anything', 'both', 'during', 'something', 'hence', 'are', 'mostly', 'hereafter', \"n't\", 'thus', 'him', 'cannot', 'throughout', 'our', 'quite', 'regarding', '’re', 'that', 'there', 'becomes', 'herein', 'ourselves', 'doing', 'when', 'sixty', 'between', 'has', 'within', 'whom', 'less', 'unless', 'up', 'hers', 'six', \"'d\", 'where', 'fifteen', 'made', 'go', 'any', '‘s', '‘re', 'becoming', 'was', 'by', 'formerly', 'per', 'enough', 'put', 'whenever', 'us', 'they', 'does', 'thru', 'while', 'herself', 'many', 'or', 'but', 'indeed', '’s', 'until', 'other', 'as', \"'m\", 'same', 'below', 'just', 'still', 'across', 'twelve', 'though', 'sometime', 'using', 'have', 'besides', 'move', 'the', 'namely', 'myself', 'everywhere', 'this', 'and', 'wherever', 'everyone', 'i'} instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[55], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m bag_of_word \u001b[39m=\u001b[39m CountVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m, stop_words\u001b[39m=\u001b[39m spacy_stop_words)  \u001b[39m# Max features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Fit on training data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m bag_of_word\u001b[39m.\u001b[39;49mfit(train_df[\u001b[39m'\u001b[39;49m\u001b[39mReviews\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n",
            "File \u001b[1;32mc:\\Users\\abdul\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, raw_documents, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1323\u001b[0m     \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \n\u001b[0;32m   1325\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[39m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   1339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\abdul\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1368\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1364\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[1;32m-> 1368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[0;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n",
            "File \u001b[1;32mc:\\Users\\abdul\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    574\u001b[0m     \u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \n\u001b[0;32m    576\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    582\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[0;32m    583\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    584\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[0;32m    585\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\abdul\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m constraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n",
            "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'thereupon', 'show', 'least', 'should', 'can', 'without', 'top', 'whence', 'three', 'seeming', 'become', 'might', 'again', 'off', 'yourselves', 'moreover', 'keep', 'because', 'own', 'if', 'others', 'through', 'whole', 'latterly', '’d', 'various', 'such', \"'s\", 'anyhow', 'together', 'done', 'eight', 'seemed', 'which', 'his', 'serious', 'another', 're', 'twenty', 'most', 'themselves', \"'ve\", 'could', 'somewhere', 'being', 'five', 'already', \"'re\", 'more', 'hereby', 'now', 'along', 'himself', 'into', 'whose', 'ours', 'noone', 'alone', 'else', 'she', 'beside', 'to', 'did', 'full', 'nevertheless', 'am', 'please', 'among', 'last', 'anyway', 'do', 'whereby', 'about', 'were', 'what', 'except', 'once', 'well', 'often', 'few', 'hundred', 'front', 'may', 'yours', 'here', 'someone', 'her', 'neither', 'will', 'had', 'much', 'my', 'yet', 'those', 'fifty', 'wherein', '‘m', 'from', 'afterwards', 'first', 'one', 'due', 'therefore', 'really', 'never', 'get', 'although', 'beforehand', 'upon', 'take', 'whither', 'an', 'empty', 'third', '‘ll', 'used', 'ca', 'then', 'before', 'also', 'nobody', '‘d', 'either', \"'ll\", 'back', 'whatever', 'four', 'amount', 'make', 'bottom', 'who', 'each', 'with', 'elsewhere', 'nothing', 'anyone', 'above', 'almost', 'ever', 'see', 'next', 'whoever', 'anywhere', 'further', 'for', '‘ve', 'only', 'meanwhile', 'rather', 'me', 'some', 'against', 'out', 'whereupon', 'around', 'your', 'whether', 'nine', 'otherwise', 'perhaps', '’m', 'all', 'why', 'at', 'give', 'mine', 'latter', 'over', 'say', 'became', 'very', 'several', 'two', 'side', 'sometimes', 'n’t', 'every', 'than', 'after', 'itself', 'of', 'former', 'is', 'nowhere', 'yourself', 'in', 'beyond', 'been', 'part', 'always', 'down', 'must', 'toward', 'be', 'thence', 'however', 'too', 'so', 'these', 'somehow', 'seem', 'behind', 'none', '’ve', '’ll', 'eleven', 'forty', 'via', 'their', 'nor', 'he', 'towards', 'on', 'onto', 'a', 'whereas', 'call', 'n‘t', 'we', 'no', 'thereby', 'under', 'you', 'thereafter', 'hereupon', 'amongst', 'therein', 'since', 'everything', 'would', 'whereafter', 'name', 'how', 'not', 'even', 'ten', 'seems', 'it', 'its', 'them', 'anything', 'both', 'during', 'something', 'hence', 'are', 'mostly', 'hereafter', \"n't\", 'thus', 'him', 'cannot', 'throughout', 'our', 'quite', 'regarding', '’re', 'that', 'there', 'becomes', 'herein', 'ourselves', 'doing', 'when', 'sixty', 'between', 'has', 'within', 'whom', 'less', 'unless', 'up', 'hers', 'six', \"'d\", 'where', 'fifteen', 'made', 'go', 'any', '‘s', '‘re', 'becoming', 'was', 'by', 'formerly', 'per', 'enough', 'put', 'whenever', 'us', 'they', 'does', 'thru', 'while', 'herself', 'many', 'or', 'but', 'indeed', '’s', 'until', 'other', 'as', \"'m\", 'same', 'below', 'just', 'still', 'across', 'twelve', 'though', 'sometime', 'using', 'have', 'besides', 'move', 'the', 'namely', 'myself', 'everywhere', 'this', 'and', 'wherever', 'everyone', 'i'} instead."
          ]
        }
      ],
      "source": [
        "# Limit Vocab size using Max features\n",
        "spacy_stop_words = nlp.Defaults.stop_words\n",
        "bag_of_word = CountVectorizer(max_features=5000, stop_words= spacy_stop_words)  # Max features\n",
        "\n",
        "# Fit on training data\n",
        "bag_of_word.fit(train_df['Reviews'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-11T08:02:57.092677Z",
          "iopub.status.busy": "2021-09-11T08:02:57.092551Z",
          "iopub.status.idle": "2021-09-11T08:03:01.788423Z",
          "shell.execute_reply": "2021-09-11T08:03:01.787845Z",
          "shell.execute_reply.started": "2021-09-11T08:02:57.092664Z"
        },
        "id": "rEq04frRXq_i"
      },
      "outputs": [],
      "source": [
        "# Transform the training and test dataset \n",
        "bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)\n",
        "bow_vector_test = bag_of_word.transform(train_df['Reviews'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-09-11T08:03:51.691500Z",
          "iopub.status.busy": "2021-09-11T08:03:51.691259Z",
          "iopub.status.idle": "2021-09-11T08:03:51.887702Z",
          "shell.execute_reply": "2021-09-11T08:03:51.887320Z",
          "shell.execute_reply.started": "2021-09-11T08:03:51.691476Z"
        },
        "id": "6EZxdHGaY2Ha",
        "outputId": "0609512d-e727-40c3-a4d3-dff9c83549ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4dbe4c0b-f379-4fc9-b382-f642e6bff8f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>13th</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>...</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youth</th>\n",
              "      <th>zero</th>\n",
              "      <th>zizek</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 5000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4dbe4c0b-f379-4fc9-b382-f642e6bff8f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4dbe4c0b-f379-4fc9-b382-f642e6bff8f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4dbe4c0b-f379-4fc9-b382-f642e6bff8f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       00  000  10  100  11  12  13  13th  14  15  ...  yesterday  york  \\\n",
              "0       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "1       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "2       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "3       0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "4       0    0   1    0   0   0   0     0   0   0  ...          0     0   \n",
              "...    ..  ...  ..  ...  ..  ..  ..   ...  ..  ..  ...        ...   ...   \n",
              "24995   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "24996   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "24997   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "24998   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "24999   0    0   0    0   0   0   0     0   0   0  ...          0     0   \n",
              "\n",
              "       young  younger  youth  zero  zizek  zombie  zombies  zone  \n",
              "0          0        0      0     0      0       0        0     0  \n",
              "1          0        0      0     0      0       0        0     0  \n",
              "2          0        0      0     0      0       0        0     0  \n",
              "3          0        0      0     0      0       0        0     0  \n",
              "4          0        0      0     0      0       0        0     0  \n",
              "...      ...      ...    ...   ...    ...     ...      ...   ...  \n",
              "24995      0        0      0     0      0       0        0     0  \n",
              "24996      2        0      0     0      0       0        0     0  \n",
              "24997      0        0      0     0      0       0        0     0  \n",
              "24998      0        0      0     0      0       0        0     0  \n",
              "24999      0        0      0     0      0       0        0     0  \n",
              "\n",
              "[25000 rows x 5000 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Document representation\n",
        "vocab = bag_of_word.get_feature_names_out()\n",
        "pd.DataFrame(bow_vector_train.toarray(), columns=vocab)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xtZUZxRsjFOZ",
        "bzV0Las4fygR",
        "aEeEQp3Pf8Kl",
        "pmUenp0Q4YDU",
        "kzKYz9DZ4YDU",
        "RTlvVAjziFYm",
        "whv4QgKPB0Gv",
        "lyhgUzi5CNgg",
        "u6kX6-YQCSuq",
        "mP8odLcIC66i",
        "9DwforYaDNsY",
        "OAPPPILCATX-",
        "dVlJmMtmhCYM",
        "nq8pvYnkxCEo",
        "z1jKODj4iRNQ",
        "UeTUZZmJiccS",
        "75w3LP5li7VM",
        "F6mnf0ZHxCEq",
        "OWwFq44kxCEq",
        "MrVX1WEtxCEq",
        "nMzV-qHwxCEr",
        "uSSOPlCRK38o",
        "STiVvP5qxCEr",
        "haBuVWSyxCEs",
        "wr_e08FxxCEs",
        "FDh2M0G9xCEs",
        "MxjTROQkxCEt",
        "uaUcb2LBXBhw"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
