{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulRauf96/NLP/blob/main/spacy_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Czf2260KKAvK"
      },
      "source": [
        "#  <font color = 'dodgerblue'> **Objective** </font>\n",
        "The first step in NLP projects is to clean the text. For example we might want to remove punctuations, white spaces etc. Futher we want to break our strings into tokens. This step is required as we want to lean the vector (number) representaion of the tokens that we can use in our models. Spacy is a very useful library which can help us in text cleaning and tokeinzation. In this notebook, you will understand the basics of the spacy library.\n",
        "\n",
        "After completing this notebook, you will be able to\n",
        "- Clean text using spacy\n",
        "- Create tokens using spacy\n",
        "- Extract Part of Speech Tags\n",
        "- Extract Named Entities"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XVtBZC3Z5W"
      },
      "source": [
        "#  <font color = 'dodgerblue'>**Install latest version of spaCy** </font>\n",
        "\n",
        "spaCy is a popular library for Natural Language Processing (NLP) in Python. It provides advanced NLP tools and pre-trained models for performing common NLP tasks such as tokenization, lemmatization, POS tagging, entity recognition, and dependency parsing.\n",
        "\n",
        "spaCy's NLP models are designed to be efficient and production-ready, which makes it a popular choice for NLP tasks in real-world applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Ohxr8GRscIpK"
      },
      "outputs": [],
      "source": [
        "# install spacy\n",
        "%pip install -U spacy -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLGAeZ7h_3Lq"
      },
      "source": [
        "The code `!pip install -U spacy -qq` is a shell command that uses the pip package manager to install the latest version of the spacy library.\n",
        "\n",
        "- The `!` at the beginning of the line indicates that this is a shell command, rather than a Python command. \n",
        "- The `pip install` command is used to install a package or library, \n",
        "- `-U` is an option that tells pip to upgrade to the latest version of the package if it is already installed.\n",
        "\n",
        "The `-qq` option at the end of the command is used to control the verbosity of the output. \n",
        "- The `-q` option is for quiet output, and the double `-qq` option is for even quieter output. With this option, pip will produce minimal output, only printing error messages, if any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZHQ4OwB3S4"
      },
      "source": [
        "#  <font color = 'dodgerblue'>**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "mzyN8ym07DCt"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18wEdQRp6_oi"
      },
      "source": [
        "UserWarning: Can't initialize NVML (Nvidia Management Library).\n",
        "\n",
        "NVML provides a uniform way to monitor and manage various NVIDIA GPU-related activities, such as monitoring GPU utilization, temperature, power, and other performance-related metrics. If the library is unable to initialize NVML, it means that it cannot access the NVIDIA GPU and perform the necessary operations.\n",
        "\n",
        "We are not using GPUs , hence we do not need to worry about the warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDgJ4PeEU-EE",
        "outputId": "317a80dd-f912-4418-89a2-ef50578b6f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.5.0\n"
          ]
        }
      ],
      "source": [
        "# check spaCy Verion\n",
        "print(spacy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6o-N-d5B-d_"
      },
      "source": [
        "#  <font color = 'dodgerblue'>**Sample Strings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "RtF6PxC6i7Kw"
      },
      "outputs": [],
      "source": [
        "# Sample String - Create a sample String\n",
        "\n",
        "text1 = \"\"\"China's capital is Beijing. \\n\\nBeijing is where we'll go. \\n\\nLet's travel to Hong Kong from Beijing. \\\n",
        "          \\n\\nA friend is pursuing his M.S from Beijing. \\n\\nBeijing is a cool place!!! :-P <3 #Awesome \\\n",
        "          \\n\\nA Rolex watch costs in the range of $3000.0 - $8000.0 in USA and China. \\n\\n@tompeter I'm \\ \n",
        "          \\n\\ngoing to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3 \\\n",
        "          for more info see: http://www.example_beijing.com! Ten is different from 10\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "hL3VOCmBZ8fe"
      },
      "outputs": [],
      "source": [
        "text2  = \"\"\"China's capital is Beijing. \\n\\nA Rolex watch costs in the range of $3000.0 - $8000.0 in USA\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3cSytbc3z7p"
      },
      "source": [
        "#  <font color = 'dodgerblue'>**White Space Tokenizers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI-hr_Lti7K6",
        "outputId": "e69d98a1-98d3-4d5a-9e97-7106dd2abda6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"China's\",\n",
              " 'capital',\n",
              " 'is',\n",
              " 'Beijing.',\n",
              " 'A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$3000.0',\n",
              " '-',\n",
              " '$8000.0',\n",
              " 'in',\n",
              " 'USA']"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Whitespace Tokenizer splits text across whitespaces\n",
        "text2.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8tGWjno5f04"
      },
      "source": [
        "#  <font color = 'dodgerblue'>**spaCy Basics**\n",
        "\n",
        "**spaCy** (https://spacy.io/) is an open-source Python library that parses and \"understands\" large volumes of text. Separate models are available that cater to specific languages (English, French, German, etc.).\n",
        "\n",
        "Models in spaCy for English Language as of release 3.0.0\n",
        "- **en_core_web_sm:** 11MB\n",
        "- **en_core_web_md:** 48MB\n",
        "- **en_core_web_lg:** 746MB\n",
        "<br><br>\n",
        "![picture](https://spacy.io/images/pipeline.svg)\n",
        "\n",
        "Picture Source :https://spacy.io/usage/processing-pipelines\n",
        "\n",
        "The first step in spaCy is to create an `nlp` object. The `nlp` object is a instance of a model and consists of various operations like tokenizaton, tagger, parser, ner etc (see figure above). When a text is passed through the object, it goes throught these operations. When creating an object ,we can disable the operations that we do not need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwzpMf35CQGz"
      },
      "source": [
        " ## <font color = 'dodgerblue'>**Download Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7WDFOLUCO0T",
        "outputId": "d9b9dc2f-75b4-48b5-fb2c-74d7917e64ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-01 18:02:31.500717: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-01 18:02:31.500815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-01 18:02:31.500851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-01 18:02:32.727239: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rbWF4oTCZW1"
      },
      "source": [
        " ## <font color = 'dodgerblue'>**Load Model**\n",
        "\n",
        "- `spacy.load` is a function that is used to load a pre-trained spaCy model.\n",
        "\n",
        "- The argument passed to the load function, `'en_core_web_sm'`, specifies which model to load. In this case, it's the small English model `en_core_web_sm`.\n",
        "\n",
        "- The result of the `load` function is assigned to the variable `nlp`. This variable now contains an instance of the spaCy NLP pipeline that is loaded with the pre-trained English model.\n",
        "\n",
        "Now, you can use the `nlp` variable to perform NLP tasks such as tokenization, part-of-speech tagging, entity recognition, and more, on text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "ePemPus6CblL"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnEnkOdDawGw"
      },
      "source": [
        "## <font color = 'dodgerblue'>**Check Pipelines**\n",
        "\n",
        "The spaCy NLP pipeline is a series of processing steps that are applied to the input text data. The pipeline components are responsible for tasks such as tokenization, part-of-speech tagging, named entity recognition, etc. Each component in the pipeline is named, and nlp.pipe_names returns a list of these names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUW3_NUDnO5U",
        "outputId": "48559aab-72fa-4b12-a16c-767a5fb6cd18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ],
      "source": [
        "# check pipelines\n",
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_X4H-hbakG"
      },
      "source": [
        "## <font color = 'dodgerblue'>**Disable Components not required**\n",
        "\n",
        "- `nlp.select_pipes` is a method that is used to select specific components of the NLP pipeline to be disabled (turned off) for processing.\n",
        "\n",
        "- The argument passed to the `select_pipes` method, `disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']`, is a list of component names that should be disabled. In this case, the components named `tok2vec`, `tagger`, `parser`, `attribute_ruler`, `lemmatizer`, and `ner` are all being disabled.\n",
        "\n",
        "- The result of the `select_pipes` method is assigned to the variable `disabled`. This variable now contains a list of the component names that were selected for disabling.\n",
        "\n",
        "By disabling specific components of the NLP pipeline, you can control which NLP tasks are performed on your text data and improve processing speed. For example, if you only need to perform tokenization, you can disable all other components to reduce processing time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "pEXllpsOnY49"
      },
      "outputs": [],
      "source": [
        "# disable all the components as we are going to use only tokenizer in this notebook\n",
        "disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlOCQxxtnrw4",
        "outputId": "031e7e85-8c95-47fe-ccc7-a6f61637117d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "# check the pipeline components\n",
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L17v9uKVwo91"
      },
      "source": [
        "# <font color = 'dodgerblue'>**Tokenization in spaCy**\n",
        "A Text is tokenized in spaCy when creating the Language processing pipeline nlp() object. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSBinQlyxpAf"
      },
      "source": [
        "![picture](https://spacy.io/images/tokenization.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRRhFhW_2Trr"
      },
      "source": [
        "Picture Source: https://spacy.io/usage/linguistic-features#how-tokenizer-works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyWTJcf6Ltg3"
      },
      "source": [
        "The algorithm below is taken from Tokenization part from spaCy's Documentation.  \n",
        "https://spacy.io/usage/linguistic-features#tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeLLMMNxg2bi"
      },
      "source": [
        "1. Iterate over space-separated substrings.\n",
        "2. Check whether we have an explicitly defined special case for this substring. If we do, use it.\n",
        "3. Look for a token match. If there is a match, stop processing and keep this token.\n",
        "4. Check whether we have an explicitly defined special case for this substring. If we do, use it.\n",
        "5. Otherwise, try to consume one prefix. If we consumed a prefix, go back to #3, so that the token match and special cases always get priority.\n",
        "6. If we didn’t consume a prefix, try to consume a suffix and then go back to #3.\n",
        "7. If we can’t consume a prefix or a suffix, look for a URL match.\n",
        "8. If there’s no URL match, then look for a special case.\n",
        "9. Look for “infixes” – stuff like hyphens etc. and split the substring into tokens on all infixes.\n",
        "10. Once we can’t consume any more of the string, handle it as a single token.\n",
        "11. Make a final pass over the text to check for special cases that include spaces or that were missed due to the incremental processing of affixes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcq_x_mMiNPx"
      },
      "source": [
        " ## <font color = 'dodgerblue'>**Create Doc Object**\n",
        "\n",
        "When we call nlp on a string, spaCy first tokenizes the text and creates a `Doc` object.\n",
        "\n",
        "A `Doc` object in spaCy is a processed representation of a text document. It is created when text is processed by the spaCy NLP pipeline, and contains the following attributes.\n",
        "\n",
        "- `text`: The original text of the document.\n",
        "\n",
        "- `ents`: A sequence of Span objects that represent named entities in the document.\n",
        "\n",
        "- `sents`: A sequence of Span objects that represent individual sentences in the document.\n",
        "\n",
        "- `vocab`: A reference to the vocabulary object used by the NLP pipeline.\n",
        "\n",
        "In addition to these attributes, `Doc` objects also have many other properties and methods that allow you to perform various NLP tasks, such as part-of-speech tagging, named entity recognition, and more.\n",
        "\n",
        "The `Doc` object is the starting point for many NLP tasks, and it is often used as the input to various spaCy components, such as the dependency parser, entity recognizer, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "W_V6pOrri7LV"
      },
      "outputs": [],
      "source": [
        "# creating a Doc object\n",
        "doc = nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfhUAFNxi7Ld",
        "outputId": "141c57a6-0d46-49f4-f956-44f1c5a453f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "China's capital is Beijing. \n",
              "\n",
              "A Rolex watch costs in the range of $3000.0 - $8000.0 in USA"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is99sNori7Ll",
        "outputId": "be8ae8dc-7389-470e-8274-dd29e6bea4ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the type of doc\n",
        "type(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ASWRteQfi-M"
      },
      "source": [
        "Code explanation: \n",
        "- The NLP pipeline object is passed a string of text as an argument: `nlp(text2)`.\n",
        "\n",
        "- The string of text is processed by each component in the NLP pipeline in the order specified by `nlp.pipe_names`.\n",
        "\n",
        "- The processed text is stored in a variable `doc`. The variable `doc` is a spaCy `Doc` object that contains the processed text and various attributes, such as tokenization, part-of-speech tags, named entities, etc.\n",
        "\n",
        "The result of the NLP processing is stored in the `doc` variable, which can then be used for further processing or analysis. For example, you can iterate over the tokens in the `doc` object and access the attributes of each token, such as its part-of-speech tag, lemma, or entity label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Qjbm4gzmUW",
        "outputId": "756765a3-f100-4c10-94b9-c54f4932250b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "China's capital is Beijing. \n",
            "\n",
            "A Rolex watch costs in the range of $3000.0 - $8000.0 in USA\n"
          ]
        }
      ],
      "source": [
        "print(doc.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeKuQp_JzuHG",
        "outputId": "7e174b2f-f263-4a1f-e111-6c29b617e652"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.vocab.Vocab at 0x7f733a2b0670>"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "304YOirRzxsO",
        "outputId": "cb4c7f5a-214d-402d-a599-3f50d0d23be0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.ents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6O-4jRw7oEv",
        "outputId": "6a64ad2d-5a9f-4701-edba-a83e53fb6c48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<generator at 0x7f733cc24ea0>"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "farHJxJ_Dhel"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Accessing text of the tokens**\n",
        "token is an object. We can acccess the text of the token using text attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDDbkA7wi7L9",
        "outputId": "0f0c826c-db3c-4bb0-9c0e-3f14f68d6010",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['China',\n",
              " \"'s\",\n",
              " 'capital',\n",
              " 'is',\n",
              " 'Beijing',\n",
              " '.',\n",
              " '\\n\\n',\n",
              " 'A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$',\n",
              " '3000.0',\n",
              " '-',\n",
              " '$',\n",
              " '8000.0',\n",
              " 'in',\n",
              " 'USA']"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[token.text for token in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vljZl9sfDZfp"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Compare spacy tokenizer with white space tokenizer**\n",
        "We can create tokenizer using Python split() function. In the last lecture we created tokenizer by splitting on non-alpha numeric characters. That gave us tokens separated by non-alphanumeric characters i.e. our tokens only have alpha numeric characters (words, numbers and underscores). We will now craete a white space tokenizer. i.e. it will split the string based on white spaces and create tokens.\n",
        "\n",
        "We wil compare this tokenizer with spacy's tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2wqquhMFXTD"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Example 1 (more and better tokens)**\n",
        "\n",
        "The Whitespace Tokenizer splits the words from whitespaces.\n",
        "\n",
        "The spaCy tokenizer splits the text into meaningful segments dependent on the language model that is used. \n",
        "\n",
        "It is apparent that spaCy is a better tokenizer, as it's tokens contain more than just words separated from whitespaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwLhE_pBi7MF",
        "outputId": "c1f47c8d-99dd-4ce8-efb8-e2a16a276d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"China's\", 'capital', 'is', 'Beijing.', 'Beijing', 'is', 'where', \"we'll\", 'go.', \"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing.', 'A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing.', 'Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome', 'A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$3000.0', '-', '$8000.0', 'in', 'USA', 'and', 'China.', '@tompeter', \"I'm\", '\\\\', 'going', 'to', 'buy', 'a', 'Rolexxxxxxxx', 'watch!!!', ':-D', '#happiness', '#rolex', '<3', 'for', 'more', 'info', 'see:', 'http://www.example_beijing.com!', 'Ten', 'is', 'different', 'from', '10']\n",
            "['China', \"'s\", 'capital', 'is', 'Beijing', '.', '\\n\\n', 'Beijing', 'is', 'where', 'we', \"'ll\", 'go', '.', '\\n\\n', 'Let', \"'s\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing', '.', '          \\n\\n', 'A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing', '.', '\\n\\n', 'Beijing', 'is', 'a', 'cool', 'place', '!', '!', '!', ':-P', '<3', '#', 'Awesome', '          \\n\\n', 'A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000.0', '-', '$', '8000.0', 'in', 'USA', 'and', 'China', '.', '\\n\\n', '@tompeter', 'I', \"'m\", '\\\\', '\\n          \\n\\n', 'going', 'to', 'buy', 'a', 'Rolexxxxxxxx', 'watch', '!', '!', '!', ':-D', '#', 'happiness', '#', 'rolex', '<3', '          ', 'for', 'more', 'info', 'see', ':', 'http://www.example_beijing.com', '!', 'Ten', 'is', 'different', 'from', '10']\n"
          ]
        }
      ],
      "source": [
        "doc1 = nlp(text1)\n",
        "# Whitespace Tokenizer\n",
        "print(text1.split())\n",
        "\n",
        "# spaCy Tokenizer\n",
        "print([token.text for token in doc1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Wi4Ewfi7MM",
        "outputId": "8c1c7e1a-029a-4837-df0f-7e5ef2de945c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "# No. of Tokens in Whitespace Tokenizer\n",
        "print(len(text1.split()))\n",
        "# No. of Tokens in spaCy's Tokenizer\n",
        "print(len([token.text for token in doc1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9-WzMA-FoG4"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Example 2**\n",
        "You can see that spacy recognizes % sy\n",
        "mbol and create a separate token for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvLyXUp7i7MU",
        "outputId": "dcb945d0-1063-4494-eff7-5be55dfaad7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['There', 'is', '20%', 'probaility', 'of', 'winning', 'a', 'lottery.']\n",
            "['There', 'is', '20', '%', 'probaility', 'of', 'winning', 'a', 'lottery', '.']\n"
          ]
        }
      ],
      "source": [
        "text3 = \"There is 20% probaility of winning a lottery.\"\n",
        "doc3 = nlp(text3)\n",
        "# Whitespace Tokenizer\n",
        "print(text3.split())\n",
        "\n",
        "# spaCy Tokenizer\n",
        "print([token.text for token in doc3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wNEEIhPF2wb"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Example 3**\n",
        "Spacy's tokenizer recognizes that m is the unit of distance (based on sentence and creates a separate token for it. It will not split random combination of numbers and alphabets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsNGne55i7Mb",
        "outputId": "9b4f0aad-89e2-41a1-9b3b-a64793c9c3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'walk', '10m', 'everyday.']\n",
            "['I', 'walk', '10', 'm', 'everyday', '.']\n",
            "[' ', 'What', 'is', '10o8iu']\n"
          ]
        }
      ],
      "source": [
        "# Another example measuring difference between Whitespace and spaCy tokenizers\n",
        "text4=\"I walk 10m everyday.\"\n",
        "doc4 = nlp(text4)\n",
        "# Whitespace Tokenizer\n",
        "print(text4.split())\n",
        "\n",
        "# spaCy Tokenizer\n",
        "print([token.text for token in doc4])\n",
        "\n",
        "text5 = \" What is 10o8iu\"\n",
        "doc5 = nlp(text5)\n",
        "# spaCy Tokenizer\n",
        "print([token.text for token in doc5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3shEsBmHDsf"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Example 4**\n",
        "It takes into acount special cases like C++, U.S.A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzwC7j9Si7Mi",
        "outputId": "10574f15-a782-4b99-dd17-b78e2e0bacb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Some', 'good', 'programming', 'languages', 'to', 'know', 'HTML,', 'CSS,', 'JavaScript,', 'C++,', 'and', 'Node.js.']\n",
            "['Some', 'good', 'programming', 'languages', 'to', 'know', 'HTML', ',', 'CSS', ',', 'JavaScript', ',', 'C++', ',', 'and', 'Node.js', '.']\n"
          ]
        }
      ],
      "source": [
        "text5=\"Some good programming languages to know HTML, CSS, JavaScript, C++, and Node.js.\"\n",
        "doc5 = nlp(text5)\n",
        "# Whitespace Tokenizer\n",
        "print(text5.split())\n",
        "\n",
        "# spaCy Tokenizer\n",
        "print([token.text for token in doc5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PtmgOHyHZZa"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Text Processing/Cleaning**\n",
        "Spacy's tokens have attributes which can be very useful in text cleaning. https://spacy.io/api/token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "7FJg3TnvY8Lw"
      },
      "outputs": [],
      "source": [
        "# let us check other attributes of token class\n",
        "att_doc1 ={'token': [token for token in doc1],\n",
        "          'token.idx': [token.idx for token in doc1],\n",
        "          'token.text': [token.text for token in doc1],\n",
        "          'token.is_alpha': [token.is_alpha for token in doc1],\n",
        "          'token.is_punct': [token.is_punct for token in doc1],\n",
        "          'token.is_space': [token.is_space for token in doc1],\n",
        "          'token.is_stop': [token.is_stop for token in doc1],\n",
        "          'token.like_num': [token.like_num for token in doc1],\n",
        "          'token.is_digit': [token.is_digit for token in doc1],\n",
        "          'token.like_url': [token.like_url for token in doc1],\n",
        "           'token.like_email': [token.like_url for token in doc1],\n",
        "          \n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yJwfSHax9Oc-",
        "outputId": "cebd4530-4a9e-4a84-b43f-d08645d1d179"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5147cfde-2105-4ce2-a796-aaa958cc181c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>token.idx</th>\n",
              "      <th>token.text</th>\n",
              "      <th>token.is_alpha</th>\n",
              "      <th>token.is_punct</th>\n",
              "      <th>token.is_space</th>\n",
              "      <th>token.is_stop</th>\n",
              "      <th>token.like_num</th>\n",
              "      <th>token.is_digit</th>\n",
              "      <th>token.like_url</th>\n",
              "      <th>token.like_email</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>China</td>\n",
              "      <td>0</td>\n",
              "      <td>China</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'s</td>\n",
              "      <td>5</td>\n",
              "      <td>'s</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>capital</td>\n",
              "      <td>8</td>\n",
              "      <td>capital</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>is</td>\n",
              "      <td>16</td>\n",
              "      <td>is</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beijing</td>\n",
              "      <td>19</td>\n",
              "      <td>Beijing</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Ten</td>\n",
              "      <td>437</td>\n",
              "      <td>Ten</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>is</td>\n",
              "      <td>441</td>\n",
              "      <td>is</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>different</td>\n",
              "      <td>444</td>\n",
              "      <td>different</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>from</td>\n",
              "      <td>454</td>\n",
              "      <td>from</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>10</td>\n",
              "      <td>459</td>\n",
              "      <td>10</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5147cfde-2105-4ce2-a796-aaa958cc181c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5147cfde-2105-4ce2-a796-aaa958cc181c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5147cfde-2105-4ce2-a796-aaa958cc181c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        token  token.idx token.text  token.is_alpha  token.is_punct  \\\n",
              "0       China          0      China            True           False   \n",
              "1          's          5         's           False           False   \n",
              "2     capital          8    capital            True           False   \n",
              "3          is         16         is            True           False   \n",
              "4     Beijing         19    Beijing            True           False   \n",
              "..        ...        ...        ...             ...             ...   \n",
              "95        Ten        437        Ten            True           False   \n",
              "96         is        441         is            True           False   \n",
              "97  different        444  different            True           False   \n",
              "98       from        454       from            True           False   \n",
              "99         10        459         10           False           False   \n",
              "\n",
              "    token.is_space  token.is_stop  token.like_num  token.is_digit  \\\n",
              "0            False          False           False           False   \n",
              "1            False           True           False           False   \n",
              "2            False          False           False           False   \n",
              "3            False           True           False           False   \n",
              "4            False          False           False           False   \n",
              "..             ...            ...             ...             ...   \n",
              "95           False           True            True           False   \n",
              "96           False           True           False           False   \n",
              "97           False          False           False           False   \n",
              "98           False           True           False           False   \n",
              "99           False          False            True            True   \n",
              "\n",
              "    token.like_url  token.like_email  \n",
              "0            False             False  \n",
              "1            False             False  \n",
              "2            False             False  \n",
              "3            False             False  \n",
              "4            False             False  \n",
              "..             ...               ...  \n",
              "95           False             False  \n",
              "96           False             False  \n",
              "97           False             False  \n",
              "98           False             False  \n",
              "99           False             False  \n",
              "\n",
              "[100 rows x 11 columns]"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(att_doc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWp36ssIEp_"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Extract only numbers and alphabets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2-4NG0KXZJY",
        "outputId": "e9b559f2-e79a-477e-86cd-4e604ca758dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['China',\n",
              " 'capital',\n",
              " 'is',\n",
              " 'Beijing',\n",
              " 'Beijing',\n",
              " 'is',\n",
              " 'where',\n",
              " 'we',\n",
              " 'go',\n",
              " 'Let',\n",
              " 'travel',\n",
              " 'to',\n",
              " 'Hong',\n",
              " 'Kong',\n",
              " 'from',\n",
              " 'Beijing',\n",
              " 'A',\n",
              " 'friend',\n",
              " 'is',\n",
              " 'pursuing',\n",
              " 'his',\n",
              " 'from',\n",
              " 'Beijing',\n",
              " 'Beijing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cool',\n",
              " 'place',\n",
              " 'Awesome',\n",
              " 'A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '3000.0',\n",
              " '8000.0',\n",
              " 'in',\n",
              " 'USA',\n",
              " 'and',\n",
              " 'China',\n",
              " 'I',\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxxxxxxx',\n",
              " 'watch',\n",
              " 'happiness',\n",
              " 'rolex',\n",
              " 'for',\n",
              " 'more',\n",
              " 'info',\n",
              " 'see',\n",
              " 'Ten',\n",
              " 'is',\n",
              " 'different',\n",
              " 'from',\n",
              " '10']"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extract only alphabets and numbers\n",
        "[token.text for token in doc1 if  (token.is_alpha or token.like_num)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opiBlWneISd3"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Remove punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTTDTLQDJkJM",
        "outputId": "3a4b517c-04d4-4d1f-86fc-ba2f1ddb13d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "China's capital is Beijing. \n",
            "\n",
            "A Rolex watch costs in the range of $3000.0 - $8000.0 in USA\n"
          ]
        }
      ],
      "source": [
        "print(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "S9Wlh-dIi3dV"
      },
      "outputs": [],
      "source": [
        "# crate doc object\n",
        "doc2 = nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XXxZO6IYaEKb",
        "outputId": "9c5b81f7-a5fe-42ba-c5f0-397247746ffa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"China 's capital is Beijing \\n\\n A Rolex watch costs in the range of $ 3000.0 $ 8000.0 in USA\""
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove punctuation\n",
        "\" \".join([token.text for token in doc2 if  not token.is_punct])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-rO3UDQIYSB"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Extract/Remove URLs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBCfP-UEZLdx",
        "outputId": "a16623d5-81fe-4121-8d76-893c917e23bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['https://colab.research.google.com/', 'utdallas.edu']"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extract urls\n",
        "text7 = 'my urls are https://colab.research.google.com/ and utdallas.edu '\n",
        "doc7 = nlp(text7)\n",
        "[token.text for token in doc7 if  token.like_url]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L3Dbeq6YIlsw",
        "outputId": "64fc3e15-963f-413f-eb47-a1fe664fd947"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my urls are and'"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove urls\n",
        "\" \".join([token.text for token in doc7 if not token.like_url])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3neuQHgiIcqI"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Extract/Remove emails**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTzBuT95ZWLR",
        "outputId": "a9d1912c-051f-43c7-ff19-b58088726786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['xyz@utdallas.edu', 'xyz@gmail.com']"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extract emails \n",
        "text8 = 'my email is xyz@utdallas.edu or xyz@gmail.com'\n",
        "doc8 = nlp(text8)\n",
        "[token.text for token in doc8 if  token.like_email]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wiKcjI8RJMiF",
        "outputId": "48517537-b9c6-4763-b77c-95cf44f75d03"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my email is or'"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\" \".join([token.text for token in doc8 if not token.like_email])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWvBlKxGL0DO"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Stopwords**\n",
        "\n",
        "# Stop words\n",
        "- Stop words are basically a set of most commonly used words in a language, for exampe, 'the', 'a', 'in', 'an' etc.\n",
        "- The stop words do not provide any contextual meaning to the text and are therefore sometimes removed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "7kLFH7IIfG_a"
      },
      "outputs": [],
      "source": [
        "#  The following paragraph has been taken from https://en.wikipedia.org/wiki/Regular_expression\n",
        "text9 = \"\"\"A regular expression (shortened as regex or regexp;[1] also referred to as rational expression[2][3]) is a sequence of characters that define a search pattern. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.\n",
        "The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.\n",
        "Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "Ekf20_b7fTyl"
      },
      "outputs": [],
      "source": [
        "doc9 = nlp(text9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tb44jB1fXDC"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Understanding Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "pxECRXBPfhPV"
      },
      "outputs": [],
      "source": [
        "# create tokens using spacy\n",
        "tokens = [token.text for token in doc9 if not token.is_punct]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "co9ue8_Efo5I"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "3HXBSlOTfiAF"
      },
      "outputs": [],
      "source": [
        "# create a counter object based on tokens obtained \n",
        "# A Counter is a class containing dict objects that is used to count hashable objects\n",
        "# Counter contains elements as keys in a dictionary and their counts as the values for the respective keys.\n",
        "\n",
        "counter = Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bol2sqh6fwXz",
        "outputId": "9c37e415-cc74-40e6-a8a4-61e627b22b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'and': 7, 'in': 6, 'the': 6, 'or': 4, 'a': 4, 'regular': 3, 'as': 3, 'of': 3, 'search': 3, 'used': 3, 'for': 3, 'text': 3, 'regex': 2, 'is': 2, 'such': 2, 'are': 2, 'find': 2, 'replace': 2, 'language': 2, '\\n': 2, 'The': 2, 'concept': 2, 'processing': 2, 'utilities': 2, 'expressions': 2, 'being': 2, 'A': 1, 'expression': 1, 'shortened': 1, 'regexp;[1': 1, 'also': 1, 'referred': 1, 'to': 1, 'rational': 1, 'expression[2][3': 1, 'sequence': 1, 'characters': 1, 'that': 1, 'define': 1, 'pattern': 1, 'Usually': 1, 'patterns': 1, 'by': 1, 'string': 1, 'searching': 1, 'algorithms': 1, 'operations': 1, 'on': 1, 'strings': 1, 'input': 1, 'validation': 1, 'It': 1, 'technique': 1, 'developed': 1, 'theoretical': 1, 'computer': 1, 'science': 1, 'formal': 1, 'theory': 1, 'arose': 1, '1950s': 1, 'when': 1, 'American': 1, 'mathematician': 1, 'Stephen': 1, 'Cole': 1, 'Kleene': 1, 'formalized': 1, 'description': 1, 'came': 1, 'into': 1, 'common': 1, 'use': 1, 'with': 1, 'Unix': 1, 'Different': 1, 'syntaxes': 1, 'writing': 1, 'have': 1, 'existed': 1, 'since': 1, '1980s': 1, 'one': 1, 'POSIX': 1, 'standard': 1, 'another': 1, 'widely': 1, 'Perl': 1, 'syntax': 1, 'Regular': 1, 'engines': 1, 'dialogs': 1, 'word': 1, 'processors': 1, 'editors': 1, 'sed': 1, 'AWK': 1, 'lexical': 1, 'analysis': 1, 'Many': 1, 'programming': 1, 'languages': 1, 'provide': 1, 'capabilities': 1, 'either': 1, 'built': 1, 'via': 1, 'libraries': 1})\n"
          ]
        }
      ],
      "source": [
        "# print counter\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZzfJeXqfw2O",
        "outputId": "56f6730b-7aed-4a65-bde3-dd802f1f52d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('and', 7),\n",
              " ('in', 6),\n",
              " ('the', 6),\n",
              " ('or', 4),\n",
              " ('a', 4),\n",
              " ('regular', 3),\n",
              " ('as', 3),\n",
              " ('of', 3),\n",
              " ('search', 3),\n",
              " ('used', 3)]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Counter class contains class methods that provide useful info using the count of elements.\n",
        "counter.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8czdjt1gB0W"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Stop Words with Spacy**\n",
        "- Each model in Spacy has default list of stopwords. \n",
        "- You can check that using model.Defaults.stop_words. \n",
        "- You can also check whether a particular word is a stopword. \n",
        "- Further, you can modify the default list of stopwords. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_HB0d-wgFu9",
        "outputId": "79076a2d-7880-4a4c-dc6d-b281a4b6fab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'elsewhere', 'me', 'again', 'within', 'latter', 'be', 'was', 'both', 'where', 'always', 'hence', '‘re', 'them', 'should', 'against', 'four', 'thus', 'you', 'and', 'full', 'everywhere', 'yours', '’re', 'is', 'another', 'make', 'hundred', 'whence', 'anywhere', 'everything', 'side', 'between', 'such', 'thence', 'perhaps', 'she', 'nobody', 'become', 'does', 'various', 'serious', 'would', 'becoming', 'in', 'somehow', 'your', 'how', 'rather', 'therein', 'see', 'could', 'neither', 'just', 'used', 'back', 'were', 'anyone', 'several', 'done', 'whereupon', 'own', 'thru', 'ca', 'they', 'part', 'call', 'whose', 'mostly', 'with', 'most', 'those', 'almost', 'show', 'along', 'throughout', 'further', 'too', 'behind', 'hereupon', 'myself', 'into', 'upon', 'are', 'quite', 'very', 'regarding', 'being', 'there', 'latterly', 'what', 'anyhow', 'third', '‘d', 'then', 'using', 'via', 'yet', 'this', 'six', 'cannot', 'move', 'empty', 'less', 'here', 'not', 'will', 'than', 'toward', 'per', 'except', 'please', 'others', 'last', 'ourselves', 'his', 'five', 'more', 'herein', 'so', 'give', 'during', 'when', 'from', 'one', 'enough', 'somewhere', \"'d\", 'indeed', 'together', 'might', 'because', 'why', 'our', 'whatever', 'am', 'alone', 'its', 'we', 'noone', 'keep', 'however', 'themselves', 'get', 'if', 'never', 'wherein', 'has', 'these', 'may', '‘ll', '’ll', 'whoever', 'as', 'mine', 'front', 'twelve', 'until', 'himself', 'else', 'some', 'ever', 'whereafter', '‘s', 'former', 'fifty', 'herself', 'anything', 'whole', 'now', 'eight', 'sometime', 'yourselves', 'my', 'to', '’d', 'becomes', 'bottom', 'once', '’s', 'sometimes', 'that', \"'re\", 'really', 'across', 'whom', 'sixty', 'wherever', 'either', 'n‘t', 'thereby', 'unless', 'beforehand', 'seems', 'who', 'amount', 'their', 'amongst', 'above', 'whether', 'much', 'for', 'eleven', 'none', 'do', 'only', 'other', 'around', 'hereafter', 'all', 'beside', 'next', 'of', 'the', 'after', 'therefore', \"'m\", 'first', 'nothing', 'anyway', 'seem', 'an', '’ve', 'least', 'on', 'over', 'say', 'besides', 'since', 'go', 'before', 'n’t', 'towards', 'hers', 'through', 'few', \"'ll\", 'ten', 'whither', 'can', 'even', 'many', 'otherwise', 'at', 'out', 'also', 'made', 'i', 'nevertheless', 'name', 'take', 'us', 'whenever', 'top', 'it', 'below', '’m', '‘m', 'no', 'often', 'something', 'thereafter', 'ours', 'must', 'formerly', 'three', 'though', 'he', 'down', 'been', 'under', 'her', 'whereby', 'already', 'about', 'same', '‘ve', 'everyone', \"'s\", 'well', 'by', 'or', 'beyond', 'namely', 'onto', 'have', 'had', 'became', 'whereas', 'nor', 'due', 'him', 'twenty', 'hereby', 'itself', \"'ve\", 'but', 'meanwhile', 'which', 'seeming', 'up', 'doing', 'any', 'while', 'fifteen', 'yourself', 'nine', 'nowhere', 'among', 'someone', 'without', 'seemed', 'did', 'thereupon', 'two', 'every', 'moreover', 'forty', 'although', 'off', 're', 'afterwards', 'put', 'a', 'each', \"n't\", 'still'}\n"
          ]
        }
      ],
      "source": [
        "# default stopwords from the loaded model in spaCy\n",
        "# the stopwords will change with the librray we import\n",
        "print(nlp.Defaults.stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8aRa8yqgLVU",
        "outputId": "aaf68a4b-ad87-4013-a9a5-f054b1886c88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nlp.Defaults.stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjd70KhJhmc4",
        "outputId": "fb4ad84e-fc00-4a45-9d91-a92364fb65cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To check whether word regular is in default stop words\n",
        "'regular' in nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eXMQdHphnIY",
        "outputId": "f0e3be54-f35b-4f53-f0ab-d29d68e6840e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# modify spacy's default stop words; \n",
        "# add regular as stopwords\n",
        "nlp.Defaults.stop_words.add('regular')\n",
        "'regular' in nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv6xl-k9hqyp",
        "outputId": "d3ddb577-48e1-4dd9-e704-213823294732"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now let us modify the default words again \n",
        "# remove regular from default stop words\n",
        "nlp.Defaults.stop_words.remove('regular')\n",
        "'regular' in nlp.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXXolZuTiOKE"
      },
      "source": [
        "  ## <font color = 'dodgerblue'>**Remove stop words from text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "GVvehQKchvQK"
      },
      "outputs": [],
      "source": [
        "tokens = [ token.text for token in doc9  if not (token.is_stop or token.is_punct)]\n",
        "text9_clean = \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD_5FP5OieEH",
        "outputId": "f7173b3c-883a-4471-ef3f-7e5821529844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "regular expression shortened regex regexp;[1 referred rational expression[2][3 sequence characters define search pattern Usually patterns string searching algorithms find find replace operations strings input validation technique developed theoretical computer science formal language theory \n",
            " concept arose 1950s American mathematician Stephen Cole Kleene formalized description regular language concept came common use Unix text processing utilities Different syntaxes writing regular expressions existed 1980s POSIX standard widely Perl syntax \n",
            " Regular expressions search engines search replace dialogs word processors text editors text processing utilities sed AWK lexical analysis programming languages provide regex capabilities built libraries\n"
          ]
        }
      ],
      "source": [
        "print(text9_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "mhVaOmz0i6XI"
      },
      "outputs": [],
      "source": [
        "counter = Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkhgekZljAAi",
        "outputId": "08915b9f-bc95-4e1f-a8b4-434c525cc282"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('regular', 3),\n",
              " ('search', 3),\n",
              " ('text', 3),\n",
              " ('regex', 2),\n",
              " ('find', 2),\n",
              " ('replace', 2),\n",
              " ('language', 2),\n",
              " ('\\n', 2),\n",
              " ('concept', 2),\n",
              " ('processing', 2)]"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counter.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBje2fG0egJW"
      },
      "source": [
        "  ## <font color = 'dodgerblue'> **Lammetization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHl8EWXue9EH"
      },
      "source": [
        "<img src =\"https://drive.google.com/uc?export=view&id=1zk5L9vyg6LlTW8nCZh-YxBOyU-IinShN\" width = 500>\n",
        "\n",
        "image source: https://spacy.io/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsxNLGqyemn8"
      },
      "source": [
        "- For Lammetization we need POS and for POS we need `['tagger', 'attribute_ruler' , tok2vec]`\n",
        "- Hence for lammetization we can disable  `['ner', 'parser']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBOLsAXvfrHM",
        "outputId": "94079c99-c1fb-41ff-d0a7-9234872300b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "8NNey3IQflhz"
      },
      "outputs": [],
      "source": [
        "# restore pipelines comonents\n",
        "disabled.restore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTU_-dsYftEo",
        "outputId": "dac5506e-a282-449d-8d51-a67b617dc7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ],
      "source": [
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Xhbp76CUfvJv"
      },
      "outputs": [],
      "source": [
        "disabled = nlp.select_pipes(disable= ['ner', 'parser'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZk_7xfEgI1C",
        "outputId": "20f4d671-3846-41ba-d393-ff51de46dcf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer']\n"
          ]
        }
      ],
      "source": [
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "z0eNtqssgcAS"
      },
      "outputs": [],
      "source": [
        "# We will look at lemmatizing on a small part of the text\n",
        "text10 =\" A regular expression also referred to as rational expression is a sequence of characters that define a search pattern\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "UrfZ-1-TguB6"
      },
      "outputs": [],
      "source": [
        "doc10=nlp(text10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "obRnprZeg9px"
      },
      "outputs": [],
      "source": [
        "# Lemmatizing the text\n",
        "lemmas= [token.lemma_ for token in doc10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0c64-VihCuo",
        "outputId": "d1aaba80-74f2-4eaf-8f1f-47083ad35250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lemmatized :   a regular expression also refer to as rational expression be a sequence of character that define a search pattern\n",
            "original   :  A regular expression also referred to as rational expression is a sequence of characters that define a search pattern\n"
          ]
        }
      ],
      "source": [
        "print(f'lemmatized : {\" \".join(lemmas)}')\n",
        "print(f'original   : {text10}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPv6PbcB6n-H"
      },
      "source": [
        "# <font color = 'dodgerblue'>**Sentence tokenization using spacy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "PiReNVEPofkT"
      },
      "outputs": [],
      "source": [
        "disabled.restore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "gZ6vK4u5ojLb"
      },
      "outputs": [],
      "source": [
        "doc2 = nlp(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v2yerQx6lHW",
        "outputId": "c43e56dc-d328-42c0-f1b2-3d56559716df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"China's capital is Beijing. \\n\\n\",\n",
              " 'A Rolex watch costs in the range of $3000.0 - $8000.0 in USA']"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We use doc.sents to tokenize sentences\n",
        "sentences = [sent.text for sent in doc2.sents]\n",
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incarAQAzYZH"
      },
      "source": [
        "# <font color = 'dodgerblue'>**Name Entity Recognition (NER)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "JpJACyiXzbYE"
      },
      "outputs": [],
      "source": [
        "disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSeRI_eFzpIk",
        "outputId": "b02afeed-a20e-4b03-8bdd-bd19d99cab0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ner']\n"
          ]
        }
      ],
      "source": [
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-O50Pijzstk",
        "outputId": "c4ea4030-cecb-41d5-b7e3-8e1bab177ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity              : Tag\n",
            "\n",
            "China                : GPE\n",
            "Beijing              : GPE\n",
            "Beijing              : GPE\n",
            "Hong Kong            : GPE\n",
            "Beijing              : GPE\n",
            "M.S                  : ORG\n",
            "Beijing              : GPE\n",
            "Beijing              : GPE\n",
            "Rolex                : PERSON\n",
            "$3000.0 - $          : MONEY\n",
            "8000.0               : MONEY\n",
            "USA                  : GPE\n",
            "China                : GPE\n",
            "Rolexxxxxxxx         : NORP\n",
            "http://www.example_beijing.com : PERSON\n",
            "Ten                  : CARDINAL\n",
            "10                   : CARDINAL\n"
          ]
        }
      ],
      "source": [
        "doc1 = nlp(text1)\n",
        "print(f'{\"Entity\":<20}: Tag\\n')\n",
        "for entity in doc1.ents:\n",
        "  print(f'{entity.text:<20} : {entity.label_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "49_YSB950cBh",
        "outputId": "ae7e608b-0199-4f2a-d494-651abf2ce749"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Countries, cities, states'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Nationalities or religious or political groups'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Numerals that do not fall under another type'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(spacy.explain('GPE'))\n",
        "display(spacy.explain('NORP'))\n",
        "spacy.explain('CARDINAL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "ANqtbmjU05AL"
      },
      "outputs": [],
      "source": [
        "# You can use displacy to visualize the named entities\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-OEWQiPB1Dgl",
        "outputId": "9dd9fffe-7d19-483d-f01d-4bec7a6ff17a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    China\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "'s capital is \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beijing\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". </br></br>\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beijing\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is where we'll go. </br></br>Let's travel to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hong Kong\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beijing\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".           </br></br>A friend is pursuing his \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    M.S\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beijing\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". </br></br>\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beijing\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is a cool place!!! :-P &lt;3 #Awesome           </br></br>A \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Rolex\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " watch costs in the range of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $3000.0 - $\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    8000.0\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    USA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    China\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". </br></br>@tompeter I'm \\ </br>          </br></br>going to buy a \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Rolexxxxxxxx\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " watch!!! :-D #happiness #rolex &lt;3           for more info see: \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    http://www.example_beijing.com\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "! \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ten\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " is different from \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    10\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(doc1,style='ent',jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "hdMPbtzw05pQ"
      },
      "outputs": [],
      "source": [
        "# text taken from https://oilprice.com/Energy/Oil-Prices/Oil-Rally-Continues-On-Bright-US-Economic-Data.html on June23 2021.\n",
        "# Defining String\n",
        "text11 = \"\"\"\n",
        "Oil prices rose early on Wednesday, driven by brighter economic prospects for the United States and continued recovery in oil demand in America and elsewhere in the world.\n",
        "As of 9:04 a.m. EDT on Wednesday, ahead of the weekly inventory report by the U.S. Energy Information Administration (EIA), WTI Crude was up 1.04 percent at $73.61, \n",
        "and Brent Crude traded at $75.54, up by 0.99 percent on the day.Prices found support late on Tuesday after the American Petroleum Institute (API) \n",
        "reported a draw in crude oil inventories of 7.199 million barrels for the week ending June 18. If the EIA confirms a draw today, it would be the fifth consecutive week of crude inventory draws in the United States, where demand for fuels continues to grow.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "02vppqSI1N4L"
      },
      "outputs": [],
      "source": [
        "# Creating doc object\n",
        "doc11 = nlp(text11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JczOtRPA1Tj-",
        "outputId": "37bae30c-1c00-4eeb-d5a5-1b859344d21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity                                        : Tag\n",
            "\n",
            "Wednesday                                     : DATE\n",
            "the United States                             : GPE\n",
            "America                                       : GPE\n",
            "9:04 a.m. EDT                                 : TIME\n",
            "Wednesday                                     : DATE\n",
            "weekly                                        : DATE\n",
            "the U.S. Energy Information Administration    : ORG\n",
            "EIA                                           : ORG\n",
            "1.04 percent                                  : PERCENT\n",
            "73.61                                         : MONEY\n",
            "Brent Crude                                   : ORG\n",
            "75.54                                         : MONEY\n",
            "0.99 percent                                  : PERCENT\n",
            "the day                                       : DATE\n",
            "Tuesday                                       : DATE\n",
            "the American Petroleum Institute              : ORG\n",
            "API                                           : ORG\n",
            "7.199 million barrels                         : QUANTITY\n",
            "the week ending June 18                       : DATE\n",
            "EIA                                           : ORG\n",
            "today                                         : DATE\n",
            "the fifth consecutive week                    : DATE\n",
            "the United States                             : GPE\n"
          ]
        }
      ],
      "source": [
        "# doc.ents give us the named entities\n",
        "# We can use entity.text and entity.label_ to get the entities and their tags\n",
        "print(f'{\"Entity\":<45} : Tag\\n')\n",
        "for entity in doc11.ents:\n",
        "  print(f'{entity.text:<45} : {entity.label_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vDH9OBGX1Z_y",
        "outputId": "c6eabb47-a7ed-4376-dea5-bd23ac09e9ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>Oil prices rose early on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Wednesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", driven by brighter economic prospects for \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the United States\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and continued recovery in oil demand in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    America\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and elsewhere in the world.</br>As of \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    9:04 a.m. EDT\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Wednesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", ahead of the \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    weekly\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " inventory report by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the U.S. Energy Information Administration\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    EIA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "), WTI Crude was up \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1.04 percent\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " at $\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    73.61\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ", </br>and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Brent Crude\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " traded at $\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    75.54\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ", up by \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    0.99 percent\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the day\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".Prices found support late on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tuesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " after \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the American Petroleum Institute\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    API\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ") </br>reported a draw in crude oil inventories of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    7.199 million barrels\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
              "</mark>\n",
              " for \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the week ending June 18\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". If the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    EIA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " confirms a draw \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    today\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", it would be \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the fifth consecutive week\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " of crude inventory draws in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the United States\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", where demand for fuels continues to grow.</br></div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# You can use displacy to visualize the named entities\n",
        "displacy.render(doc11,style='ent',jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt2gZ_Im3KZY"
      },
      "source": [
        "# <font color = 'dodgerblue'>**Part of Speech Tagging**\n",
        "\n",
        "<img src =\"https://drive.google.com/uc?export=view&id=1zk5L9vyg6LlTW8nCZh-YxBOyU-IinShN\" width = 500>\n",
        "\n",
        "image source: https://spacy.io/models\n",
        "\n",
        "- For POS we need `['tagger', 'attribute_ruler' , tok2vec]`\n",
        "-The POS tags come from rules that map token.tag to token.pos in (see mapping here https://spacy.io/api/annotation) the attribute_ruler component.\n",
        "- If the dependency parse is available, there are more specific rules it can apply related to AUX and VERB. \n",
        "- The mapping is hard to do perfectly because the token.tag (PTB tags) that come from the tagger don't make an aux/verb distinction at all.\n",
        "- Hence for POS we can disable `['lemmatizer', 'ner']`\n",
        "\n",
        "source: https://stackoverflow.com/questions/69313960/does-spacys-version3-1-pos-tagger-depends-on-parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM83lPHy3qPI",
        "outputId": "8320632a-9a87-44a4-ea7f-ca540520f4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ],
      "source": [
        "disabled.restore()\n",
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWAQyD7v3ttn",
        "outputId": "aa8f716e-3042-42ec-929c-27e83be72bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler']\n"
          ]
        }
      ],
      "source": [
        "disabled = nlp.select_pipes(disable= ['ner','lemmatizer'])\n",
        "print(nlp.pipe_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1sqN0-Rjkvo",
        "outputId": "98ec5cd3-099b-4d12-b7f3-f8e15f08f419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A               -> DET        -> DT        \n",
            "regular         -> ADJ        -> JJ        \n",
            "expression      -> NOUN       -> NN        \n",
            "(               -> PUNCT      -> -LRB-     \n",
            "shortened       -> VERB       -> VBN       \n",
            "as              -> ADP        -> IN        \n",
            "regex           -> NOUN       -> NN        \n",
            "or              -> CCONJ      -> CC        \n",
            "regexp;[1       -> PROPN      -> NNP       \n",
            "]               -> PUNCT      -> -RRB-     \n",
            "also            -> ADV        -> RB        \n",
            "referred        -> VERB       -> VBD       \n",
            "to              -> ADP        -> IN        \n",
            "as              -> ADP        -> IN        \n",
            "rational        -> ADJ        -> JJ        \n",
            "expression[2][3 -> PROPN      -> NNP       \n",
            "]               -> PUNCT      -> -RRB-     \n",
            ")               -> PUNCT      -> -RRB-     \n",
            "is              -> AUX        -> VBZ       \n",
            "a               -> DET        -> DT        \n",
            "sequence        -> NOUN       -> NN        \n",
            "of              -> ADP        -> IN        \n",
            "characters      -> NOUN       -> NNS       \n",
            "that            -> PRON       -> WDT       \n",
            "define          -> VERB       -> VBP       \n",
            "a               -> DET        -> DT        \n",
            "search          -> NOUN       -> NN        \n",
            "pattern         -> NOUN       -> NN        \n",
            ".               -> PUNCT      -> .         \n",
            "Usually         -> ADV        -> RB        \n",
            "such            -> ADJ        -> JJ        \n",
            "patterns        -> NOUN       -> NNS       \n",
            "are             -> AUX        -> VBP       \n",
            "used            -> VERB       -> VBN       \n",
            "by              -> ADP        -> IN        \n",
            "string          -> NOUN       -> NN        \n",
            "-               -> PUNCT      -> HYPH      \n",
            "searching       -> VERB       -> VBG       \n",
            "algorithms      -> NOUN       -> NNS       \n",
            "for             -> ADP        -> IN        \n",
            "\"               -> PUNCT      -> ``        \n",
            "find            -> VERB       -> VB        \n",
            "\"               -> PUNCT      -> ''        \n",
            "or              -> CCONJ      -> CC        \n",
            "\"               -> PUNCT      -> ``        \n",
            "find            -> VERB       -> VB        \n",
            "and             -> CCONJ      -> CC        \n",
            "replace         -> VERB       -> VB        \n",
            "\"               -> PUNCT      -> ''        \n",
            "operations      -> NOUN       -> NNS       \n",
            "on              -> ADP        -> IN        \n",
            "strings         -> NOUN       -> NNS       \n",
            ",               -> PUNCT      -> ,         \n",
            "or              -> CCONJ      -> CC        \n",
            "for             -> ADP        -> IN        \n",
            "input           -> NOUN       -> NN        \n",
            "validation      -> NOUN       -> NN        \n",
            ".               -> PUNCT      -> .         \n",
            "It              -> PRON       -> PRP       \n",
            "is              -> AUX        -> VBZ       \n",
            "a               -> DET        -> DT        \n",
            "technique       -> NOUN       -> NN        \n",
            "developed       -> VERB       -> VBN       \n",
            "in              -> ADP        -> IN        \n",
            "theoretical     -> ADJ        -> JJ        \n",
            "computer        -> NOUN       -> NN        \n",
            "science         -> NOUN       -> NN        \n",
            "and             -> CCONJ      -> CC        \n",
            "formal          -> ADJ        -> JJ        \n",
            "language        -> NOUN       -> NN        \n",
            "theory          -> NOUN       -> NN        \n",
            ".               -> PUNCT      -> .         \n",
            "\n",
            "               -> SPACE      -> _SP       \n",
            "The             -> DET        -> DT        \n",
            "concept         -> NOUN       -> NN        \n",
            "arose           -> VERB       -> VBD       \n",
            "in              -> ADP        -> IN        \n",
            "the             -> DET        -> DT        \n",
            "1950s           -> NOUN       -> NNS       \n",
            "when            -> SCONJ      -> WRB       \n",
            "the             -> DET        -> DT        \n",
            "American        -> ADJ        -> JJ        \n",
            "mathematician   -> NOUN       -> NN        \n",
            "Stephen         -> PROPN      -> NNP       \n",
            "Cole            -> PROPN      -> NNP       \n",
            "Kleene          -> PROPN      -> NNP       \n",
            "formalized      -> VERB       -> VBD       \n",
            "the             -> DET        -> DT        \n",
            "description     -> NOUN       -> NN        \n",
            "of              -> ADP        -> IN        \n",
            "a               -> DET        -> DT        \n",
            "regular         -> ADJ        -> JJ        \n",
            "language        -> NOUN       -> NN        \n",
            ".               -> PUNCT      -> .         \n",
            "The             -> DET        -> DT        \n",
            "concept         -> NOUN       -> NN        \n",
            "came            -> VERB       -> VBD       \n",
            "into            -> ADP        -> IN        \n",
            "common          -> ADJ        -> JJ        \n",
            "use             -> NOUN       -> NN        \n",
            "with            -> ADP        -> IN        \n",
            "Unix            -> PROPN      -> NNP       \n",
            "text            -> NOUN       -> NN        \n",
            "-               -> PUNCT      -> HYPH      \n",
            "processing      -> VERB       -> VBG       \n",
            "utilities       -> NOUN       -> NNS       \n",
            ".               -> PUNCT      -> .         \n",
            "Different       -> ADJ        -> JJ        \n",
            "syntaxes        -> NOUN       -> NNS       \n",
            "for             -> ADP        -> IN        \n",
            "writing         -> VERB       -> VBG       \n",
            "regular         -> ADJ        -> JJ        \n",
            "expressions     -> NOUN       -> NNS       \n",
            "have            -> AUX        -> VBP       \n",
            "existed         -> VERB       -> VBN       \n",
            "since           -> SCONJ      -> IN        \n",
            "the             -> DET        -> DT        \n",
            "1980s           -> NUM        -> CD        \n",
            ",               -> PUNCT      -> ,         \n",
            "one             -> NUM        -> CD        \n",
            "being           -> AUX        -> VBG       \n",
            "the             -> DET        -> DT        \n",
            "POSIX           -> PROPN      -> NNP       \n",
            "standard        -> NOUN       -> NN        \n",
            "and             -> CCONJ      -> CC        \n",
            "another         -> PRON       -> DT        \n",
            ",               -> PUNCT      -> ,         \n",
            "widely          -> ADV        -> RB        \n",
            "used            -> ADJ        -> JJ        \n",
            ",               -> PUNCT      -> ,         \n",
            "being           -> AUX        -> VBG       \n",
            "the             -> DET        -> DT        \n",
            "Perl            -> PROPN      -> NNP       \n",
            "syntax          -> NOUN       -> NN        \n",
            ".               -> PUNCT      -> .         \n",
            "\n",
            "               -> SPACE      -> _SP       \n",
            "Regular         -> ADJ        -> JJ        \n",
            "expressions     -> NOUN       -> NNS       \n",
            "are             -> AUX        -> VBP       \n",
            "used            -> VERB       -> VBN       \n",
            "in              -> ADP        -> IN        \n",
            "search          -> NOUN       -> NN        \n",
            "engines         -> NOUN       -> NNS       \n",
            ",               -> PUNCT      -> ,         \n",
            "search          -> NOUN       -> NN        \n",
            "and             -> CCONJ      -> CC        \n",
            "replace         -> VERB       -> VB        \n",
            "dialogs         -> NOUN       -> NNS       \n",
            "of              -> ADP        -> IN        \n",
            "word            -> NOUN       -> NN        \n",
            "processors      -> NOUN       -> NNS       \n",
            "and             -> CCONJ      -> CC        \n",
            "text            -> NOUN       -> NN        \n",
            "editors         -> NOUN       -> NNS       \n",
            ",               -> PUNCT      -> ,         \n",
            "in              -> ADP        -> IN        \n",
            "text            -> NOUN       -> NN        \n",
            "processing      -> NOUN       -> NN        \n",
            "utilities       -> NOUN       -> NNS       \n",
            "such            -> ADJ        -> JJ        \n",
            "as              -> ADP        -> IN        \n",
            "sed             -> VERB       -> VBN       \n",
            "and             -> CCONJ      -> CC        \n",
            "AWK             -> PROPN      -> NNP       \n",
            "and             -> CCONJ      -> CC        \n",
            "in              -> ADP        -> IN        \n",
            "lexical         -> ADJ        -> JJ        \n",
            "analysis        -> NOUN       -> NN        \n",
            ".               -> PUNCT      -> .         \n",
            "Many            -> ADJ        -> JJ        \n",
            "programming     -> NOUN       -> NN        \n",
            "languages       -> NOUN       -> NNS       \n",
            "provide         -> VERB       -> VBP       \n",
            "regex           -> NOUN       -> NN        \n",
            "capabilities    -> NOUN       -> NNS       \n",
            "either          -> CCONJ      -> CC        \n",
            "built           -> VERB       -> VBN       \n",
            "-               -> PUNCT      -> HYPH      \n",
            "in              -> ADP        -> RP        \n",
            "or              -> CCONJ      -> CC        \n",
            "via             -> ADP        -> IN        \n",
            "libraries       -> NOUN       -> NNS       \n",
            ".               -> PUNCT      -> .         \n"
          ]
        }
      ],
      "source": [
        "# Get Part of Speech (POS) tags \n",
        "# print token text, pos and tag\n",
        "doc9 = nlp(text9)\n",
        "for token in doc9:\n",
        "    print(f'{token.text:<15} -> {token.pos_:<10} -> {token.tag_:<10}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9mNHiGJt-Dh"
      },
      "source": [
        "The list of pos_ attributes along with its meaning:\n",
        "\n",
        "* ADJ: adjective, e.g. old, green, first, etc.\n",
        "* ADP: adposition, e.g. in, to, during, etc.\n",
        "* ADV: adverb, e.g. very, tomorrow, down, where, there, etc.\n",
        "* AUX: auxiliary, e.g. is, has (done), will (do), should (do), etc.\n",
        "* CONJ: conjunction, e.g. and, or, but, etc.\n",
        "* CCONJ: coordinating conjunction, e.g. and, or, but, etc.\n",
        "* DET: determiner, e.g. a, an, the, etc.\n",
        "* INTJ: interjection, e.g. psst, ouch, bravo, hello, etc.\n",
        "* NOUN: noun, e.g. girl, cat, tree, air, etc.\n",
        "* NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV, etc.\n",
        "* PART: particle, e.g. ’s, not, etc.\n",
        "* PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody, etc.\n",
        "* PROPN: proper noun, e.g. Mary, John, Chucago, NATO, etc.\n",
        "* PUNCT: punctuation, e.g. ., (, ), ?, etc.\n",
        "* SCONJ: subordinating conjunction, e.g. if, while, that, etc.\n",
        "* SYM: symbol, e.g. $, %, §, ©, +, −, ×, ÷, =, :), 😝, etc.\n",
        "* VERB: verb, e.g. run, runs, running, ate, eating, etc.\n",
        "* X: other, e.g. sfpksdpsxmsa(some random text).\n",
        "* SPACE: space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "Gz1Mr0MEjkwL"
      },
      "outputs": [],
      "source": [
        "# We can get any of the above Part of Speech\n",
        "# For a list of the Parts of Speech click the link here\n",
        "# https://spacy.io/usage/linguistic-features\n",
        "# Let us get Verbs , Nouns and Proper Nouns\n",
        "\n",
        "Verbs = [token.text for token in doc9 if(token.pos_=='VERB')]\n",
        "Nouns = [token.text for token in doc9 if(token.pos_=='NOUN')]\n",
        "Proper_Nouns = [token.text for token in doc9 if(token.pos_=='PROPN')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM6Foq6-jkwS",
        "outputId": "1ef15e58-4a0b-48d2-86da-e99af65024f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shortened\n",
            "referred\n",
            "define\n",
            "used\n",
            "searching\n",
            "find\n",
            "find\n",
            "replace\n",
            "developed\n",
            "arose\n"
          ]
        }
      ],
      "source": [
        "# Print Verbs\n",
        "for verb in Verbs[:10]:\n",
        "  print(verb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDWs4ypDxwKq",
        "outputId": "ab313238-b7b9-4a7b-c3b3-fa4540e4da59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "expression\n",
            "regex\n",
            "sequence\n",
            "characters\n",
            "search\n",
            "pattern\n",
            "patterns\n",
            "string\n",
            "algorithms\n",
            "operations\n"
          ]
        }
      ],
      "source": [
        "# Print Nouns\n",
        "for noun in Nouns[:10]:\n",
        "  print(noun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ummJZrvx0lKU"
      },
      "source": [
        "# <font color = 'dodgerblue'>**Stemming**\n",
        "- Not available in SPacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "D3Ch1Yais_jr"
      },
      "outputs": [],
      "source": [
        "%pip install -U nltk -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:38:09.490944Z",
          "iopub.status.busy": "2022-08-28T23:38:09.490415Z",
          "iopub.status.idle": "2022-08-28T23:38:09.951854Z",
          "shell.execute_reply": "2022-08-28T23:38:09.951303Z",
          "shell.execute_reply.started": "2022-08-28T23:38:09.490902Z"
        },
        "id": "BjfG4wSds_jr"
      },
      "outputs": [],
      "source": [
        "# Import PorterStemmer from nltk.stem\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fYOHH7hVngD"
      },
      "source": [
        "## <font color = 'dodgerblue'>**Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:39:24.130677Z",
          "iopub.status.busy": "2022-08-28T23:39:24.130365Z",
          "iopub.status.idle": "2022-08-28T23:39:24.134007Z",
          "shell.execute_reply": "2022-08-28T23:39:24.133565Z",
          "shell.execute_reply.started": "2022-08-28T23:39:24.130663Z"
        },
        "id": "juQm06FateFv",
        "outputId": "46e32ede-68fb-46a0-b3a1-47233efb5c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "connection  :  connect\n",
            "connected  :  connect\n",
            "connnecter  :  connnect\n",
            "connnecting  :  connnect\n",
            "connect  :  connect\n"
          ]
        }
      ],
      "source": [
        "# Create an object of class PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['connection', 'connected', 'connnecter', 'connnecting', 'connect']\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \", stemmer.stem(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOj1X0pks_jr"
      },
      "source": [
        "# <font color = 'dodgerblue'>**Remove HTML Tags**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T02:26:02.857594Z",
          "iopub.status.busy": "2022-08-29T02:26:02.857358Z",
          "iopub.status.idle": "2022-08-29T02:26:02.860337Z",
          "shell.execute_reply": "2022-08-29T02:26:02.859895Z",
          "shell.execute_reply.started": "2022-08-29T02:26:02.857580Z"
        },
        "id": "FtOkdqESs_jr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "text3=\"\"\"I just can't understand the negative comments about this film. Yes it is a typical\n",
        "boy-meets-girl romance but it is done with such flair and polish that the time just flies by. \n",
        "Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever \n",
        "(who says the golden age of cinema is dead?) and Vartan holds his own.<br /><br />There is \n",
        "simmering chemistry between the two leads; the film is most alive when they share a scene - \n",
        "lots! It is done so well that you find yourself willing them to get together...<br /><br />Ignore \n",
        "the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much \n",
        "better. If you are already happy, then you will be euphoric.<br /><br />(PS: I am 33, Male, \n",
        "from the UK and a hopeless romantic still searching for his Princess...)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T02:29:47.357070Z",
          "iopub.status.busy": "2022-08-29T02:29:47.356872Z",
          "iopub.status.idle": "2022-08-29T02:29:47.359439Z",
          "shell.execute_reply": "2022-08-29T02:29:47.359124Z",
          "shell.execute_reply.started": "2022-08-29T02:29:47.357057Z"
        },
        "id": "-PSmWoQ-s_js",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T02:26:44.406623Z",
          "iopub.status.busy": "2022-08-29T02:26:44.406265Z",
          "iopub.status.idle": "2022-08-29T02:26:44.409549Z",
          "shell.execute_reply": "2022-08-29T02:26:44.409060Z",
          "shell.execute_reply.started": "2022-08-29T02:26:44.406608Z"
        },
        "id": "4lZnq5Mus_js",
        "tags": []
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(text3, \"html.parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3W8S9LW2m7-"
      },
      "source": [
        "- The code above is using the BeautifulSoup library in Python to parse an HTML document represented by the variable `text3`. The `BeautifulSoup` function creates a new BeautifulSoup object, which is stored in the variable `soup`.\n",
        "\n",
        "- The second argument, `\"html.parser\"`, specifies the parser to be used. In this case, the `\"html.parser\"` argument means that the HTML document will be parsed using Python's built-in HTML parser. This parser will process the HTML and convert it into a tree-like structure that can be easily navigated and searched using BeautifulSoup methods.\n",
        "\n",
        "- The resulting `soup` object can be used to extract data from the HTML document, such as extracting tags, searching for tags based on specific attributes, and modifying the HTML document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fffUPhPn9oTI",
        "outputId": "8d2d0521-5475-4d5b-d553-993cf15a1900"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<br/>"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the first tag matching the criteria and return it\n",
        "soup.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3PJ0OZ-3Uce"
      },
      "source": [
        "The code above uses the find method of the soup object to search for the first tag in the HTML document that matches the specified criteria. If **no criteria are specified**, the find method will return the first tag it encounters in the HTML document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoOGFWkp90GB",
        "outputId": "d9bd2cdb-521e-424a-ce5f-f904d323512a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bool(soup.find())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T02:27:06.185972Z",
          "iopub.status.busy": "2022-08-29T02:27:06.185740Z",
          "iopub.status.idle": "2022-08-29T02:27:06.188519Z",
          "shell.execute_reply": "2022-08-29T02:27:06.188157Z",
          "shell.execute_reply.started": "2022-08-29T02:27:06.185958Z"
        },
        "id": "_eIS5dLws_js"
      },
      "outputs": [],
      "source": [
        "# Extract all text from HTML document and store it in cleaned_text3\n",
        "cleaned_text3 = soup.get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CK1nLi3uBN"
      },
      "source": [
        "- The code uses the `get_text` method of the soup object to extract all text from the HTML document and store it in the variable `cleaned_text3`. The `get_text` method retrieves all text from the HTML document, including text within tags and any whitespace, and returns it as a string. This string can then be further processed, such as being split into sentences or tokens, or used for text analysis.\n",
        "\n",
        "- By using the `get_text` method, the code is effectively \"cleaning\" the HTML document of all its tags and just retaining its plain text content, which can be useful for certain text processing tasks where the HTML tags are not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "execution": {
          "iopub.execute_input": "2022-08-29T02:27:14.927358Z",
          "iopub.status.busy": "2022-08-29T02:27:14.926810Z",
          "iopub.status.idle": "2022-08-29T02:27:14.932890Z",
          "shell.execute_reply": "2022-08-29T02:27:14.932563Z",
          "shell.execute_reply.started": "2022-08-29T02:27:14.927343Z"
        },
        "id": "8AQKgKKEs_js",
        "outputId": "3eab48d1-01ce-4149-e32b-c83fac45fd81",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I just can't understand the negative comments about this film. Yes it is a typical\\nboy-meets-girl romance but it is done with such flair and polish that the time just flies by. \\nHenstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever \\n(who says the golden age of cinema is dead?) and Vartan holds his own.There is \\nsimmering chemistry between the two leads; the film is most alive when they share a scene - \\nlots! It is done so well that you find yourself willing them to get together...Ignore \\nthe negative comments - if you are feeling a bit blue, watch this flick, you will feel so much \\nbetter. If you are already happy, then you will be euphoric.(PS: I am 33, Male, \\nfrom the UK and a hopeless romantic still searching for his Princess...)\""
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T02:29:49.988862Z",
          "iopub.status.busy": "2022-08-29T02:29:49.988652Z",
          "iopub.status.idle": "2022-08-29T02:29:49.991577Z",
          "shell.execute_reply": "2022-08-29T02:29:49.991242Z",
          "shell.execute_reply.started": "2022-08-29T02:29:49.988849Z"
        },
        "id": "7JQoHQiVs_js",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def basic_clean(text: str) -> str:\n",
        "    \"\"\"\n",
        "    This function performs basic text cleaning on an input string by removing HTML tags (if present) \n",
        "    and replacing newline and return characters with a space.\n",
        "    \n",
        "    Parameters:\n",
        "    text (str): The input string to be cleaned\n",
        "    \n",
        "    Returns:\n",
        "    str: The cleaned string\n",
        "    \"\"\"\n",
        "    # Use BeautifulSoup to remove HTML tags (if present)\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    \n",
        "    # Replace newline and return characters with a space\n",
        "    return re.sub(r'[\\n\\r]',' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-29T02:29:50.482046Z",
          "iopub.status.busy": "2022-08-29T02:29:50.481790Z",
          "iopub.status.idle": "2022-08-29T02:29:50.484993Z",
          "shell.execute_reply": "2022-08-29T02:29:50.484508Z",
          "shell.execute_reply.started": "2022-08-29T02:29:50.482031Z"
        },
        "id": "kYaWgMP-s_js",
        "tags": []
      },
      "outputs": [],
      "source": [
        "cleaned_text = basic_clean(text=text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "execution": {
          "iopub.execute_input": "2022-08-29T02:29:56.702834Z",
          "iopub.status.busy": "2022-08-29T02:29:56.702641Z",
          "iopub.status.idle": "2022-08-29T02:29:56.706111Z",
          "shell.execute_reply": "2022-08-29T02:29:56.705709Z",
          "shell.execute_reply.started": "2022-08-29T02:29:56.702821Z"
        },
        "id": "TGyBFp_2s_js",
        "outputId": "a9dec527-108e-411c-cecc-dd48a7c45c50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I just can't understand the negative comments about this film. Yes it is a typical boy-meets-girl romance but it is done with such flair and polish that the time just flies by.  Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever  (who says the golden age of cinema is dead?) and Vartan holds his own.There is  simmering chemistry between the two leads; the film is most alive when they share a scene -  lots! It is done so well that you find yourself willing them to get together...Ignore  the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much  better. If you are already happy, then you will be euphoric.(PS: I am 33, Male,  from the UK and a hopeless romantic still searching for his Princess...)\""
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
