{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulRauf96/NLP/blob/main/Custom_Spacy_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUh_cas3lv7G"
      },
      "source": [
        "# <font color = 'pickle'>**Advanced Spacy**</font>\n",
        "    \n",
        "In this notebook, we will learn some advanced features of Spacy:\n",
        "\n",
        "1. Custom Tokenizer\n",
        "2. Rule-Based Matching\n",
        "3. Custom Extensions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'pickle'>**Set Path for Data**"
      ],
      "metadata": {
        "id": "X1znF9Q1JM5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use for normal projects\n",
        "from pathlib import Path\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive') \n",
        "  %pip install swifter -qq\n",
        "  %pip install -U spacy -qq\n",
        "  base_folder = Path('/content/drive/MyDrive/colab_notebooks/')\n",
        "  subject = 'nlp'\n",
        "  data = base_folder/subject/'data/'\n",
        "  archive = base_folder/subject/'archive/'\n",
        "  output = base_folder/subject/'output'\n",
        "else:\n",
        "  base_folder = Path('C:/Users/Abdul Rauf Maroof/OneDrive/Documents/MSBA')\n",
        "  data = base_folder/subject/'data/'\n",
        "  archive = base_folder/subject/'archive/'\n",
        "  output = base_folder/subject/'output'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz1oexrl0q6u",
        "outputId": "14dc0119-91fa-4095-b868-eacc95ca46df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m830.9/830.9 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nvCtHdTlv7I"
      },
      "source": [
        "# <font color = 'pickle'>**Install/Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:45:38.730465Z",
          "iopub.status.busy": "2022-08-28T23:45:38.730260Z",
          "iopub.status.idle": "2022-08-28T23:45:40.344524Z",
          "shell.execute_reply": "2022-08-28T23:45:40.343995Z",
          "shell.execute_reply.started": "2022-08-28T23:45:38.730426Z"
        },
        "id": "wVhmiNTA80Lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86855623-925d-4301-cd20-ac093cac4f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "# Importing required libraries\n",
        "# Path from the 'pathlib' library is used for working with files and directories in a portable manner across different operating systems\n",
        "import re\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:45:41.102966Z",
          "iopub.status.busy": "2022-08-28T23:45:41.102584Z",
          "iopub.status.idle": "2022-08-28T23:45:41.111664Z",
          "shell.execute_reply": "2022-08-28T23:45:41.111167Z",
          "shell.execute_reply.started": "2022-08-28T23:45:41.102945Z"
        },
        "id": "3gvC72D5C5f1",
        "outputId": "08ead037-3faa-44f0-b2ea-2aa34c080e75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "spacy.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgGJnqAqlv7K"
      },
      "source": [
        "# <font color = 'pickle'>**Load Spacy Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W1zpo7QQbEnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f31ed16-ffd4-4d8e-ff76-ef0272f92a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-01 19:53:01.743442: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-01 19:53:01.743615: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-01 19:53:01.743641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-01 19:53:03.450923: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:53:54.463327Z",
          "iopub.status.busy": "2022-08-28T23:53:54.463023Z",
          "iopub.status.idle": "2022-08-28T23:53:54.807468Z",
          "shell.execute_reply": "2022-08-28T23:53:54.806929Z",
          "shell.execute_reply.started": "2022-08-28T23:53:54.463307Z"
        },
        "id": "K9yk6cXlcEGp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Loading the 'en_core_web_sm' language model from the spaCy library\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Selecting pipes to disable for the loaded language model\n",
        "# Here, the pipes for token-to-vector, part-of-speech tagging, dependency parsing, attribute ruler, \n",
        "# lemmatization and named entity recognition are being disabled.\n",
        "disabled = nlp.select_pipes(disable= ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:53:57.260247Z",
          "iopub.status.busy": "2022-08-28T23:53:57.259973Z",
          "iopub.status.idle": "2022-08-28T23:53:57.263955Z",
          "shell.execute_reply": "2022-08-28T23:53:57.263558Z",
          "shell.execute_reply.started": "2022-08-28T23:53:57.260227Z"
        },
        "id": "thbEf7lHEK9i",
        "outputId": "01095d95-d0cf-40b4-c8fd-306d15e8bdd8",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# check the disabiled pipelines\n",
        "disabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:53:57.746630Z",
          "iopub.status.busy": "2022-08-28T23:53:57.746322Z",
          "iopub.status.idle": "2022-08-28T23:53:57.749641Z",
          "shell.execute_reply": "2022-08-28T23:53:57.749214Z",
          "shell.execute_reply.started": "2022-08-28T23:53:57.746609Z"
        },
        "id": "Plb5BFVeMmpS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "sample_text = \" #Reg #Ex @abc@xyz.com! prefixes  stop-words wow!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:53:58.185377Z",
          "iopub.status.busy": "2022-08-28T23:53:58.185128Z",
          "iopub.status.idle": "2022-08-28T23:53:58.189255Z",
          "shell.execute_reply": "2022-08-28T23:53:58.188758Z",
          "shell.execute_reply.started": "2022-08-28T23:53:58.185356Z"
        },
        "id": "iEc8jIghi2By",
        "outputId": "74ea7784-1418-4f9a-cf7e-7fe50b778e9d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '#', 'Reg', '#', 'Ex', '@abc@xyz.com', '!', 'prefixes', ' ', 'stop', '-', 'words', 'wow', '!']\n"
          ]
        }
      ],
      "source": [
        "# Creating a spaCy document from the sample text\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Printing the text of each token in the document\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:59:33.369001Z",
          "iopub.status.busy": "2022-08-28T23:59:33.368800Z",
          "iopub.status.idle": "2022-08-28T23:59:33.371295Z",
          "shell.execute_reply": "2022-08-28T23:59:33.370965Z",
          "shell.execute_reply.started": "2022-08-28T23:59:33.368987Z"
        },
        "id": "9llrdrL5lv7N"
      },
      "source": [
        "# <font color = 'pickle'>**Custom Tokenizer in spaCy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgCsbeEGW9WD"
      },
      "source": [
        "## <font color = 'pickle'>**Modfiy Prefixes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:03.624549Z",
          "iopub.status.busy": "2022-08-28T23:54:03.624374Z",
          "iopub.status.idle": "2022-08-28T23:54:03.628804Z",
          "shell.execute_reply": "2022-08-28T23:54:03.628330Z",
          "shell.execute_reply.started": "2022-08-28T23:54:03.624535Z"
        },
        "id": "WapY6DzPHl2T",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7ec43b-a078-4ccf-c610-24f6ce3bd204"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\\\{', '\\\\}', '<', '>', '_', '\\\\*', '&', '。', '？', '！']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Modify the prefix character used by spacy tokenizer\n",
        "# Let us say if we want to keep hashes (#) together in a token\n",
        "# spacy treats # as prefixes and hence separates them when creating tokens\n",
        "\n",
        "# Accessing the default prefixes for the loaded language model\n",
        "prefixes = nlp.Defaults.prefixes\n",
        "prefixes[20:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Remove Prefixes**"
      ],
      "metadata": {
        "id": "Y57_qT4JA_UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the '#' symbol from the prefixes list\n",
        "prefixes.remove(r'#')\n",
        "\n",
        "# Compiling a regular expression pattern for the remaining prefixes\n",
        "prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "\n",
        "# Assigning the compiled prefix regular expression to the spaCy tokenizer's prefix search method\n",
        "nlp.tokenizer.prefix_search = prefix_regex.search"
      ],
      "metadata": {
        "id": "FMgvYXQajGHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation:\n",
        "\n",
        "This code modifies the prefixes used by the spaCy tokenizer. \n",
        "- The first line removes the `#` symbol from the prefixes list. \n",
        "- Then, a regular expression pattern is compiled from the updated prefixes list using the `spacy.util.compile_prefix_regex` function. \n",
        "- Finally, the compiled regular expression is assigned to the `prefix_search` method of the spaCy tokenizer, which is used to identify prefixes in text during tokenization. \n",
        "\n",
        "By updating the prefix_search method, this code changes the behavior of the spaCy tokenizer to no longer treat the `#` symbol as a prefix."
      ],
      "metadata": {
        "id": "HmiNRHOROyYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Add Prefixes**"
      ],
      "metadata": {
        "id": "98Pcs8Y6BIHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:13.894882Z",
          "iopub.status.busy": "2022-08-28T23:54:13.894678Z",
          "iopub.status.idle": "2022-08-28T23:54:13.898317Z",
          "shell.execute_reply": "2022-08-28T23:54:13.897984Z",
          "shell.execute_reply.started": "2022-08-28T23:54:13.894868Z"
        },
        "id": "ytyIiVuTZgwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1825039-fb2e-44e9-c243-8963ca9b9103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '#Reg', '#Ex', '@abc@xyz.com', '!', 'prefixes', ' ', 'stop', '-', 'words', 'wow', '!']\n"
          ]
        }
      ],
      "source": [
        "# create doc\n",
        "doc = nlp(sample_text)\n",
        "# print tokens\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:16.758287Z",
          "iopub.status.busy": "2022-08-28T23:54:16.758098Z",
          "iopub.status.idle": "2022-08-28T23:54:16.829476Z",
          "shell.execute_reply": "2022-08-28T23:54:16.829009Z",
          "shell.execute_reply.started": "2022-08-28T23:54:16.758273Z"
        },
        "id": "wD-VO0AQOIb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f552c9-1e99-4ced-9746-1c10355fdce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '#Reg', '#Ex', '@', 'abc@xyz.com', '!', 'prefixes', ' ', 'stop', '-', 'words', 'wow', '!']\n"
          ]
        }
      ],
      "source": [
        "# Add prefix character to split from text\n",
        "prefixes.append(r'@')\n",
        "\n",
        "# Compiling a regular expression pattern for the modified prefixes\n",
        "prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
        "\n",
        "# Assigning the compiled prefix regular expression to the spaCy tokenizer's prefix search method\n",
        "nlp.tokenizer.prefix_search = prefix_regex.search\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvHZ1RC4XENE"
      },
      "source": [
        "## <font color = 'pickle'>**Modify Suffixes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:18.119467Z",
          "iopub.status.busy": "2022-08-28T23:54:18.119299Z",
          "iopub.status.idle": "2022-08-28T23:54:18.123206Z",
          "shell.execute_reply": "2022-08-28T23:54:18.122888Z",
          "shell.execute_reply.started": "2022-08-28T23:54:18.119454Z"
        },
        "id": "NruLGymAK7Rs",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "outputId": "bd1f015d-0877-4b47-c0a9-42c4c134d556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['&', '。', '？', '！', '，', '、', '；', '：', '～', '·']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# check default suffixes in spacy\n",
        "suffixes = nlp.Defaults.suffixes\n",
        "suffixes[20:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:26.748218Z",
          "iopub.status.busy": "2022-08-28T23:54:26.747969Z",
          "iopub.status.idle": "2022-08-28T23:54:26.847414Z",
          "shell.execute_reply": "2022-08-28T23:54:26.846923Z",
          "shell.execute_reply.started": "2022-08-28T23:54:26.748197Z"
        },
        "id": "aarRNTxELjrk"
      },
      "outputs": [],
      "source": [
        "# Remove suffix characters to not split from text\n",
        "suffixes.remove(r'\\!')\n",
        "\n",
        "# Compiling a regular expression pattern for the modified suffixes\n",
        "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
        "\n",
        "# Assigning the compiled prefix regular expression to the spaCy tokenizer's suffix search method\n",
        "nlp.tokenizer.suffix_search = suffix_regex.search\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCKHbHc-XKa5"
      },
      "source": [
        "## <font color = 'pickle'>**Modify infixes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:28.608460Z",
          "iopub.status.busy": "2022-08-28T23:54:28.608254Z",
          "iopub.status.idle": "2022-08-28T23:54:28.612141Z",
          "shell.execute_reply": "2022-08-28T23:54:28.611684Z",
          "shell.execute_reply.started": "2022-08-28T23:54:28.608446Z"
        },
        "id": "-7aEaSntJ5zM",
        "outputId": "7689fa60-fdd1-4d54-bda8-7863e80a2115"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\\\.\\\\.+',\n",
              " '…',\n",
              " '[\\\\u00A6\\\\u00A9\\\\u00AE\\\\u00B0\\\\u0482\\\\u058D\\\\u058E\\\\u060E\\\\u060F\\\\u06DE\\\\u06E9\\\\u06FD\\\\u06FE\\\\u07F6\\\\u09FA\\\\u0B70\\\\u0BF3-\\\\u0BF8\\\\u0BFA\\\\u0C7F\\\\u0D4F\\\\u0D79\\\\u0F01-\\\\u0F03\\\\u0F13\\\\u0F15-\\\\u0F17\\\\u0F1A-\\\\u0F1F\\\\u0F34\\\\u0F36\\\\u0F38\\\\u0FBE-\\\\u0FC5\\\\u0FC7-\\\\u0FCC\\\\u0FCE\\\\u0FCF\\\\u0FD5-\\\\u0FD8\\\\u109E\\\\u109F\\\\u1390-\\\\u1399\\\\u1940\\\\u19DE-\\\\u19FF\\\\u1B61-\\\\u1B6A\\\\u1B74-\\\\u1B7C\\\\u2100\\\\u2101\\\\u2103-\\\\u2106\\\\u2108\\\\u2109\\\\u2114\\\\u2116\\\\u2117\\\\u211E-\\\\u2123\\\\u2125\\\\u2127\\\\u2129\\\\u212E\\\\u213A\\\\u213B\\\\u214A\\\\u214C\\\\u214D\\\\u214F\\\\u218A\\\\u218B\\\\u2195-\\\\u2199\\\\u219C-\\\\u219F\\\\u21A1\\\\u21A2\\\\u21A4\\\\u21A5\\\\u21A7-\\\\u21AD\\\\u21AF-\\\\u21CD\\\\u21D0\\\\u21D1\\\\u21D3\\\\u21D5-\\\\u21F3\\\\u2300-\\\\u2307\\\\u230C-\\\\u231F\\\\u2322-\\\\u2328\\\\u232B-\\\\u237B\\\\u237D-\\\\u239A\\\\u23B4-\\\\u23DB\\\\u23E2-\\\\u2426\\\\u2440-\\\\u244A\\\\u249C-\\\\u24E9\\\\u2500-\\\\u25B6\\\\u25B8-\\\\u25C0\\\\u25C2-\\\\u25F7\\\\u2600-\\\\u266E\\\\u2670-\\\\u2767\\\\u2794-\\\\u27BF\\\\u2800-\\\\u28FF\\\\u2B00-\\\\u2B2F\\\\u2B45\\\\u2B46\\\\u2B4D-\\\\u2B73\\\\u2B76-\\\\u2B95\\\\u2B98-\\\\u2BC8\\\\u2BCA-\\\\u2BFE\\\\u2CE5-\\\\u2CEA\\\\u2E80-\\\\u2E99\\\\u2E9B-\\\\u2EF3\\\\u2F00-\\\\u2FD5\\\\u2FF0-\\\\u2FFB\\\\u3004\\\\u3012\\\\u3013\\\\u3020\\\\u3036\\\\u3037\\\\u303E\\\\u303F\\\\u3190\\\\u3191\\\\u3196-\\\\u319F\\\\u31C0-\\\\u31E3\\\\u3200-\\\\u321E\\\\u322A-\\\\u3247\\\\u3250\\\\u3260-\\\\u327F\\\\u328A-\\\\u32B0\\\\u32C0-\\\\u32FE\\\\u3300-\\\\u33FF\\\\u4DC0-\\\\u4DFF\\\\uA490-\\\\uA4C6\\\\uA828-\\\\uA82B\\\\uA836\\\\uA837\\\\uA839\\\\uAA77-\\\\uAA79\\\\uFDFD\\\\uFFE4\\\\uFFE8\\\\uFFED\\\\uFFEE\\\\uFFFC\\\\uFFFD\\\\U00010137-\\\\U0001013F\\\\U00010179-\\\\U00010189\\\\U0001018C-\\\\U0001018E\\\\U00010190-\\\\U0001019B\\\\U000101A0\\\\U000101D0-\\\\U000101FC\\\\U00010877\\\\U00010878\\\\U00010AC8\\\\U0001173F\\\\U00016B3C-\\\\U00016B3F\\\\U00016B45\\\\U0001BC9C\\\\U0001D000-\\\\U0001D0F5\\\\U0001D100-\\\\U0001D126\\\\U0001D129-\\\\U0001D164\\\\U0001D16A-\\\\U0001D16C\\\\U0001D183\\\\U0001D184\\\\U0001D18C-\\\\U0001D1A9\\\\U0001D1AE-\\\\U0001D1E8\\\\U0001D200-\\\\U0001D241\\\\U0001D245\\\\U0001D300-\\\\U0001D356\\\\U0001D800-\\\\U0001D9FF\\\\U0001DA37-\\\\U0001DA3A\\\\U0001DA6D-\\\\U0001DA74\\\\U0001DA76-\\\\U0001DA83\\\\U0001DA85\\\\U0001DA86\\\\U0001ECAC\\\\U0001F000-\\\\U0001F02B\\\\U0001F030-\\\\U0001F093\\\\U0001F0A0-\\\\U0001F0AE\\\\U0001F0B1-\\\\U0001F0BF\\\\U0001F0C1-\\\\U0001F0CF\\\\U0001F0D1-\\\\U0001F0F5\\\\U0001F110-\\\\U0001F16B\\\\U0001F170-\\\\U0001F1AC\\\\U0001F1E6-\\\\U0001F202\\\\U0001F210-\\\\U0001F23B\\\\U0001F240-\\\\U0001F248\\\\U0001F250\\\\U0001F251\\\\U0001F260-\\\\U0001F265\\\\U0001F300-\\\\U0001F3FA\\\\U0001F400-\\\\U0001F6D4\\\\U0001F6E0-\\\\U0001F6EC\\\\U0001F6F0-\\\\U0001F6F9\\\\U0001F700-\\\\U0001F773\\\\U0001F780-\\\\U0001F7D8\\\\U0001F800-\\\\U0001F80B\\\\U0001F810-\\\\U0001F847\\\\U0001F850-\\\\U0001F859\\\\U0001F860-\\\\U0001F887\\\\U0001F890-\\\\U0001F8AD\\\\U0001F900-\\\\U0001F90B\\\\U0001F910-\\\\U0001F93E\\\\U0001F940-\\\\U0001F970\\\\U0001F973-\\\\U0001F976\\\\U0001F97A\\\\U0001F97C-\\\\U0001F9A2\\\\U0001F9B0-\\\\U0001F9B9\\\\U0001F9C0-\\\\U0001F9C2\\\\U0001F9D0-\\\\U0001F9FF\\\\U0001FA60-\\\\U0001FA6D]']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Create a list of default infixes from spaCy's \"nlp.Defaults\" module\n",
        "infixes = list(nlp.Defaults.infixes)\n",
        "infixes[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:29.382645Z",
          "iopub.status.busy": "2022-08-28T23:54:29.382428Z",
          "iopub.status.idle": "2022-08-28T23:54:29.521144Z",
          "shell.execute_reply": "2022-08-28T23:54:29.520682Z",
          "shell.execute_reply.started": "2022-08-28T23:54:29.382631Z"
        },
        "id": "xR0zR68GsLss",
        "outputId": "6d45c6e2-d29f-4c99-b66f-1c5498efbf01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '#Reg', '#Ex', '@', 'abc@xyz.com!', 'prefixes', ' ', 'stop-words', 'wow!']\n"
          ]
        }
      ],
      "source": [
        "# Create a new list of infixes without elements containing the string '-'\n",
        "infixes = [x for x in infixes if r'-' not in x]\n",
        "\n",
        "# Compiling a regular expression pattern for the modified infixes\n",
        "infix_regex = spacy.util.compile_infix_regex(infixes)\n",
        "\n",
        "# Assigning the compiled prefix regular expression to the spaCy tokenizer's infix search method\n",
        "nlp.tokenizer.infix_finditer =infix_regex.finditer \n",
        "\n",
        "doc = nlp(sample_text)\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can't use the `.remove()` method to remove the string `'-'` from the infixes list because there are multiple instances of the string `'-'` in the infixes list.\n",
        "\n",
        "The list comprehension `[x for x in infixes if r'-' not in x]` creates a new list that only contains elements from the original infixes list if the string `'-'` is not present in the element. This ensures that all elements in the list that contain the string `'-'` are removed."
      ],
      "metadata": {
        "id": "O0oigd4oELd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**Adding special case tokenization rules**"
      ],
      "metadata": {
        "id": "k0XSdolTCePN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"gimme that\")  \n",
        "print([w.text for w in doc])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjRpfdhEPA--",
        "outputId": "e558cd09-e22a-44f8-9cab-85a57631dd17"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gimme', 'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the ORTH symbol from the spaCy symbols module\n",
        "from spacy.symbols import ORTH"
      ],
      "metadata": {
        "id": "8-Vv7_R8Qavp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ORTH symbol is a constant from the spacy.symbols module in spaCy. It represents the string form of a token, including any whitespace or special characters. "
      ],
      "metadata": {
        "id": "dBCK1_EdQxAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a special case for the text \"gimme\"\n",
        "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
        "\n",
        "# Add the special case to the tokenizer of the NLP object\n",
        "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
        "\n",
        "# Tokenize the text \"gimme that\" and print the resulting tokens\n",
        "print([w.text for w in nlp(\"gimme that\")]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH9z3yGmQpqX",
        "outputId": "9b004109-c75b-4a93-a8b5-0b5bad96ca8e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gim', 'me', 'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nlp.tokenizer.add_special_case` is a method in spaCy's NLP object that allows you to add a special case tokenization for specific texts. This method takes two arguments:\n",
        "\n",
        "- The first argument is the text you want to add a special case for.\n",
        "- The second argument is the special case tokenization you want to apply to the text. This is defined as a list of dictionaries, where each dictionary represents a token and maps the ORTH key to the text of the token."
      ],
      "metadata": {
        "id": "EEhG8QAPXZG0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vskE4nvxPbhb"
      },
      "source": [
        "# <font color = 'pickle'>**Rule-based matching using spaCy**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:36.044928Z",
          "iopub.status.busy": "2022-08-28T23:54:36.044727Z",
          "iopub.status.idle": "2022-08-28T23:54:36.047667Z",
          "shell.execute_reply": "2022-08-28T23:54:36.047186Z",
          "shell.execute_reply.started": "2022-08-28T23:54:36.044915Z"
        },
        "id": "0guOFL7_Sd5S"
      },
      "outputs": [],
      "source": [
        "# Import the Matcher class from spaCy's 'spacy.matcher' module\n",
        "from spacy.matcher import Matcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iFtPpM3JeRw"
      },
      "source": [
        "\n",
        "Matcher objects let us match sequences of tokens based on pattern rules. This is used as an alternative to regex pattern matching.\n",
        "\n",
        "- Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
        "\n",
        "- When we use Matcher object on Tokens, we can use word level features of spaCy such as LOWER, LENGTH, LEMMA, SHAPE and flags such as IS_PUNCT, IS_DIGIT, LIKE_URL, etc. \n",
        "\n",
        "- We can also use part of speech tags and named entities in patterns e.g., find the word `\"cloud\"` only if it's a `verb`, not a `noun`.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:43.059615Z",
          "iopub.status.busy": "2022-08-28T23:54:43.059412Z",
          "iopub.status.idle": "2022-08-28T23:54:43.062436Z",
          "shell.execute_reply": "2022-08-28T23:54:43.061954Z",
          "shell.execute_reply.started": "2022-08-28T23:54:43.059602Z"
        },
        "id": "7COXFc6FKgyF"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"New version of operation system is iOS 11. It is better than iOS 9 and iOS 9. \n",
        "The new version of iPhone X seems cool. The video of iphone x released. I liked iOS 9 but I like iOS 11 more.\n",
        "You may not like my like. Contact us : xyz@gmail.com., abc@utdallas.edu\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KrryM9pXTCT"
      },
      "source": [
        "## <font color = 'pickle'>**1. Matching Exact Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:44.434886Z",
          "iopub.status.busy": "2022-08-28T23:54:44.434402Z",
          "iopub.status.idle": "2022-08-28T23:54:44.439897Z",
          "shell.execute_reply": "2022-08-28T23:54:44.439425Z",
          "shell.execute_reply.started": "2022-08-28T23:54:44.434871Z"
        },
        "id": "jQh37KXqLmsE",
        "outputId": "a7d5eeec-cc1b-4273-afe1-0514b63cf0f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9385982399280393077, 6, 7),\n",
              " (9385982399280393077, 13, 14),\n",
              " (9385982399280393077, 16, 17),\n",
              " (9385982399280393077, 24, 26),\n",
              " (9385982399280393077, 38, 39),\n",
              " (9385982399280393077, 43, 44)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Example 1: Matching Exact Text\n",
        "\n",
        "# When initiating Matcher we need to specify vocab\n",
        "# Instantiate Matcher object using nlp.vocab\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(text)\n",
        "\n",
        "# Match Exact Tokens : match TEXT iOS\n",
        "pattern1 = [{\"TEXT\":\"iOS\"}]\n",
        "\n",
        "# Match sequence of texts : iPhone followed by X\n",
        "pattern2 = [{\"TEXT\": \"iPhone\"},{\"TEXT\": \"X\"}]\n",
        "\n",
        "# matcher.add() method to add patterns to matcher\n",
        "matcher.add(\"TextOnly\",[pattern1, pattern2])\n",
        "\n",
        "# When we call the matcher on a doc, it returns a list of tuples.\n",
        "# Each tuple consists of three values: the match ID, the star index and the end index of the matched span.\n",
        "matches = matcher(doc)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:51.498740Z",
          "iopub.status.busy": "2022-08-28T23:54:51.498492Z",
          "iopub.status.idle": "2022-08-28T23:54:51.501831Z",
          "shell.execute_reply": "2022-08-28T23:54:51.501402Z",
          "shell.execute_reply.started": "2022-08-28T23:54:51.498727Z"
        },
        "id": "B7WqAfkMREXx",
        "outputId": "7c63c6d1-33be-4a3e-fba3-8a4299effe19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['iOS', 'iOS', 'iOS', 'iPhone X', 'iOS', 'iOS']\n"
          ]
        }
      ],
      "source": [
        "# We can acees a span from doc using slicing (similar to arrays in numpy)\n",
        "print([doc[start:end].text for match_id, start, end in matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**2. Matching Attribute (LOWER, IS_DIGIT)**"
      ],
      "metadata": {
        "id": "DVqxB0eQ94Nx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:54:56.311954Z",
          "iopub.status.busy": "2022-08-28T23:54:56.311466Z",
          "iopub.status.idle": "2022-08-28T23:54:56.316939Z",
          "shell.execute_reply": "2022-08-28T23:54:56.316430Z",
          "shell.execute_reply.started": "2022-08-28T23:54:56.311934Z"
        },
        "id": "HCVmP7UCLJbc",
        "outputId": "ad214b8b-b3f9-434c-f170-bbda1c347c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['iOS 11', 'iOS 9', 'iOS 9', 'iPhone X', 'iphone x', 'iOS 9', 'iOS 11']\n"
          ]
        }
      ],
      "source": [
        "# Match Exact tokens and attributes\n",
        "# List of availible attributes that can be used with matcher : https://spacy.io/usage/rule-based-matching\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(text)\n",
        "\n",
        "# pattern 1 : text iOS followed by digit\n",
        "pattern1 = [{\"TEXT\":\"iOS\"}, {\"IS_DIGIT\":True}]\n",
        "\n",
        "# pattern 2 : iphone followed by x (irrespective if case (lower/upper) for both iphone and X)\n",
        "pattern2 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
        "\n",
        "# matcher.add() method to add patterns to matcher\n",
        "matcher.add(\"TextAndLower\",[pattern1, pattern2])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "print([doc[start:end].text for match_id, start, end in matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ClTjpT3bRhF"
      },
      "source": [
        "<font color = 'dodgerblue'> Note: For pattern 2 in above example, The `‘LOWER’: ‘iphone'`, `‘LOWER’: 'x'` means that we want to match a word where its lower form is `‘iphone x'`. So with this, we can match the word `‘Iphone X’` or even `‘IPHONE X'`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**3. Matching Attribute (IS_LOWER)**"
      ],
      "metadata": {
        "id": "dbk3QekeKuVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Match Exact tokens and attributes\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(text)\n",
        "\n",
        "# pattern 1 : text iOS followed by digit\n",
        "pattern1 = [{\"TEXT\":\"iOS\"}, {\"IS_DIGIT\":True}]\n",
        "\n",
        "# pattern2 :lowercase iPhone\n",
        "pattern2 = [{\"TEXT\": \"iphone\" ,\"IS_LOWER\":True},  {\"LOWER\": \"x\"}]\n",
        "             \n",
        "# matcher.add() method to add patterns to matcher\n",
        "matcher.add(\"TextAndIsLower\",[pattern1, pattern2])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "print([doc[start:end].text for match_id, start, end in matches])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcoHwyXSIQZh",
        "outputId": "72183ba6-8e1f-49e7-8531-00e5a910dd44"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['iOS 11', 'iOS 9', 'iOS 9', 'iphone x', 'iOS 9', 'iOS 11']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhuN70MyLesN"
      },
      "source": [
        "<font color = 'dodgerblue'>For pattern 2 in above example, we only want to extract iphone when it is in lowercase."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**4. Matching Attribute (LEMMA)**"
      ],
      "metadata": {
        "id": "DpZzs462-Y4l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:01.587068Z",
          "iopub.status.busy": "2022-08-28T23:55:01.586852Z",
          "iopub.status.idle": "2022-08-28T23:55:01.599582Z",
          "shell.execute_reply": "2022-08-28T23:55:01.598968Z",
          "shell.execute_reply.started": "2022-08-28T23:55:01.587054Z"
        },
        "id": "d5eHEpQqS3lb",
        "outputId": "8433270a-b0f0-4b19-bc8c-fc9273c410e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipe names: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "\n",
            "[(12849222793144466734, 37, 38), (12849222793144466734, 42, 43), (12849222793144466734, 51, 52), (12849222793144466734, 53, 54)]\n",
            "['liked', 'like', 'like', 'like']\n"
          ]
        }
      ],
      "source": [
        "# Matching other attributes\n",
        "disabled.restore()\n",
        "\n",
        "print(f'pipe names: {nlp.pipe_names}')\n",
        "print()\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(text)\n",
        "\n",
        "# write a pattern to match word whose lemma is 'like'\n",
        "pattern = [{\"LEMMA\": \"like\"}]\n",
        "\n",
        "# matcher.add() method to add patterns to matcher\n",
        "matcher.add(\"Lemma\",[pattern],greedy='LONGEST')\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "print(matches)\n",
        "print([doc[start:end].text for match_id, start, end in matches])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:04.711132Z",
          "iopub.status.busy": "2022-08-28T23:55:04.710927Z",
          "iopub.status.idle": "2022-08-28T23:55:04.715326Z",
          "shell.execute_reply": "2022-08-28T23:55:04.714880Z",
          "shell.execute_reply.started": "2022-08-28T23:55:04.711118Z"
        },
        "id": "7oZV6MUpev9x",
        "outputId": "68dbcbe1-5cc1-4c97-e2e2-3a37e7bc99c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VERB', 'VERB', 'VERB', 'INTJ']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "[token.pos_ for token in doc if token.lemma_ =='like']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**5. Matching Attribute (LENGTH)**"
      ],
      "metadata": {
        "id": "sdffU9fLGkE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a spaCy Doc object\n",
        "doc = nlp(\"I see you are doing a good job.\")\n",
        "\n",
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern to match tokens with a length of 3\n",
        "pattern = [{\"LENGTH\": 3}]\n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"Length\", [pattern])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Print the matching text from the Doc\n",
        "    print(doc[start:end].text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq13vOO178dh",
        "outputId": "14e0579d-a562-4a77-b920-26c629f3f939"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "see\n",
            "you\n",
            "are\n",
            "job\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color = 'pickle'>**6. Using POS tags in matcher**"
      ],
      "metadata": {
        "id": "OwaqhIS5-vOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example 1**"
      ],
      "metadata": {
        "id": "XSa61yO-MSDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:08.636588Z",
          "iopub.status.busy": "2022-08-28T23:55:08.636383Z",
          "iopub.status.idle": "2022-08-28T23:55:08.647512Z",
          "shell.execute_reply": "2022-08-28T23:55:08.647052Z",
          "shell.execute_reply.started": "2022-08-28T23:55:08.636574Z"
        },
        "id": "W1rThjvoeJUE",
        "outputId": "5b0fc972-d4fa-4b6c-a418-82f0083fdfac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(12506337956553590349, 37, 38), (12506337956553590349, 42, 43), (12506337956553590349, 51, 52)]\n",
            "['liked', 'like', 'like']\n"
          ]
        }
      ],
      "source": [
        "disabled.restore()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(text)\n",
        "\n",
        "# write a pattern to match word whose lemma is like and pos tag is VERB\n",
        "pattern = [{\"LEMMA\": \"like\", \"POS\": \"VERB\"}]\n",
        "\n",
        "# matcher.add() method to add patterns to matcher\n",
        "matcher.add(\"Pos\",[pattern])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(matches)\n",
        "print([doc[start:end].text for match_id, start, end in matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example 2**"
      ],
      "metadata": {
        "id": "ufnFlrTRMV68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:12.199130Z",
          "iopub.status.busy": "2022-08-28T23:55:12.198928Z",
          "iopub.status.idle": "2022-08-28T23:55:12.210313Z",
          "shell.execute_reply": "2022-08-28T23:55:12.209905Z",
          "shell.execute_reply.started": "2022-08-28T23:55:12.199116Z"
        },
        "id": "du2cB0vyTO7l",
        "outputId": "65d40d3f-94c8-4ba0-cccc-a6164bc9d15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['like iOS']\n"
          ]
        }
      ],
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "doc = nlp(text)\n",
        "\n",
        "# pattern to match word whose lemma is like and pos tag is VERB. \n",
        "# This word should be followed by a word whose pos tag is Noun\n",
        "pattern = [{\"LEMMA\": \"like\", \"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "# matcher.add() method to add patterns to matcher\n",
        "matcher.add(\"LemmaPos\",[pattern])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "\n",
        "print([doc[start:end].text for match_id, start, end in matches])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**7. Use Quantifiers**"
      ],
      "metadata": {
        "id": "8T3jNBM4_kin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a spaCy Doc object\n",
        "doc = nlp(\"I am reading a new book on NLP. I read an excellent Deep Learning book last week\")\n",
        "\n",
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern to match the lemma \"read\" followed by an optional determiner and an adjective\n",
        "pattern = [\n",
        "    {\"LEMMA\": \"read\"},\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
        "    {\"POS\": \"ADJ\"}\n",
        "]\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"Quantifier\", [pattern])\n",
        "\n",
        "# Iterate over the matches for the Doc object\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print the matching text from the Doc\n",
        "    print(doc[start:end].text)"
      ],
      "metadata": {
        "id": "M3vEkVF_670l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a963b95-d032-4917-992e-2a130b9b8072"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading a new\n",
            "read an excellent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**8. Using Regular Expressions in Matcher**"
      ],
      "metadata": {
        "id": "1CKw0i-BvWQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example 1**"
      ],
      "metadata": {
        "id": "jiKKe6tSL-xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Email Addresses using Regular Expressions and spaCy's Matcher\n",
        "\n",
        "# The text we want to extract email addresses from\n",
        "text = 'You can contact me at @twitter, xyz@utdallas.edu, abx@gmail.com'\n",
        "\n",
        "# Create a spaCy Doc object from the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern to match email addresses using a regular expression\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"\\w+@\\w+\"}}]\n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"Email\", [pattern])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Print the matched text from the Doc\n",
        "print([doc[start:end].text for match_id, start, end in matches])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNYVbetQtLdV",
        "outputId": "bf9dd73c-0c01-4d47-c4b2-73ff733cc259"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xyz@utdallas.edu', 'abx@gmail.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'dodgerblue'> **Note**: Matcher object gives the complete token where the pattern occurs. Let us compare the result with re.findall."
      ],
      "metadata": {
        "id": "Xm0xy63d1TpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[\\w]+@[\\w]+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5U28Jp915if",
        "outputId": "3025b73b-988d-40ed-cafb-e83101d954a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xyz@utdallas', 'abx@gmail']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example 2**"
      ],
      "metadata": {
        "id": "4Bze4GupOONX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Create two spaCy Doc objects\n",
        "doc1 = nlp(\"I travelled by bus.\")\n",
        "doc2 = nlp(\"She traveled by bike.\")\n",
        "\n",
        "# Define a pattern to match using a combination of POS and a regex pattern\n",
        "pattern = [{\"POS\": \"PRON\"}, {\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}]\n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"PosRegex\", [pattern])\n",
        "\n",
        "# Iterate over the matches in the first Doc object\n",
        "for matchid, start, end in matcher(doc1):\n",
        "    # Print the matching span from the first Doc\n",
        "    print(doc1[start:end])\n",
        "\n",
        "# Iterate over the matches in the second Doc object\n",
        "for mid, start, end in matcher(doc2):\n",
        "    # Print the matching span from the second Doc\n",
        "    print(doc2[start:end])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExqPFfYi_3N7",
        "outputId": "14ff55e1-1161-4cd7-da92-ccc6fffce6d2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I travelled\n",
            "She traveled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example 3**"
      ],
      "metadata": {
        "id": "fJ-OmRrzOBAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Let us try different frequency of radio stations - FM 12.9, AM 104.9, FM 104.1,  AM 123.8. 1234\"\n",
        "radio_stations = re.findall(r'[FA]M\\s\\d{2,3}\\.\\d', text)\n",
        "radio_stations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCnH2HUoHn0z",
        "outputId": "c16ad8d3-4174-458e-e018-372d79bdd6d6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['FM 12.9', 'AM 104.9', 'FM 104.1', 'AM 123.8']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Radio Stations using Regular Expressions and spaCy's Matcher\n",
        "\n",
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# The text we want to extract radio stations from\n",
        "text = \"Let us try different frequency of radio stations - FM 12.9, AM 104.9, FM 104.1, AM 123.8. 1234\"\n",
        "\n",
        "# Create a spaCy Doc object from the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Define a pattern to match radio stations using a regular expression\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"[FA]M\\\\s\\\\d{2,3}\\\\.\\\\d\"}}]\n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"RadioStation\", [pattern])\n",
        "\n",
        "# Get the matches from the Matcher for the Doc object\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Print the matched text from the Doc\n",
        "for match_id, start, end in matches:\n",
        "    print(doc[start:end].text)"
      ],
      "metadata": {
        "id": "9niecSefLDjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'dodgerblue'>**Using the same pattern as used in `re.findall()` does not match anything. WHY?**\n",
        "\n",
        " - The match was applied to a single token \n",
        " - No single token matched the pattern\n",
        "\n",
        " Let us modify the pattern now.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2wBWvSk3c80a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the input text\n",
        "text = \"Let us try different frequency of radio stations - FM 12.9, AM 104.9, FM 104.1,  AM 123.8. 1234\"\n",
        "\n",
        "# Apply the spaCy's NLP model on the input text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Define a pattern to match \"FM\" or \"AM\" followed by a number with 2 to 3 digits and a dot followed by another number\n",
        "pattern = [\n",
        "    {\"TEXT\": {'REGEX': '[FA]M'}}, \n",
        "    {\"TEXT\": {'REGEX': '\\d{2,3}\\.\\d'}} \n",
        "]\n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"RegexMulti2\", [pattern]) \n",
        "\n",
        "# Find matches in the text using the Matcher\n",
        "matches = matcher(doc) \n",
        "\n",
        "# Print the matched text\n",
        "for match_id, start, end in matches: \n",
        "    print(doc[start:end].text) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW_zyiRJM8tP",
        "outputId": "7c0a6449-b661-4a22-ab08-1108cf79bd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FM 12.9\n",
            "AM 104.9\n",
            "FM 104.1\n",
            "AM 123.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**9. Matching Attribute (SHAPE)**"
      ],
      "metadata": {
        "id": "sPAZBI728LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Understanding Shape Attribute**"
      ],
      "metadata": {
        "id": "7Qlqu4rJNP2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Let us try different radio stations - FM 12.9, AM 104.9, FM 104.1,  AM 123.8 and A234Hj.,-9\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Get the text and shape of each token in the doc\n",
        "[(token.text,token.shape_) for token in doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mQ2EnrkAng6",
        "outputId": "8dffe2e1-505c-47af-e884-cbe8b523ade2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', 'Xxx'),\n",
              " ('us', 'xx'),\n",
              " ('try', 'xxx'),\n",
              " ('different', 'xxxx'),\n",
              " ('radio', 'xxxx'),\n",
              " ('stations', 'xxxx'),\n",
              " ('-', '-'),\n",
              " ('FM', 'XX'),\n",
              " ('12.9', 'dd.d'),\n",
              " (',', ','),\n",
              " ('AM', 'XX'),\n",
              " ('104.9', 'ddd.d'),\n",
              " (',', ','),\n",
              " ('FM', 'XX'),\n",
              " ('104.1', 'ddd.d'),\n",
              " (',', ','),\n",
              " (' ', ' '),\n",
              " ('AM', 'XX'),\n",
              " ('123.8', 'ddd.d'),\n",
              " ('and', 'xxx'),\n",
              " ('A234Hj.,-9', 'XdddXx.,-d')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: `shape_` represents the token's shape in the form of a string, where each character represents a different type of character in the token. For example, `'x'` represents a lowercase letter, `'X'` represents an uppercase letter, `'d'` represents a digit, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vd7qkssMNDAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Use SHAPE in Matcher**"
      ],
      "metadata": {
        "id": "_MrCJcM9Nf5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a doc object using spaCy\n",
        "doc = nlp(\"Let us try different frequency of radio stations - FM 12.9, AM 104.9, FM 104.1,  AM 123.8.\")\n",
        "\n",
        "# Create a Matcher object using spaCy's vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern to match the text with shape ddd.d\n",
        "pattern = [{\"SHAPE\": 'ddd.d'}]\n",
        "\n",
        "# Add the pattern to the Matcher\n",
        "matcher.add(\"Shape\", [pattern])\n",
        "\n",
        "# Apply the matcher to the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Loop through the matches and print matched text\n",
        "for match_id, start, end in matches:\n",
        "    print(doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "309eXUnf-d-F",
        "outputId": "ec0b24a3-edee-4636-efae-c1c3926edb85"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104.9\n",
            "104.1\n",
            "123.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Use SHAPE with Regex in Matcher**"
      ],
      "metadata": {
        "id": "IfsxNKlwNwfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Let us try different frequency of radio stations - FM 12.9, AM 104.9, FM 104.1,  AM 123.8.\") \n",
        "\n",
        "# Creating a Matcher object\n",
        "matcher = Matcher(nlp.vocab) \n",
        "\n",
        "# Define the pattern to match\n",
        "# Here we are using SHAPE property with a regular expression\n",
        "pattern = [{\"SHAPE\": {'REGEX': 'd?dd.d'}} ]\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "matcher.add(\"ShapeRegex\", [pattern]) \n",
        "\n",
        "# Use the matcher to find matches in the text\n",
        "matches = matcher(doc) \n",
        "\n",
        "# Iterate over the matches and print the matched text\n",
        "for match_id, start, end in matches: \n",
        "    print(doc[start:end].text)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9TiBpgcDYxg",
        "outputId": "91d8e1f3-c660-436a-e7a4-c881ff2fb8af"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.9\n",
            "104.9\n",
            "104.1\n",
            "123.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**10 Extract X relationship Y using Dependency Labels**\n",
        "\n",
        "Here we will extract pair of entities: (X, Y) if there is a relationship like X acquired (bought) Y, Y was acquired (bought) by Y.\n"
      ],
      "metadata": {
        "id": "47sYGRGqOrLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 =  \"In their largest acquisition to date, Google has acquired YouTube for $1.65 billion\"\n",
        "text2 = \" YouTube was acquired by Google for $1.65 billion\"\n",
        "text3 = \" Google bought YouTube for $1.65 billion\"\n",
        "text4 = \" Work was done\"\n",
        "doc1 = nlp(text1)\n",
        "doc2 = nlp(text2)\n",
        "doc3 = nlp(text3)\n",
        "doc4 = nlp(text4)"
      ],
      "metadata": {
        "id": "dMNKxjuB7M9K"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Understanding Dependency Labels**"
      ],
      "metadata": {
        "id": "tC4lXn8nTPGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{\"Text\":<12}: {\"Lemma\":<10}: {\"POS\":<10}: DEP\\n')\n",
        "for token in doc1:\n",
        "  print(f'{token.text:<12}: {token.lemma_:<10}: {token.pos_:<10}: {token.dep_}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABaozRt177Ul",
        "outputId": "32cdab5c-5657-478c-aef5-646fe56fb372"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text        : Lemma     : POS       : DEP\n",
            "\n",
            "In          : in        : ADP       : prep\n",
            "their       : their     : PRON      : poss\n",
            "largest     : large     : ADJ       : amod\n",
            "acquisition : acquisition: NOUN      : pobj\n",
            "to          : to        : ADP       : prep\n",
            "date        : date      : NOUN      : pobj\n",
            ",           : ,         : PUNCT     : punct\n",
            "Google      : Google    : PROPN     : nsubj\n",
            "has         : have      : AUX       : aux\n",
            "acquired    : acquire   : VERB      : ROOT\n",
            "YouTube     : YouTube   : PROPN     : dobj\n",
            "for         : for       : ADP       : prep\n",
            "$           : $         : SYM       : quantmod\n",
            "1.65        : 1.65      : NUM       : compound\n",
            "billion     : billion   : NUM       : pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{\"Text\":<12}: {\"Lemma\":<10}: {\"POS\":<10}: DEP\\n')\n",
        "for token in doc2:\n",
        "  print(f'{token.text:<12}: {token.lemma_:<10}: {token.pos_:<10}: {token.dep_}')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi7DjXgS9Mx6",
        "outputId": "fd51223f-041e-4beb-ec02-8f485e541323"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text        : Lemma     : POS       : DEP\n",
            "\n",
            "            :           : SPACE     : dep\n",
            "YouTube     : YouTube   : PROPN     : nsubjpass\n",
            "was         : be        : AUX       : auxpass\n",
            "acquired    : acquire   : VERB      : ROOT\n",
            "by          : by        : ADP       : agent\n",
            "Google      : Google    : PROPN     : pobj\n",
            "for         : for       : ADP       : prep\n",
            "$           : $         : SYM       : quantmod\n",
            "1.65        : 1.65      : NUM       : compound\n",
            "billion     : billion   : NUM       : pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Label description with spacy.explain()**"
      ],
      "metadata": {
        "id": "v5RdOc2oTbZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy.explain('nsubjpass'))\n",
        "print(spacy.explain('nsubj'))\n",
        "print(spacy.explain('pobj'))\n",
        "print(spacy.explain('dobj'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xftK0__69S66",
        "outputId": "d72ec3a9-904e-4ef9-ac98-ff7da21b2c55"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nominal subject (passive)\n",
            "nominal subject\n",
            "object of preposition\n",
            "direct object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{\"Text\":<12}: {\"Lemma\":<10}: {\"POS\":<10}: DEP\\n')\n",
        "for token in doc3:\n",
        "  print(f'{token.text:<12}: {token.lemma_:<10}: {token.pos_:<10}: {token.dep_}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucd8VoZfCClR",
        "outputId": "fa0b1906-0beb-4e93-ec2e-620df33720f5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text        : Lemma     : POS       : DEP\n",
            "\n",
            "            :           : SPACE     : dep\n",
            "Google      : Google    : PROPN     : nsubj\n",
            "bought      : buy       : VERB      : ROOT\n",
            "YouTube     : YouTube   : PROPN     : dobj\n",
            "for         : for       : ADP       : prep\n",
            "$           : $         : SYM       : quantmod\n",
            "1.65        : 1.65      : NUM       : compound\n",
            "billion     : billion   : NUM       : pobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{\"Text\":<12}: {\"Lemma\":<10}: {\"POS\":<10}: DEP\\n')\n",
        "for token in doc4:\n",
        "  print(f'{token.text:<12}: {token.lemma_:<10}: {token.pos_:<10}: {token.dep_}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNYiZtguGACQ",
        "outputId": "3ab6e9a9-ec40-4a40-aac9-3ba11a9d8e69"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text        : Lemma     : POS       : DEP\n",
            "\n",
            "            :           : SPACE     : dep\n",
            "Work        : Work      : PROPN     : nsubjpass\n",
            "was         : be        : AUX       : auxpass\n",
            "done        : do        : VERB      : ROOT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Step1: Check lemma of ROOT word**"
      ],
      "metadata": {
        "id": "l6htqd-xVR_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def root_acquire(doc)-> bool:\n",
        "    \"\"\"\n",
        "    A function that checks if a root of a given spaCy doc is 'acquire' or 'buy'\n",
        "\n",
        "    Parameters:\n",
        "    doc (spacy.tokens.doc.Doc): spaCy doc to be analyzed\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the root of the given doc is either 'acquire' or 'buy' else False\n",
        "\n",
        "    \"\"\"\n",
        "    return len([token for token in doc if token.dep_ == 'ROOT' if token.lemma_ in  ['acquire', 'buy']]) >0\n"
      ],
      "metadata": {
        "id": "FudULP_zA2ue"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(root_acquire(doc1))\n",
        "print(root_acquire(doc2))\n",
        "print(root_acquire(doc3))\n",
        "print(root_acquire(doc4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQjduG_MAjWA",
        "outputId": "59c49f87-d610-42ac-a2f2-1f18285ddca8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'> **Step2: Check active/passive voice**"
      ],
      "metadata": {
        "id": "f2iDfV-SULfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_passive(doc) -> bool:\n",
        "  \"\"\"\n",
        "  Check if a document contains passive voice sentences.\n",
        "  This function takes in a spaCy doc object and returns True if \n",
        "  it contains one or more tokens with a dependency label of \"nsubjpass\".\n",
        "\n",
        "  Args:\n",
        "  doc (spaCy doc object): The document to be analyzed.\n",
        "\n",
        "  Returns:\n",
        "  bool: True if the document contains passive voice sentences, False otherwise.\n",
        "  \"\"\"\n",
        "  return len([token for token in doc if token.dep_ == 'nsubjpass']) >0"
      ],
      "metadata": {
        "id": "G1GSGRX8-N8c"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(is_passive(doc1))\n",
        "print(is_passive(doc2))\n",
        "print(is_passive(doc3))\n",
        "print(is_passive(doc4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsY8EIwv_ohL",
        "outputId": "ecace9ab-16a5-45e9-effd-d92bb2e4f6fb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'> **Step3: Extract Relationship**"
      ],
      "metadata": {
        "id": "pl3jPFGwVpC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def get_x_acquire_y_pairs(doc) -> Tuple[str, str]:\n",
        "  \"\"\"\n",
        "  Extract the X acquire Y pairs from a spaCy doc object.\n",
        "  \n",
        "  Args:\n",
        "    doc (spaCy doc object): The document to be analyzed.\n",
        "    \n",
        "  Returns:\n",
        "    Tuple[str, str]: A tuple of X and Y if X acquires Y.\n",
        "    None: If X acquires Y pair is not present in the document.\n",
        "  \"\"\"\n",
        "  if root_acquire(doc):\n",
        "    if is_passive(doc):\n",
        "      # Get X if the document is in passive voice\n",
        "      x = [token.text for token in doc if token.dep_.endswith('obj')]\n",
        "      # Get Y if the document is in passive voice\n",
        "      y = [token.text for token in doc if token.dep_ in ('nsubjpass')]\n",
        "    else:\n",
        "      # Get X if the document is not in passive voice\n",
        "      x = [token.text for token in doc if token.dep_.endswith('subj')]\n",
        "      # Get Y if the document is not in passive voice\n",
        "      y = [token.text for token in doc if token.dep_.endswith('dobj')]\n",
        "    return (x[0], y[0])\n",
        "  else: \n",
        "    print('X acquire Y pair is not present in document') \n",
        "\n"
      ],
      "metadata": {
        "id": "H_JGpl4RHt6T"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_x_acquire_y_pairs(doc1))\n",
        "print(get_x_acquire_y_pairs(doc2))\n",
        "print(get_x_acquire_y_pairs(doc3))\n",
        "get_x_acquire_y_pairs(doc4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH56neawLwP5",
        "outputId": "a145d981-da90-4dd6-fcb2-f853a9d5ce1d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Google', 'YouTube')\n",
            "('Google', 'YouTube')\n",
            "('Google', 'YouTube')\n",
            "X acquire Y pair is not present in document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**11. Phrase Matcher**\n",
        "\n",
        "Using the PhraseMatcher to construct Doc objects rather than token patterns is a far more effective option if you need to match extensive terminology lists. For Example - It is difficult to define patterns that will match all the country names. However, we can easily enumerate all the country names and creaet a list. We can create a doc object from this list and use that as the basis of our information extraction script."
      ],
      "metadata": {
        "id": "xiW91KFT9jRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "import json"
      ],
      "metadata": {
        "id": "7LFsLsA67IfW"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example1 - Countries**"
      ],
      "metadata": {
        "id": "Tp8Dwk31nxth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'pickle'>**Download list of countries**"
      ],
      "metadata": {
        "id": "Vwo0wniIJvxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_folder = Path(base_folder/'nlp')\n",
        "data_folder = base_folder/'data'"
      ],
      "metadata": {
        "id": "EvGr9aQEuOwM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = data_folder/'countries.json'\n",
        "URL = 'https://raw.githubusercontent.com/explosion/spacy-course/master/exercises/en/countries.json'\n",
        "if not file.exists():\n",
        "  !wget {URL} -P {data_folder} -O {file}"
      ],
      "metadata": {
        "id": "oF0CZa8jJs3z"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file, 'r') as f:\n",
        "  COUNTRIES = json.loads(f.read())"
      ],
      "metadata": {
        "id": "i3qWvQwC71Aq"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COUNTRIES[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sp3H8fA8Kz0",
        "outputId": "a6949188-36e3-4766-d740-e4e22800d68f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Afghanistan',\n",
              " 'Åland Islands',\n",
              " 'Albania',\n",
              " 'Algeria',\n",
              " 'American Samoa',\n",
              " 'Andorra',\n",
              " 'Angola',\n",
              " 'Anguilla',\n",
              " 'Antarctica',\n",
              " 'Antigua and Barbuda']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'pickle'>**Create patterns**\n",
        "We will now create a list of doc object as patterns"
      ],
      "metadata": {
        "id": "GgpUt6AWKEkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JKXdyYeLfCW",
        "outputId": "825903ab-ced1-418e-ab28-73b010b53271"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disable = nlp.select_pipes(disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])"
      ],
      "metadata": {
        "id": "TW64I3YaLxaZ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# patterns = [nlp.make_doc(country) for country in COUNTRIES] # slower version\n",
        "patterns = list(nlp.pipe(COUNTRIES))"
      ],
      "metadata": {
        "id": "qcZ_iXNn7nl7"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patterns[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfr15POq9vmM",
        "outputId": "f925259a-f4ec-42de-c5c2-d60e493c5bfe"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Afghanistan,\n",
              " Åland Islands,\n",
              " Albania,\n",
              " Algeria,\n",
              " American Samoa,\n",
              " Andorra,\n",
              " Angola,\n",
              " Anguilla,\n",
              " Antarctica,\n",
              " Antigua and Barbuda]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patterns are doc objects not text\n",
        "type(patterns[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMc-QsON92I4",
        "outputId": "fdaca1da-d672-4b0a-f5cd-db2d73941d37"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'pickle'>**Add patterns to Phrase Matcher**"
      ],
      "metadata": {
        "id": "5ERrglg6KcB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = 'New Zealand defated Germany in rugby'\n",
        "\n",
        "# Apply spaCy NLP pipeline to the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Create a PhraseMatcher object to match phrases in the document\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "# Add the patterns to be matched to the matcher\n",
        "matcher.add('phrase-country', patterns)\n",
        "\n",
        "# Get the matches for the patterns in the document\n",
        "matches = matcher(doc) \n",
        "\n",
        "# Loop over the matches and print the matching text\n",
        "for match_id, start, end in matches: \n",
        "    # Matching text\n",
        "    print(doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uGJoICs9ZkG",
        "outputId": "9e9c614d-cd7a-4a9e-d9cc-b488cda28046"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Zealand\n",
            "Germany\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try a variation with lowercase and uppercase."
      ],
      "metadata": {
        "id": "0LlpHeLMMGSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'new zealand defated GERMANY in rugby.'\n",
        "doc = nlp(text)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add('phrase-country', patterns)\n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches: \n",
        "    print(doc[start:end].text) "
      ],
      "metadata": {
        "id": "_MzjZw7R-4yS"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not get any result as the patterns are case senstive (patterns are in Camel case (First word is capital letter)"
      ],
      "metadata": {
        "id": "Q7oBb8LhMUf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'pickle'>**Use attributes in Phrase Matcher**\n",
        "\n",
        "We can easily overcome the above issue by adding attribute - LOWER in our matcher."
      ],
      "metadata": {
        "id": "c3El2WcEKobB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'new zealand defated GERMANY in rugby. Some other Variations iNDIA, united STATES OF America'\n",
        "doc = nlp(text)\n",
        "matcher = PhraseMatcher(nlp.vocab, attr = 'LOWER')\n",
        "matcher.add('phrase-country', patterns)\n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches: \n",
        "    print(doc[start:end].text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuNIOXPP_fvJ",
        "outputId": "d25004f4-d7d3-49c9-d30f-31086f608f6f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new zealand\n",
            "GERMANY\n",
            "iNDIA\n",
            "united STATES OF America\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'pickle'>**Example2 - IP Addresses**"
      ],
      "metadata": {
        "id": "hW26TqUOnyuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
        "ip_adresses = [\"197.1.1.1\", \"197.197.1.1\"]\n",
        "patterns = list(nlp.pipe(ip_adresses))\n",
        "matcher.add(\"IpAddressess\", patterns)\n",
        "\n",
        "doc = nlp(\"The static IP adress for this facility are 127.3.4.1, 127.123.2.2\")\n",
        "for match_id, start, end in matcher(doc):\n",
        "    print( doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAKdRAA2IwmM",
        "outputId": "82cc5d62-fbe7-4284-e3d2-de4addcd4d97"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127.3.4.1\n",
            "127.123.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'pickle'>**12. Use entities in Matcher**"
      ],
      "metadata": {
        "id": "8i9hIYpJb510"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GflfBzctdyd",
        "outputId": "3a59e2c4-a175-4d4f-ef45-7ff0e5728702"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disabled.restore()"
      ],
      "metadata": {
        "id": "wdTMJVUhtfWu"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RuJfYBMtgaa",
        "outputId": "ceb29118-0db5-4f3b-af59-e41923d91d36"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I work at Apple. My favorite fruit is apple.\"\n",
        "doc = nlp(text)\n",
        "[(entity.label_, entity.text) for entity in doc.ents ]"
      ],
      "metadata": {
        "id": "O-XQuh8gcD4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4dad9a-54db-42c4-8f08-857d893cfdfe"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ORG', 'Apple')]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab) \n",
        "\n",
        "pattern = [{\"ENT_TYPE\": \"ORG\", \"LOWER\": \"apple\"} ]\n",
        "matcher.add(\"entity\", [pattern]) \n",
        "matches = matcher(doc) \n",
        "for match_id, start, end in matches: \n",
        "    print(doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LazkN6XsI_b",
        "outputId": "6cbb96f0-d622-4673-f7d6-a94e220b6690"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIdaZvqk9pX8"
      },
      "source": [
        "# <font color = 'pickle'>**Custom extensions for tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:23.434048Z",
          "iopub.status.busy": "2022-08-28T23:55:23.433845Z",
          "iopub.status.idle": "2022-08-28T23:55:23.441117Z",
          "shell.execute_reply": "2022-08-28T23:55:23.440766Z",
          "shell.execute_reply.started": "2022-08-28T23:55:23.434034Z"
        },
        "id": "S36SXnm9wlgA"
      },
      "outputs": [],
      "source": [
        "# we need to import Token class to set custom extension\n",
        "from spacy.tokens import Token\n",
        "doc = nlp(\"My email is harpreet@utdallas.edu and my url is https://j.u.edu/faculty/hs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:24.447794Z",
          "iopub.status.busy": "2022-08-28T23:55:24.447622Z",
          "iopub.status.idle": "2022-08-28T23:55:24.450352Z",
          "shell.execute_reply": "2022-08-28T23:55:24.449888Z",
          "shell.execute_reply.started": "2022-08-28T23:55:24.447780Z"
        },
        "id": "l01E70pCKR9u"
      },
      "outputs": [],
      "source": [
        "# Define the extension attribute on the token level with name as \"clean\" and default value as False\n",
        "Token.set_extension('clean', default=False, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:25.485945Z",
          "iopub.status.busy": "2022-08-28T23:55:25.485772Z",
          "iopub.status.idle": "2022-08-28T23:55:25.489067Z",
          "shell.execute_reply": "2022-08-28T23:55:25.488583Z",
          "shell.execute_reply.started": "2022-08-28T23:55:25.485931Z"
        },
        "id": "jTwO2LsihAg4",
        "outputId": "6fa8c311-2f25-4158-ea5c-dd3871057379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token.text                  : token._.clean\n",
            "My                          : False\n",
            "email                       : False\n",
            "is                          : False\n",
            "harpreet@utdallas.edu       : False\n",
            "and                         : False\n",
            "my                          : False\n",
            "url                         : False\n",
            "is                          : False\n",
            "https://j.u.edu/faculty/hs  : False\n",
            ".                           : False\n"
          ]
        }
      ],
      "source": [
        "# Printing each token on the doc object and the stored value by the extension attribute.\n",
        "# All the values default to 'False'\n",
        "print(f'{\"token.text\":<27} : {\"token._.clean\"}')\n",
        "for token in doc:\n",
        "  print(f'{token.text:<27} : {token._.clean}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:26.360554Z",
          "iopub.status.busy": "2022-08-28T23:55:26.360339Z",
          "iopub.status.idle": "2022-08-28T23:55:26.363472Z",
          "shell.execute_reply": "2022-08-28T23:55:26.362948Z",
          "shell.execute_reply.started": "2022-08-28T23:55:26.360541Z"
        },
        "id": "bPZWhi9dglS5"
      },
      "outputs": [],
      "source": [
        "# Change the value of custom extension (clean) to True if it is not a punctuation, url or email\n",
        "for token in doc:\n",
        "  if not (token.is_punct or token.like_url or token.like_email):\n",
        "    token._.set('clean', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:27.209811Z",
          "iopub.status.busy": "2022-08-28T23:55:27.209593Z",
          "iopub.status.idle": "2022-08-28T23:55:27.212979Z",
          "shell.execute_reply": "2022-08-28T23:55:27.212511Z",
          "shell.execute_reply.started": "2022-08-28T23:55:27.209797Z"
        },
        "id": "SFwB0CA6nz7Y",
        "outputId": "1a179515-5a77-4d82-efaf-535b77d7df54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token.text                  : token._.clean\n",
            "My                          : True\n",
            "email                       : True\n",
            "is                          : True\n",
            "harpreet@utdallas.edu       : False\n",
            "and                         : True\n",
            "my                          : True\n",
            "url                         : True\n",
            "is                          : True\n",
            "https://j.u.edu/faculty/hs  : False\n",
            ".                           : False\n"
          ]
        }
      ],
      "source": [
        "# Printing the tokens again to see the modified values.\n",
        "print(f'{\"token.text\":<27} : {\"token._.clean\"}')\n",
        "for token in doc:\n",
        "  print(f'{token.text:<27} : {token._.clean}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-08-28T23:55:27.996950Z",
          "iopub.status.busy": "2022-08-28T23:55:27.996775Z",
          "iopub.status.idle": "2022-08-28T23:55:28.001117Z",
          "shell.execute_reply": "2022-08-28T23:55:28.000517Z",
          "shell.execute_reply.started": "2022-08-28T23:55:27.996936Z"
        },
        "id": "YLijGGTKhVrx",
        "outputId": "f95730b0-2ced-48f4-84fd-b2fd5eb960f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'email', 'is', 'and', 'my', 'url', 'is']"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "[token.text for token in doc if token._.clean]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3nvCtHdTlv7I",
        "X1znF9Q1JM5A",
        "AgGJnqAqlv7K",
        "TgCsbeEGW9WD",
        "98Pcs8Y6BIHY",
        "vvHZ1RC4XENE",
        "CCKHbHc-XKa5",
        "k0XSdolTCePN",
        "vskE4nvxPbhb",
        "XIdaZvqk9pX8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}